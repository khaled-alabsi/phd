# MSPC Benchmarking Metrics

This folder contains comprehensive documentation on benchmarking metrics for Multivariate Statistical Process Control (MSPC) methods, particularly for fault detection and diagnosis.

## üìÇ Folder Structure

### 01-Core-Metrics/
Detailed definitions of all key metrics:
- **ARL (Average Run Length)**: Expected samples until signal
- **SDRL (Standard Deviation of Run Length)**: Variability in detection timing
- **MDRL (Median Run Length)**: Robust central tendency measure
- **FAR (False Alarm Rate)**: Rate of false positives
- **Detection Rate & Power**: Ability to detect true faults
- **Classification Metrics**: Precision, Recall, F1, AUC, etc.

### 02-Implementation-Guides/
Practical implementation approaches:
- ARL calculation methods (analytical vs. simulation)
- Monte Carlo simulation for complex scenarios
- Comprehensive metrics comparison tables

### 03-Examples-and-QA/
Learning resources:
- Worked examples with numerical calculations
- Industry-specific practice questions (semiconductor, automotive, pharma)
- Step-by-step solutions

### 04-Context/
Background and applications:
- MSPC benchmarking overview and motivation
- Standard datasets (Tennessee Eastman Process, etc.)
- Industry-specific applications

## üéØ Quick Reference

### When to Use Each Metric

| Goal | Recommended Metrics |
|------|-------------------|
| Compare false alarm rates | ARL‚ÇÄ, FAR, Type I Error |
| Measure detection speed | ARL‚ÇÅ, TTD, Detection Rate |
| Assess consistency | SDRL, MDRL |
| Evaluate classification | Precision, Recall, F1, AUC |
| Handle imbalanced data | Balanced Accuracy, F1 |

### Common Metric Relationships

- **ARL‚ÇÄ = 1 / FAR** (for memoryless charts)
- **Detection Rate = 1 - Type II Error**
- **Specificity = 1 - FPR**
- **F1 = 2 √ó (Precision √ó Recall) / (Precision + Recall)**

## üöÄ Getting Started

1. Start with **04-Context/MSPC-Benchmarking-Overview.md** for background
2. Review **01-Core-Metrics/** for metric definitions
3. Check **02-Implementation-Guides/** for calculation methods
4. Practice with **03-Examples-and-QA/** scenarios

## üìö Key Concepts

### Phase I vs Phase II
- **Phase I**: Historical data analysis, model building, control limit establishment
- **Phase II**: Real-time monitoring, online fault detection

### In-Control vs Out-of-Control
- **In-Control (IC)**: Process operating normally (ARL‚ÇÄ, FAR relevant)
- **Out-of-Control (OC)**: Process experiencing faults (ARL‚ÇÅ, Detection Rate relevant)

### Trade-offs
- Sensitivity vs False Alarms: Higher sensitivity ‚Üí more false alarms
- ARL‚ÇÄ vs ARL‚ÇÅ: Tightening limits reduces ARL‚ÇÄ but may increase ARL‚ÇÅ
- Precision vs Recall: Depends on cost of false positives vs false negatives
