{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81718398",
   "metadata": {},
   "source": [
    "# Main Comparison and Analysis Dashboard\n",
    "\n",
    "This notebook imports and compares results from all previous analysis notebooks, providing a comprehensive overview and comparison of all methods across different tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7e50e7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import os\n",
    "import json\n",
    "from typing import Dict, List, Tuple\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from tabulate import tabulate\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e7eaf5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "VERSION = \"1.00\"\n",
    "OUTPUT_PATH = \"output\"\n",
    "RESULTS_PATH = os.path.join(OUTPUT_PATH, VERSION)\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "161a32c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_plot(plot_name: str, suffix: str = \"\", plot_path: str = \"comparison\") -> None:\n",
    "    \"\"\"Save current matplotlib figure.\"\"\"\n",
    "    timestamp: str = \"\"\n",
    "    base_dir: str = os.path.join(OUTPUT_PATH, VERSION, plot_path)\n",
    "    os.makedirs(base_dir, exist_ok=True)\n",
    "\n",
    "    filename: str = f\"{plot_name}_{suffix}_v{VERSION}_{timestamp}.png\" if suffix else f\"{plot_name}_v{VERSION}_{timestamp}.png\"\n",
    "    filepath: str = os.path.join(base_dir, filename)\n",
    "\n",
    "    plt.savefig(filepath, bbox_inches=\"tight\", dpi=300)\n",
    "    print(f\"Plot saved: {filepath}\")\n",
    "\n",
    "def save_dataframe(df: pd.DataFrame, name: str, suffix: str = \"\") -> None:\n",
    "    \"\"\"Save a DataFrame to CSV.\"\"\"\n",
    "    timestamp: str = \"\"\n",
    "    base_dir: str = os.path.join(OUTPUT_PATH, VERSION)\n",
    "    os.makedirs(base_dir, exist_ok=True)\n",
    "\n",
    "    filename: str = f\"{name}_{suffix}_v{VERSION}_{timestamp}.csv\" if suffix else f\"{name}_v{VERSION}_{timestamp}.csv\"\n",
    "    filepath: str = os.path.join(base_dir, filename)\n",
    "\n",
    "    df.to_csv(filepath, index=True)\n",
    "    print(f\"Data saved: {filepath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea23942",
   "metadata": {},
   "source": [
    "## Load All Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f99cb9b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Loading Results from Previous Notebooks ===\n",
      "No files found matching pattern: classification_metrics_multiclass_*.csv\n",
      "Loading: output/1.00/classification_summary_v1.00_.json\n",
      "No files found matching pattern: anomaly_detection_metrics_anomaly_*.csv\n",
      "Loading: output/1.00/anomaly_detection_summary_v1.00_.json\n",
      "No files found matching pattern: merged_fault_classification_metrics_merging_*.csv\n",
      "Loading: output/1.00/merged_fault_analysis_summary_v1.00_.json\n",
      "No files found matching pattern: eda_summary_*.json\n",
      "\n",
      "=== Results Loading Complete ===\n"
     ]
    }
   ],
   "source": [
    "def load_csv_results(pattern: str) -> pd.DataFrame:\n",
    "    \"\"\"Load CSV results matching a pattern.\"\"\"\n",
    "    files = glob.glob(os.path.join(RESULTS_PATH, pattern))\n",
    "    if files:\n",
    "        # Get the most recent file\n",
    "        latest_file = max(files, key=os.path.getctime)\n",
    "        print(f\"Loading: {latest_file}\")\n",
    "        return pd.read_csv(latest_file, index_col=0)\n",
    "    else:\n",
    "        print(f\"No files found matching pattern: {pattern}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def load_json_summary(pattern: str) -> dict:\n",
    "    \"\"\"Load JSON summary matching a pattern.\"\"\"\n",
    "    files = glob.glob(os.path.join(RESULTS_PATH, pattern))\n",
    "    if files:\n",
    "        latest_file = max(files, key=os.path.getctime)\n",
    "        print(f\"Loading: {latest_file}\")\n",
    "        with open(latest_file, 'r') as f:\n",
    "            return json.load(f)\n",
    "    else:\n",
    "        print(f\"No files found matching pattern: {pattern}\")\n",
    "        return {}\n",
    "\n",
    "# Load results from each notebook\n",
    "print(\"=== Loading Results from Previous Notebooks ===\")\n",
    "\n",
    "# Classification results\n",
    "classification_results = load_csv_results(\"classification_metrics_multiclass_*.csv\")\n",
    "classification_summary = load_json_summary(\"classification_summary_*.json\")\n",
    "\n",
    "# Anomaly detection results\n",
    "anomaly_results = load_csv_results(\"anomaly_detection_metrics_anomaly_*.csv\")\n",
    "anomaly_summary = load_json_summary(\"anomaly_detection_summary_*.json\")\n",
    "\n",
    "# Merged fault results\n",
    "merged_results = load_csv_results(\"merged_fault_classification_metrics_merging_*.csv\")\n",
    "merged_summary = load_json_summary(\"merged_fault_analysis_summary_*.json\")\n",
    "\n",
    "# EDA results (if available)\n",
    "eda_summary = load_json_summary(\"eda_summary_*.json\")\n",
    "\n",
    "print(\"\\n=== Results Loading Complete ===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7813112f",
   "metadata": {},
   "source": [
    "## Data Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6ab8332f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved: output/1.00/analysis_overview_comparison_v1.00_.csv\n",
      "=== Tennessee Eastman Process Analysis Overview ===\n",
      "+----+-----------------------------+--------------------+----------------+---------------------+---------------+-----------------+\n",
      "|    | Analysis Type               |   Number of Models |   Test Samples |   Number of Classes | Best Model    |   Best Accuracy |\n",
      "+====+=============================+====================+================+=====================+===============+=================+\n",
      "|  0 | Multi-class Classification  |                  3 |          40320 |                   0 | Random Forest |           0.594 |\n",
      "+----+-----------------------------+--------------------+----------------+---------------------+---------------+-----------------+\n",
      "|  1 | Binary Anomaly Detection    |                  7 |          40320 |                   2 | MCUSUM        |           0.954 |\n",
      "+----+-----------------------------+--------------------+----------------+---------------------+---------------+-----------------+\n",
      "|  2 | Merged Fault Classification |                  3 |          40320 |                  17 | Random Forest |           0.692 |\n",
      "+----+-----------------------------+--------------------+----------------+---------------------+---------------+-----------------+\n"
     ]
    }
   ],
   "source": [
    "# Create data overview\n",
    "data_overview = {\n",
    "    'Analysis Type': ['Multi-class Classification', 'Binary Anomaly Detection', 'Merged Fault Classification'],\n",
    "    'Number of Models': [\n",
    "        len(classification_summary.get('models_trained', [])) if classification_summary else 0,\n",
    "        len(anomaly_summary.get('models_trained', [])) if anomaly_summary else 0,\n",
    "        len(merged_summary.get('models_trained', [])) if merged_summary else 0\n",
    "    ],\n",
    "    'Test Samples': [\n",
    "        classification_summary.get('total_test_samples', 0) if classification_summary else 0,\n",
    "        anomaly_summary.get('total_test_samples', 0) if anomaly_summary else 0,\n",
    "        merged_summary.get('total_test_samples', 0) if merged_summary else 0\n",
    "    ],\n",
    "    'Number of Classes': [\n",
    "        classification_summary.get('num_classes', 0) if classification_summary else 0,\n",
    "        2,  # Binary classification\n",
    "        merged_summary.get('num_classes', 0) if merged_summary else 0\n",
    "    ],\n",
    "    'Best Model': [\n",
    "        classification_summary.get('best_model', 'N/A') if classification_summary else 'N/A',\n",
    "        anomaly_summary.get('best_model', 'N/A') if anomaly_summary else 'N/A',\n",
    "        merged_summary.get('best_model', 'N/A') if merged_summary else 'N/A'\n",
    "    ],\n",
    "    'Best Accuracy': [\n",
    "        f\"{classification_summary.get('best_accuracy', 0):.3f}\" if classification_summary else 'N/A',\n",
    "        f\"{anomaly_summary.get('best_accuracy', 0):.3f}\" if anomaly_summary else 'N/A',\n",
    "        f\"{merged_summary.get('best_accuracy', 0):.3f}\" if merged_summary else 'N/A'\n",
    "    ]\n",
    "}\n",
    "\n",
    "overview_df = pd.DataFrame(data_overview)\n",
    "save_dataframe(overview_df, \"analysis_overview\", \"comparison\")\n",
    "\n",
    "print(\"=== Tennessee Eastman Process Analysis Overview ===\")\n",
    "print(tabulate(overview_df, headers=\"keys\", tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d3bae5",
   "metadata": {},
   "source": [
    "## Model Performance Comparison Across Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bd9156af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved: output/1.00/cross_task_comparison_comparison_v1.00_.csv\n",
      "No comparison data available.\n"
     ]
    }
   ],
   "source": [
    "def prepare_comparison_data():\n",
    "    \"\"\"Prepare data for cross-task model comparison.\"\"\"\n",
    "    comparison_data = []\n",
    "    \n",
    "    # Classification results\n",
    "    if not classification_results.empty:\n",
    "        for _, row in classification_results.iterrows():\n",
    "            comparison_data.append({\n",
    "                'Task': 'Multi-class Classification',\n",
    "                'Model': row['Model'],\n",
    "                'Accuracy': row['Accuracy'],\n",
    "                'F1_Score': row.get('Macro_F1', row.get('F1-Score', np.nan)),\n",
    "                'Precision': row.get('Macro_Precision', row.get('Precision', np.nan)),\n",
    "                'Recall': row.get('Macro_Recall', row.get('Recall', np.nan))\n",
    "            })\n",
    "    \n",
    "    # Anomaly detection results\n",
    "    if not anomaly_results.empty:\n",
    "        for _, row in anomaly_results.iterrows():\n",
    "            comparison_data.append({\n",
    "                'Task': 'Anomaly Detection',\n",
    "                'Model': row['Model'],\n",
    "                'Accuracy': row['Accuracy'],\n",
    "                'F1_Score': row['F1-Score'],\n",
    "                'Precision': row['Precision'],\n",
    "                'Recall': row['Recall / TPR']\n",
    "            })\n",
    "    \n",
    "    # Merged fault results\n",
    "    if not merged_results.empty:\n",
    "        for _, row in merged_results.iterrows():\n",
    "            comparison_data.append({\n",
    "                'Task': 'Merged Fault Classification',\n",
    "                'Model': row['Model'],\n",
    "                'Accuracy': row['Accuracy'],\n",
    "                'F1_Score': row['Macro_F1'],\n",
    "                'Precision': row['Macro_Precision'],\n",
    "                'Recall': row['Macro_Recall']\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(comparison_data)\n",
    "\n",
    "comparison_df = prepare_comparison_data()\n",
    "save_dataframe(comparison_df, \"cross_task_comparison\", \"comparison\")\n",
    "\n",
    "if not comparison_df.empty:\n",
    "    print(\"\\n=== Cross-Task Model Performance Comparison ===\")\n",
    "    print(tabulate(comparison_df, headers=\"keys\", tablefmt=\"grid\", floatfmt=\".3f\"))\n",
    "else:\n",
    "    print(\"No comparison data available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c61851",
   "metadata": {},
   "source": [
    "## Comprehensive Visualization Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2a4bb382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Accuracy comparison across all tasks\n",
    "if not comparison_df.empty:\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    \n",
    "    # Pivot data for better visualization\n",
    "    pivot_accuracy = comparison_df.pivot(index='Model', columns='Task', values='Accuracy')\n",
    "    \n",
    "    # Create grouped bar plot\n",
    "    ax = pivot_accuracy.plot(kind='bar', width=0.8)\n",
    "    plt.title('Model Accuracy Comparison Across All Tasks', fontsize=16, fontweight='bold')\n",
    "    plt.xlabel('Model', fontsize=12)\n",
    "    plt.ylabel('Accuracy', fontsize=12)\n",
    "    plt.legend(title='Task', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.ylim(0, 1.05)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for container in ax.containers:\n",
    "        ax.bar_label(container, fmt='%.3f', rotation=90, fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    save_plot('accuracy_comparison_all_tasks')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a2eff07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. F1-Score comparison across all tasks\n",
    "if not comparison_df.empty:\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    \n",
    "    pivot_f1 = comparison_df.pivot(index='Model', columns='Task', values='F1_Score')\n",
    "    \n",
    "    ax = pivot_f1.plot(kind='bar', width=0.8)\n",
    "    plt.title('Model F1-Score Comparison Across All Tasks', fontsize=16, fontweight='bold')\n",
    "    plt.xlabel('Model', fontsize=12)\n",
    "    plt.ylabel('F1-Score', fontsize=12)\n",
    "    plt.legend(title='Task', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.ylim(0, 1.05)\n",
    "    \n",
    "    for container in ax.containers:\n",
    "        ax.bar_label(container, fmt='%.3f', rotation=90, fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    save_plot('f1_score_comparison_all_tasks')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2d941755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Radar chart for best performing models\n",
    "def create_radar_chart(data_dict: dict, title: str):\n",
    "    \"\"\"Create radar chart for model comparison.\"\"\"\n",
    "    from math import pi\n",
    "    \n",
    "    # Metrics to include\n",
    "    metrics = ['Accuracy', 'F1_Score', 'Precision', 'Recall']\n",
    "    \n",
    "    # Number of metrics\n",
    "    N = len(metrics)\n",
    "    \n",
    "    # Compute angle for each metric\n",
    "    angles = [n / float(N) * 2 * pi for n in range(N)]\n",
    "    angles += angles[:1]  # Complete the circle\n",
    "    \n",
    "    # Initialize plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))\n",
    "    \n",
    "    # Plot data for each model\n",
    "    colors = ['b', 'r', 'g', 'orange', 'purple', 'brown', 'pink']\n",
    "    \n",
    "    for i, (model, values) in enumerate(data_dict.items()):\n",
    "        # Get values for this model\n",
    "        model_values = [values.get(metric, 0) for metric in metrics]\n",
    "        model_values += model_values[:1]  # Complete the circle\n",
    "        \n",
    "        # Plot\n",
    "        ax.plot(angles, model_values, 'o-', linewidth=2, \n",
    "                label=model, color=colors[i % len(colors)])\n",
    "        ax.fill(angles, model_values, alpha=0.25, color=colors[i % len(colors)])\n",
    "    \n",
    "    # Add metric labels\n",
    "    ax.set_xticks(angles[:-1])\n",
    "    ax.set_xticklabels(metrics)\n",
    "    \n",
    "    # Set y-axis limits\n",
    "    ax.set_ylim(0, 1)\n",
    "    \n",
    "    # Add title and legend\n",
    "    plt.title(title, size=16, fontweight='bold', y=1.08)\n",
    "    plt.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))\n",
    "    \n",
    "    return fig, ax\n",
    "\n",
    "# Create radar charts for each task\n",
    "if not comparison_df.empty:\n",
    "    for task in comparison_df['Task'].unique():\n",
    "        task_data = comparison_df[comparison_df['Task'] == task]\n",
    "        \n",
    "        # Prepare data for radar chart\n",
    "        radar_data = {}\n",
    "        for _, row in task_data.iterrows():\n",
    "            radar_data[row['Model']] = {\n",
    "                'Accuracy': row['Accuracy'],\n",
    "                'F1_Score': row['F1_Score'],\n",
    "                'Precision': row['Precision'],\n",
    "                'Recall': row['Recall']\n",
    "            }\n",
    "        \n",
    "        if radar_data:\n",
    "            fig, ax = create_radar_chart(radar_data, f'{task} - Model Performance')\n",
    "            safe_task_name = task.lower().replace(' ', '_').replace('-', '_')\n",
    "            save_plot(f'radar_chart_{safe_task_name}')\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103e9b54",
   "metadata": {},
   "source": [
    "## Best Model Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a23a8124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify best performing models for each task\n",
    "if not comparison_df.empty:\n",
    "    best_models_summary = []\n",
    "    \n",
    "    for task in comparison_df['Task'].unique():\n",
    "        task_data = comparison_df[comparison_df['Task'] == task]\n",
    "        \n",
    "        # Find best model by accuracy\n",
    "        best_by_accuracy = task_data.loc[task_data['Accuracy'].idxmax()]\n",
    "        # Find best model by F1-score\n",
    "        best_by_f1 = task_data.loc[task_data['F1_Score'].idxmax()]\n",
    "        \n",
    "        best_models_summary.append({\n",
    "            'Task': task,\n",
    "            'Best_by_Accuracy': best_by_accuracy['Model'],\n",
    "            'Best_Accuracy': f\"{best_by_accuracy['Accuracy']:.3f}\",\n",
    "            'Best_by_F1': best_by_f1['Model'],\n",
    "            'Best_F1': f\"{best_by_f1['F1_Score']:.3f}\",\n",
    "            'Avg_Accuracy': f\"{task_data['Accuracy'].mean():.3f}\",\n",
    "            'Avg_F1': f\"{task_data['F1_Score'].mean():.3f}\"\n",
    "        })\n",
    "    \n",
    "    best_models_df = pd.DataFrame(best_models_summary)\n",
    "    save_dataframe(best_models_df, \"best_models_summary\", \"comparison\")\n",
    "    \n",
    "    print(\"\\n=== Best Performing Models by Task ===\")\n",
    "    print(tabulate(best_models_df, headers=\"keys\", tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fb35e1",
   "metadata": {},
   "source": [
    "## Model Consistency Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1343d6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze which models perform consistently well across tasks\n",
    "if not comparison_df.empty:\n",
    "    # Calculate average performance across all tasks for each model\n",
    "    model_consistency = comparison_df.groupby('Model').agg({\n",
    "        'Accuracy': ['mean', 'std', 'count'],\n",
    "        'F1_Score': ['mean', 'std'],\n",
    "        'Precision': ['mean', 'std'],\n",
    "        'Recall': ['mean', 'std']\n",
    "    }).round(3)\n",
    "    \n",
    "    # Flatten column names\n",
    "    model_consistency.columns = ['_'.join(col).strip() for col in model_consistency.columns]\n",
    "    model_consistency = model_consistency.reset_index()\n",
    "    \n",
    "    # Sort by average accuracy\n",
    "    model_consistency = model_consistency.sort_values('Accuracy_mean', ascending=False)\n",
    "    \n",
    "    save_dataframe(model_consistency, \"model_consistency_analysis\", \"comparison\")\n",
    "    \n",
    "    print(\"\\n=== Model Consistency Analysis (Average Performance Across Tasks) ===\")\n",
    "    print(tabulate(model_consistency, headers=\"keys\", tablefmt=\"grid\", floatfmt=\".3f\"))\n",
    "    \n",
    "    # Plot consistency\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Create bar plot with error bars\n",
    "    x_pos = range(len(model_consistency))\n",
    "    plt.bar(x_pos, model_consistency['Accuracy_mean'], \n",
    "            yerr=model_consistency['Accuracy_std'], \n",
    "            capsize=5, alpha=0.7)\n",
    "    \n",
    "    plt.xlabel('Model')\n",
    "    plt.ylabel('Average Accuracy')\n",
    "    plt.title('Model Consistency Across All Tasks\\n(Error bars show standard deviation)', \n",
    "              fontsize=14, fontweight='bold')\n",
    "    plt.xticks(x_pos, model_consistency['Model'], rotation=45, ha='right')\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, (mean_val, std_val) in enumerate(zip(model_consistency['Accuracy_mean'], \n",
    "                                                model_consistency['Accuracy_std'])):\n",
    "        plt.text(i, mean_val + std_val + 0.01, f'{mean_val:.3f}¬±{std_val:.3f}', \n",
    "                ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    save_plot('model_consistency_analysis')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5204d32",
   "metadata": {},
   "source": [
    "## Task Difficulty Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2e68d358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze task difficulty based on average model performance\n",
    "if not comparison_df.empty:\n",
    "    task_difficulty = comparison_df.groupby('Task').agg({\n",
    "        'Accuracy': ['mean', 'std', 'min', 'max'],\n",
    "        'F1_Score': ['mean', 'std', 'min', 'max']\n",
    "    }).round(3)\n",
    "    \n",
    "    task_difficulty.columns = ['_'.join(col).strip() for col in task_difficulty.columns]\n",
    "    task_difficulty = task_difficulty.reset_index()\n",
    "    \n",
    "    # Sort by average accuracy (ascending = more difficult tasks first)\n",
    "    task_difficulty = task_difficulty.sort_values('Accuracy_mean')\n",
    "    \n",
    "    save_dataframe(task_difficulty, \"task_difficulty_analysis\", \"comparison\")\n",
    "    \n",
    "    print(\"\\n=== Task Difficulty Analysis (Based on Average Model Performance) ===\")\n",
    "    print(tabulate(task_difficulty, headers=\"keys\", tablefmt=\"grid\", floatfmt=\".3f\"))\n",
    "    \n",
    "    # Plot task difficulty\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    x_pos = range(len(task_difficulty))\n",
    "    plt.bar(x_pos, task_difficulty['Accuracy_mean'], \n",
    "            yerr=task_difficulty['Accuracy_std'], \n",
    "            capsize=5, alpha=0.7, color=['red', 'orange', 'green'])\n",
    "    \n",
    "    plt.xlabel('Task')\n",
    "    plt.ylabel('Average Accuracy Across All Models')\n",
    "    plt.title('Task Difficulty Analysis\\n(Lower average accuracy indicates higher difficulty)', \n",
    "              fontsize=14, fontweight='bold')\n",
    "    plt.xticks(x_pos, task_difficulty['Task'], rotation=45, ha='right')\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, (mean_val, std_val) in enumerate(zip(task_difficulty['Accuracy_mean'], \n",
    "                                                task_difficulty['Accuracy_std'])):\n",
    "        plt.text(i, mean_val + std_val + 0.01, f'{mean_val:.3f}¬±{std_val:.3f}', \n",
    "                ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    save_plot('task_difficulty_analysis')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34218166",
   "metadata": {},
   "source": [
    "## Comprehensive Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "41723d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "\t\tTENNESSEE EASTMAN PROCESS - COMPREHENSIVE ANALYSIS SUMMARY\n",
      "================================================================================\n",
      "\n",
      "üìä PROJECT INFORMATION:\n",
      "   ‚Ä¢ Analysis Version: 1.00\n",
      "   ‚Ä¢ Analysis Date: 2025-08-21 22:24:56\n",
      "   ‚Ä¢ Total Notebooks: 6\n",
      "   ‚Ä¢ Completed Analyses: 5\n",
      "\n",
      "üî¨ ANALYSIS SCOPE:\n",
      "   ‚Ä¢ Dataset: Tennessee Eastman Process\n",
      "   ‚Ä¢ Features: 52\n",
      "   ‚Ä¢ Analysis Types: Multi-class Classification, Binary Anomaly Detection, Merged Fault Classification\n",
      "\n",
      "ü§ñ MODEL SUMMARY:\n",
      "   ‚Ä¢ Total Models Trained: 0\n",
      "   ‚Ä¢ Model Types: \n",
      "   ‚Ä¢ Best Overall Model: N/A\n",
      "\n",
      "üìà PERFORMANCE HIGHLIGHTS:\n",
      "   ‚Ä¢ Highest Accuracy: 0.000\n",
      "   ‚Ä¢ Highest F1-Score: 0.000\n",
      "   ‚Ä¢ Average Accuracy: 0.000\n",
      "   ‚Ä¢ Average F1-Score: 0.000\n",
      "\n",
      "üìÅ OUTPUT FILES GENERATED:\n",
      "   ‚Ä¢ Location: output/1.00/\n",
      "   ‚Ä¢ Comprehensive Summary: comprehensive_analysis_summary_v1.00_.json\n",
      "   ‚Ä¢ Cross-Task Comparison: cross_task_comparison_comparison_v1.00_.csv\n",
      "   ‚Ä¢ Best Models Summary: best_models_summary_comparison_v1.00_.csv\n",
      "   ‚Ä¢ Model Consistency Analysis: model_consistency_analysis_comparison_v1.00_.csv\n",
      "   ‚Ä¢ Task Difficulty Analysis: task_difficulty_analysis_comparison_v1.00_.csv\n",
      "   ‚Ä¢ Visualization Plots: comparison/ subfolder\n",
      "\n",
      "================================================================================\n",
      "\t\t\t\tANALYSIS COMPLETE\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Create comprehensive summary report\n",
    "summary_report = {\n",
    "    'project_info': {\n",
    "        'name': 'Tennessee Eastman Process Analysis',\n",
    "        'version': VERSION,\n",
    "        'analysis_date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'total_notebooks': 6,\n",
    "        'completed_notebooks': 5  # Data prep, EDA, Classification, Anomaly, Merging\n",
    "    },\n",
    "    'data_summary': {\n",
    "        'dataset': 'Tennessee Eastman Process',\n",
    "        'features': 52,  # Assuming from typical TE dataset\n",
    "        'analysis_types': ['Multi-class Classification', 'Binary Anomaly Detection', 'Merged Fault Classification']\n",
    "    },\n",
    "    'model_summary': {\n",
    "        'total_models_trained': len(comparison_df['Model'].unique()) if not comparison_df.empty else 0,\n",
    "        'model_types': list(comparison_df['Model'].unique()) if not comparison_df.empty else [],\n",
    "        'best_overall_model': model_consistency.iloc[0]['Model'] if not comparison_df.empty else 'N/A',\n",
    "        'most_consistent_model': model_consistency.iloc[0]['Model'] if not comparison_df.empty else 'N/A'\n",
    "    },\n",
    "    'performance_summary': {\n",
    "        'highest_accuracy': comparison_df['Accuracy'].max() if not comparison_df.empty else 0,\n",
    "        'highest_f1_score': comparison_df['F1_Score'].max() if not comparison_df.empty else 0,\n",
    "        'average_accuracy_across_all': comparison_df['Accuracy'].mean() if not comparison_df.empty else 0,\n",
    "        'average_f1_across_all': comparison_df['F1_Score'].mean() if not comparison_df.empty else 0\n",
    "    },\n",
    "    'task_specific_summaries': {\n",
    "        'classification': classification_summary,\n",
    "        'anomaly_detection': anomaly_summary,\n",
    "        'merged_faults': merged_summary\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save comprehensive summary\n",
    "summary_path = os.path.join(OUTPUT_PATH, VERSION, f\"comprehensive_analysis_summary_v{VERSION}_.json\")\n",
    "with open(summary_path, 'w') as f:\n",
    "    json.dump(summary_report, f, indent=2, default=str)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\t\\tTENNESSEE EASTMAN PROCESS - COMPREHENSIVE ANALYSIS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüìä PROJECT INFORMATION:\")\n",
    "print(f\"   ‚Ä¢ Analysis Version: {summary_report['project_info']['version']}\")\n",
    "print(f\"   ‚Ä¢ Analysis Date: {summary_report['project_info']['analysis_date']}\")\n",
    "print(f\"   ‚Ä¢ Total Notebooks: {summary_report['project_info']['total_notebooks']}\")\n",
    "print(f\"   ‚Ä¢ Completed Analyses: {summary_report['project_info']['completed_notebooks']}\")\n",
    "\n",
    "print(f\"\\nüî¨ ANALYSIS SCOPE:\")\n",
    "print(f\"   ‚Ä¢ Dataset: {summary_report['data_summary']['dataset']}\")\n",
    "print(f\"   ‚Ä¢ Features: {summary_report['data_summary']['features']}\")\n",
    "print(f\"   ‚Ä¢ Analysis Types: {', '.join(summary_report['data_summary']['analysis_types'])}\")\n",
    "\n",
    "print(f\"\\nü§ñ MODEL SUMMARY:\")\n",
    "print(f\"   ‚Ä¢ Total Models Trained: {summary_report['model_summary']['total_models_trained']}\")\n",
    "print(f\"   ‚Ä¢ Model Types: {', '.join(summary_report['model_summary']['model_types'])}\")\n",
    "print(f\"   ‚Ä¢ Best Overall Model: {summary_report['model_summary']['best_overall_model']}\")\n",
    "\n",
    "print(f\"\\nüìà PERFORMANCE HIGHLIGHTS:\")\n",
    "print(f\"   ‚Ä¢ Highest Accuracy: {summary_report['performance_summary']['highest_accuracy']:.3f}\")\n",
    "print(f\"   ‚Ä¢ Highest F1-Score: {summary_report['performance_summary']['highest_f1_score']:.3f}\")\n",
    "print(f\"   ‚Ä¢ Average Accuracy: {summary_report['performance_summary']['average_accuracy_across_all']:.3f}\")\n",
    "print(f\"   ‚Ä¢ Average F1-Score: {summary_report['performance_summary']['average_f1_across_all']:.3f}\")\n",
    "\n",
    "print(f\"\\nüìÅ OUTPUT FILES GENERATED:\")\n",
    "print(f\"   ‚Ä¢ Location: {OUTPUT_PATH}/{VERSION}/\")\n",
    "print(f\"   ‚Ä¢ Comprehensive Summary: comprehensive_analysis_summary_v{VERSION}_.json\")\n",
    "print(f\"   ‚Ä¢ Cross-Task Comparison: cross_task_comparison_comparison_v{VERSION}_.csv\")\n",
    "print(f\"   ‚Ä¢ Best Models Summary: best_models_summary_comparison_v{VERSION}_.csv\")\n",
    "print(f\"   ‚Ä¢ Model Consistency Analysis: model_consistency_analysis_comparison_v{VERSION}_.csv\")\n",
    "print(f\"   ‚Ä¢ Task Difficulty Analysis: task_difficulty_analysis_comparison_v{VERSION}_.csv\")\n",
    "print(f\"   ‚Ä¢ Visualization Plots: comparison/ subfolder\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\t\\t\\t\\tANALYSIS COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c90fb6",
   "metadata": {},
   "source": [
    "## Key Insights and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "75249411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç KEY INSIGHTS:\n",
      "==================================================\n",
      "\n",
      "üí° RECOMMENDATIONS:\n",
      "==================================================\n",
      "‚Ä¢ For Production Deployment: Use the most consistent model across tasks\n",
      "‚Ä¢ For Specific Tasks: Use task-specific best performing models\n",
      "‚Ä¢ For Further Research: Focus on improving performance for the most challenging task\n",
      "‚Ä¢ Model Selection: Consider ensemble methods combining top performers\n",
      "‚Ä¢ Data Quality: Investigate why certain tasks are more challenging\n",
      "\n",
      "‚úÖ NEXT STEPS:\n",
      "==================================================\n",
      "1. Review individual notebook results for detailed analysis\n",
      "2. Consider hyperparameter tuning for best performing models\n",
      "3. Implement ensemble methods for improved performance\n",
      "4. Analyze feature importance across different tasks\n",
      "5. Consider domain-specific feature engineering\n",
      "6. Validate results with additional cross-validation\n",
      "7. Deploy selected models for real-time monitoring\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüîç KEY INSIGHTS:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "if not comparison_df.empty:\n",
    "    # Task-specific insights\n",
    "    tasks = comparison_df['Task'].unique()\n",
    "    \n",
    "    for task in tasks:\n",
    "        task_data = comparison_df[comparison_df['Task'] == task]\n",
    "        best_model = task_data.loc[task_data['Accuracy'].idxmax(), 'Model']\n",
    "        best_accuracy = task_data['Accuracy'].max()\n",
    "        worst_accuracy = task_data['Accuracy'].min()\n",
    "        \n",
    "        print(f\"\\n‚Ä¢ {task}:\")\n",
    "        print(f\"  - Best Model: {best_model} ({best_accuracy:.3f} accuracy)\")\n",
    "        print(f\"  - Performance Range: {worst_accuracy:.3f} - {best_accuracy:.3f}\")\n",
    "        print(f\"  - Performance Spread: {best_accuracy - worst_accuracy:.3f}\")\n",
    "    \n",
    "    # Overall insights\n",
    "    most_consistent = model_consistency.iloc[0]['Model']\n",
    "    print(f\"\\n‚Ä¢ Overall Best Performing Model: {most_consistent}\")\n",
    "    print(f\"‚Ä¢ Most Challenging Task: {task_difficulty.iloc[0]['Task']}\")\n",
    "    print(f\"‚Ä¢ Easiest Task: {task_difficulty.iloc[-1]['Task']}\")\n",
    "\n",
    "print(\"\\nüí° RECOMMENDATIONS:\")\n",
    "print(\"=\"*50)\n",
    "print(\"‚Ä¢ For Production Deployment: Use the most consistent model across tasks\")\n",
    "print(\"‚Ä¢ For Specific Tasks: Use task-specific best performing models\")\n",
    "print(\"‚Ä¢ For Further Research: Focus on improving performance for the most challenging task\")\n",
    "print(\"‚Ä¢ Model Selection: Consider ensemble methods combining top performers\")\n",
    "print(\"‚Ä¢ Data Quality: Investigate why certain tasks are more challenging\")\n",
    "\n",
    "print(\"\\n‚úÖ NEXT STEPS:\")\n",
    "print(\"=\"*50)\n",
    "print(\"1. Review individual notebook results for detailed analysis\")\n",
    "print(\"2. Consider hyperparameter tuning for best performing models\")\n",
    "print(\"3. Implement ensemble methods for improved performance\")\n",
    "print(\"4. Analyze feature importance across different tasks\")\n",
    "print(\"5. Consider domain-specific feature engineering\")\n",
    "print(\"6. Validate results with additional cross-validation\")\n",
    "print(\"7. Deploy selected models for real-time monitoring\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
