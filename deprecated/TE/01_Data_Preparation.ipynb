{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c93f6021",
   "metadata": {},
   "source": [
    "# Data Loading and Preparation\n",
    "\n",
    "This notebook handles data loading, preprocessing, and standardization for the Tennessee Eastman Process dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d3e6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "def check_python_version() -> None:\n",
    "    required_major: int = 3\n",
    "    required_minor: int = 11\n",
    "    current_version: tuple[int, int, int] = sys.version_info[:3]\n",
    "\n",
    "    if current_version[0] != required_major or current_version[1] != required_minor:\n",
    "        raise RuntimeError(\n",
    "            f\"Python {required_major}.{required_minor}.xx is required, \"\n",
    "            f\"but you are using {current_version[0]}.{current_version[1]}.{current_version[2]}\"\n",
    "        )\n",
    "\n",
    "check_python_version()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f832bd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import pyreadr\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88dc85f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Configuration\n",
    "VERSION = \"1.00\"\n",
    "OUTPUT_PATH = \"output\"\n",
    "TARGET_VARIABLE_COLUMN_NAME = \"faultNumber\"\n",
    "SIMULATION_RUN_COLUMN_NAME = \"simulationRun\"\n",
    "COLUMNS_TO_REMOVE = [\"simulationRun\", \"sample\"]\n",
    "SKIPED_FAULTS = []\n",
    "FAULTS_TO_BE_MERGED_TOGETHER = [3, 8, 9, 18, 15]\n",
    "MERGE_FAUTS_TO_NUMBER = 3\n",
    "FAULT_INJECTION_STARTING_POINT = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40d1f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataframe(df: pd.DataFrame, name: str, suffix: str = \"\") -> None:\n",
    "    \"\"\"Save a DataFrame to CSV with versioned filename.\"\"\"\n",
    "    timestamp: str = \"\"\n",
    "    base_dir: str = os.path.join(OUTPUT_PATH, \"data\")\n",
    "    os.makedirs(base_dir, exist_ok=True)\n",
    "\n",
    "    filename: str = f\"{name}_{suffix}_v{VERSION}_{timestamp}.csv\" if suffix else f\"{name}_v{VERSION}_{timestamp}.csv\"\n",
    "    filepath: str = os.path.join(base_dir, filename)\n",
    "\n",
    "    df.to_csv(filepath, index=True)\n",
    "    print(f\"Data saved: {filepath}\")\n",
    "\n",
    "def save_pickle(obj, name: str, suffix: str = \"\") -> None:\n",
    "    \"\"\"Save object as pickle file.\"\"\"\n",
    "    timestamp: str = \"\"\n",
    "    base_dir: str = os.path.join(OUTPUT_PATH, \"data\")\n",
    "    os.makedirs(base_dir, exist_ok=True)\n",
    "\n",
    "    filename: str = f\"{name}_{suffix}_v{VERSION}_{timestamp}.pkl\" if suffix else f\"{name}_v{VERSION}_{timestamp}.pkl\"\n",
    "    filepath: str = os.path.join(base_dir, filename)\n",
    "\n",
    "    with open(filepath, 'wb') as f:\n",
    "        pickle.dump(obj, f)\n",
    "    print(f\"Saved: {filepath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a746fb3",
   "metadata": {},
   "source": [
    "## Load Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9d5678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load .RData files\n",
    "fault_free_training_dict = pyreadr.read_r(\"data/TEP_FaultFree_Training.RData\")\n",
    "fault_free_testing_dict = pyreadr.read_r(\"data/TEP_FaultFree_Testing.RData\")\n",
    "faulty_training_dict = pyreadr.read_r(\"data/TEP_Faulty_Training.RData\")\n",
    "faulty_testing_dict = pyreadr.read_r(\"data/TEP_Faulty_Testing.RData\")\n",
    "\n",
    "# Extract DataFrames\n",
    "DF_FF_TRAINING_RAW = fault_free_training_dict[\"fault_free_training\"]\n",
    "DF_FF_TEST_RAW = fault_free_testing_dict[\"fault_free_testing\"]\n",
    "DF_F_TRAINING_RAW = faulty_training_dict[\"faulty_training\"]\n",
    "DF_F_TEST_RAW = faulty_testing_dict[\"faulty_testing\"]\n",
    "\n",
    "print(f\"Fault-free training: {DF_FF_TRAINING_RAW.shape}\")\n",
    "print(f\"Fault-free testing: {DF_FF_TEST_RAW.shape}\")\n",
    "print(f\"Faulty training: {DF_F_TRAINING_RAW.shape}\")\n",
    "print(f\"Faulty testing: {DF_F_TEST_RAW.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bda642c",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815b0668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip specified faults\n",
    "DF_F_TRAIN_SKIPPED_FAULTS = DF_F_TRAINING_RAW[~DF_F_TRAINING_RAW[TARGET_VARIABLE_COLUMN_NAME].isin(SKIPED_FAULTS)].reset_index(drop=True)\n",
    "DF_F_TEST_SKIPPED_FAULTS = DF_F_TEST_RAW[~DF_F_TEST_RAW[TARGET_VARIABLE_COLUMN_NAME].isin(SKIPED_FAULTS)].reset_index(drop=True)\n",
    "\n",
    "# Reduce data for development (keeping balanced)\n",
    "DF_FF_TRAINING_REDUCED = DF_FF_TRAINING_RAW[(DF_FF_TRAINING_RAW[SIMULATION_RUN_COLUMN_NAME] > 0) & (DF_FF_TRAINING_RAW[SIMULATION_RUN_COLUMN_NAME] < 2)].drop(columns=COLUMNS_TO_REMOVE, axis=1)\n",
    "DF_F_TRAINING_REDUCED = DF_F_TRAIN_SKIPPED_FAULTS[(DF_F_TRAIN_SKIPPED_FAULTS[SIMULATION_RUN_COLUMN_NAME] > 4) & (DF_F_TRAIN_SKIPPED_FAULTS[SIMULATION_RUN_COLUMN_NAME] < 6) &(DF_F_TRAIN_SKIPPED_FAULTS[\"sample\"] > FAULT_INJECTION_STARTING_POINT)].drop(columns=COLUMNS_TO_REMOVE, axis=1)\n",
    "\n",
    "\n",
    "DF_FF_TEST_REDUCED = DF_FF_TEST_RAW[(DF_FF_TEST_RAW[SIMULATION_RUN_COLUMN_NAME] > 2) & (DF_FF_TEST_RAW[SIMULATION_RUN_COLUMN_NAME] < 4)].drop(columns=COLUMNS_TO_REMOVE, axis=1)\n",
    "DF_F_TEST_REDUCED = DF_F_TEST_SKIPPED_FAULTS[(DF_F_TEST_SKIPPED_FAULTS[SIMULATION_RUN_COLUMN_NAME] > 5) & (DF_F_TEST_SKIPPED_FAULTS[SIMULATION_RUN_COLUMN_NAME] < 7) & (DF_F_TEST_SKIPPED_FAULTS[\"sample\"] > FAULT_INJECTION_STARTING_POINT)].drop(columns=COLUMNS_TO_REMOVE, axis=1)\n",
    "\n",
    "print(f\"Reduced fault-free training: {DF_FF_TRAINING_REDUCED.shape}\")\n",
    "print(f\"Reduced faulty training: {DF_F_TRAINING_REDUCED.shape}\")\n",
    "print(f\"Reduced fault-free testing: {DF_FF_TEST_REDUCED.shape}\")\n",
    "print(f\"Reduced faulty testing: {DF_F_TEST_REDUCED.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c426103f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_balance_difference(df1: pd.DataFrame, df2: pd.DataFrame, threshold: int = 100) -> None:\n",
    "    size_diff: int = abs(df1.shape[0] - df2.shape[0])\n",
    "    print(f\"Data difference: {size_diff}\")\n",
    "    if size_diff > threshold:\n",
    "        raise ValueError(f\"Data imbalance too large: difference = {size_diff} rows\")\n",
    "\n",
    "# Check balance\n",
    "check_balance_difference(DF_FF_TRAINING_REDUCED, DF_F_TRAINING_REDUCED.query(\"faultNumber == 1\"))\n",
    "check_balance_difference(DF_FF_TEST_REDUCED, DF_F_TEST_REDUCED.query(\"faultNumber == 1\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6ba62f",
   "metadata": {},
   "source": [
    "## Prepare Datasets for Different Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258452ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Supervised Classification Dataset\n",
    "DF_TRAINING_REDUCED_CONCATED = pd.concat([DF_FF_TRAINING_REDUCED, DF_F_TRAINING_REDUCED])\n",
    "DF_TEST_REDUCED_CONCATED = pd.concat([DF_FF_TEST_REDUCED, DF_F_TEST_REDUCED])\n",
    "\n",
    "# Standardize data\n",
    "sc = StandardScaler()\n",
    "sc.fit(DF_TRAINING_REDUCED_CONCATED.drop(columns=[TARGET_VARIABLE_COLUMN_NAME], axis=1))\n",
    "X_TRAIN = sc.transform(DF_TRAINING_REDUCED_CONCATED.drop(columns=[TARGET_VARIABLE_COLUMN_NAME], axis=1))\n",
    "Y_TRAIN_DF = DF_TRAINING_REDUCED_CONCATED[TARGET_VARIABLE_COLUMN_NAME]\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "Y_TRAIN = le.fit_transform(Y_TRAIN_DF)\n",
    "\n",
    "# Test data\n",
    "X_TEST_REDUCED = sc.transform(DF_TEST_REDUCED_CONCATED.drop(columns=[TARGET_VARIABLE_COLUMN_NAME], axis=1))\n",
    "Y_TEST_REDUCED_DF = DF_TEST_REDUCED_CONCATED[TARGET_VARIABLE_COLUMN_NAME]\n",
    "Y_TEST_REDUCED = le.transform(Y_TEST_REDUCED_DF)\n",
    "\n",
    "# One-hot encode\n",
    "encoder_1 = OneHotEncoder(sparse_output=False)\n",
    "Y_reshaped = DF_TRAINING_REDUCED_CONCATED[TARGET_VARIABLE_COLUMN_NAME].to_numpy().reshape(-1, 1)\n",
    "Y_ENC_TRAIN = encoder_1.fit_transform(Y_reshaped)\n",
    "\n",
    "Y_test_reshaped = DF_TEST_REDUCED_CONCATED[TARGET_VARIABLE_COLUMN_NAME].to_numpy().reshape(-1, 1)\n",
    "Y_ENC_TEST_REDUCED = encoder_1.transform(Y_test_reshaped)\n",
    "\n",
    "print(f\"Training features shape: {X_TRAIN.shape}\")\n",
    "print(f\"Training labels shape: {Y_TRAIN.shape}\")\n",
    "print(f\"Test features shape: {X_TEST_REDUCED.shape}\")\n",
    "print(f\"Test labels shape: {Y_TEST_REDUCED.shape}\")\n",
    "print(f\"Unique fault numbers in training: {DF_TRAINING_REDUCED_CONCATED[TARGET_VARIABLE_COLUMN_NAME].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d30adfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Anomaly Detection Dataset (binary: normal vs fault)\n",
    "# Training data\n",
    "X_INCONTROL_TRAIN_REDUCED_DF = DF_FF_TRAINING_REDUCED.drop(columns=[TARGET_VARIABLE_COLUMN_NAME], axis=1)\n",
    "sc_anomaly = StandardScaler()\n",
    "sc_anomaly.fit(X_INCONTROL_TRAIN_REDUCED_DF)\n",
    "X_INCONTROL_TRAIN_REDUCED = sc_anomaly.transform(X_INCONTROL_TRAIN_REDUCED_DF)\n",
    "\n",
    "# Binary labels for anomaly detection\n",
    "Y_TRAIN_ANOMALY_REDUCED_DF = DF_TRAINING_REDUCED_CONCATED[TARGET_VARIABLE_COLUMN_NAME].apply(lambda x: 0 if x == 0 else 1)\n",
    "encoder_2 = OneHotEncoder(sparse_output=False)\n",
    "Y_reshaped_anomaly = Y_TRAIN_ANOMALY_REDUCED_DF.to_numpy().reshape(-1, 1)\n",
    "Y_ENC_ANOMALY_TRAIN_REDUCED = encoder_2.fit_transform(Y_reshaped_anomaly)\n",
    "\n",
    "# Test data\n",
    "X_INCONTROL_TEST_REDUCED_DF = DF_FF_TEST_REDUCED.drop(columns=[TARGET_VARIABLE_COLUMN_NAME], axis=1)\n",
    "X_INCONTROL_TEST_REDUCED = sc_anomaly.transform(X_INCONTROL_TEST_REDUCED_DF)\n",
    "\n",
    "X_OUT_OF_CONTROL_TEST_REDUCED_DF = DF_F_TEST_REDUCED.drop(columns=[TARGET_VARIABLE_COLUMN_NAME], axis=1)\n",
    "X_OUT_OF_CONTROL_TEST_REDUCED = sc_anomaly.transform(X_OUT_OF_CONTROL_TEST_REDUCED_DF)\n",
    "\n",
    "Y_TEST_ANOMALY_REDUCED_DF = DF_TEST_REDUCED_CONCATED[TARGET_VARIABLE_COLUMN_NAME].apply(lambda x: 0 if x == 0 else 1)\n",
    "Y_test_reshaped_anomaly = Y_TEST_ANOMALY_REDUCED_DF.to_numpy().reshape(-1, 1)\n",
    "Y_ENC_ANOMALY_TEST_REDUCED = encoder_2.transform(Y_test_reshaped_anomaly)\n",
    "\n",
    "print(f\"Anomaly detection - In-control training: {X_INCONTROL_TRAIN_REDUCED.shape}\")\n",
    "print(f\"Anomaly detection - Test features: {X_TEST_REDUCED.shape}\")\n",
    "print(f\"Anomaly detection - Binary labels: {Y_TRAIN_ANOMALY_REDUCED_DF.value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ea05df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Merged Faults Dataset\n",
    "Y_TRAIN_MERGED_FAULTS_REDUCED_DF = DF_TRAINING_REDUCED_CONCATED[TARGET_VARIABLE_COLUMN_NAME].apply(\n",
    "    lambda x: MERGE_FAUTS_TO_NUMBER if x in FAULTS_TO_BE_MERGED_TOGETHER else x\n",
    ")\n",
    "encoder_3 = OneHotEncoder(sparse_output=False)\n",
    "Y_reshaped_train_merged = Y_TRAIN_MERGED_FAULTS_REDUCED_DF.to_numpy().reshape(-1, 1)\n",
    "Y_ENC_MERGED_TRAIN_REDUCED = encoder_3.fit_transform(Y_reshaped_train_merged)\n",
    "\n",
    "Y_TEST_MERGED_FAULTS_DF = DF_TEST_REDUCED_CONCATED[TARGET_VARIABLE_COLUMN_NAME].apply(\n",
    "    lambda x: MERGE_FAUTS_TO_NUMBER if x in FAULTS_TO_BE_MERGED_TOGETHER else x\n",
    ")\n",
    "Y_reshaped_test_merged = Y_TEST_MERGED_FAULTS_DF.to_numpy().reshape(-1, 1)\n",
    "Y_ENC_MERGED_TEST_REDUCED = encoder_3.transform(Y_reshaped_test_merged)\n",
    "\n",
    "print(f\"Merged faults - Unique values in training: {np.unique(Y_reshaped_train_merged)}\")\n",
    "print(f\"Merged faults - Unique values in testing: {np.unique(Y_reshaped_test_merged)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3206a1a",
   "metadata": {},
   "source": [
    "## Export Prepared Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec21e10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save raw dataframes\n",
    "save_dataframe(DF_FF_TRAINING_RAW, \"raw_fault_free_training\")\n",
    "save_dataframe(DF_FF_TEST_RAW, \"raw_fault_free_test\")\n",
    "save_dataframe(DF_F_TRAINING_RAW, \"raw_faulty_training\")\n",
    "save_dataframe(DF_F_TEST_RAW, \"raw_faulty_test\")\n",
    "\n",
    "# Save reduced dataframes\n",
    "save_dataframe(DF_FF_TRAINING_REDUCED, \"fault_free_training_reduced\")\n",
    "save_dataframe(DF_FF_TEST_REDUCED, \"fault_free_test_reduced\")\n",
    "save_dataframe(DF_F_TRAINING_REDUCED, \"faulty_training_reduced\")\n",
    "save_dataframe(DF_F_TEST_REDUCED, \"faulty_test_reduced\")\n",
    "\n",
    "# Save combined dataframes\n",
    "save_dataframe(DF_TRAINING_REDUCED_CONCATED, \"training_combined\")\n",
    "save_dataframe(DF_TEST_REDUCED_CONCATED, \"test_combined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d917b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed arrays for classification\n",
    "data_classification = {\n",
    "    'X_TRAIN': X_TRAIN,\n",
    "    'Y_TRAIN': Y_TRAIN,\n",
    "    'Y_TRAIN_DF': Y_TRAIN_DF,\n",
    "    'X_TEST_REDUCED': X_TEST_REDUCED,\n",
    "    'Y_TEST_REDUCED': Y_TEST_REDUCED,\n",
    "    'Y_TEST_REDUCED_DF': Y_TEST_REDUCED_DF,\n",
    "    'Y_ENC_TRAIN': Y_ENC_TRAIN,\n",
    "    'Y_ENC_TEST_REDUCED': Y_ENC_TEST_REDUCED,\n",
    "    'scaler': sc,\n",
    "    'label_encoder': le,\n",
    "    'onehot_encoder': encoder_1\n",
    "}\n",
    "save_pickle(data_classification, \"data_classification\")\n",
    "\n",
    "# Save processed arrays for anomaly detection\n",
    "data_anomaly = {\n",
    "    'X_INCONTROL_TRAIN_REDUCED': X_INCONTROL_TRAIN_REDUCED,\n",
    "    'X_INCONTROL_TEST_REDUCED': X_INCONTROL_TEST_REDUCED,\n",
    "    'X_OUT_OF_CONTROL_TEST_REDUCED': X_OUT_OF_CONTROL_TEST_REDUCED,\n",
    "    'Y_TRAIN_ANOMALY_REDUCED_DF': Y_TRAIN_ANOMALY_REDUCED_DF,\n",
    "    'Y_TEST_ANOMALY_REDUCED_DF': Y_TEST_ANOMALY_REDUCED_DF,\n",
    "    'Y_ENC_ANOMALY_TRAIN_REDUCED': Y_ENC_ANOMALY_TRAIN_REDUCED,\n",
    "    'Y_ENC_ANOMALY_TEST_REDUCED': Y_ENC_ANOMALY_TEST_REDUCED,\n",
    "    'scaler_anomaly': sc_anomaly,\n",
    "    'onehot_encoder_anomaly': encoder_2\n",
    "}\n",
    "save_pickle(data_anomaly, \"data_anomaly\")\n",
    "\n",
    "# Save processed arrays for merged faults\n",
    "data_merged = {\n",
    "    'Y_TRAIN_MERGED_FAULTS_REDUCED_DF': Y_TRAIN_MERGED_FAULTS_REDUCED_DF,\n",
    "    'Y_TEST_MERGED_FAULTS_DF': Y_TEST_MERGED_FAULTS_DF,\n",
    "    'Y_ENC_MERGED_TRAIN_REDUCED': Y_ENC_MERGED_TRAIN_REDUCED,\n",
    "    'Y_ENC_MERGED_TEST_REDUCED': Y_ENC_MERGED_TEST_REDUCED,\n",
    "    'onehot_encoder_merged': encoder_3\n",
    "}\n",
    "save_pickle(data_merged, \"data_merged\")\n",
    "\n",
    "print(\"All data preparation complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
