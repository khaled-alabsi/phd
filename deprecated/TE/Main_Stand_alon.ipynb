{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b468614",
   "metadata": {},
   "source": [
    "# Import librarys and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1f1929",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ba96f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "\n",
    "def check_python_version() -> None:\n",
    "    required_major: int = 3\n",
    "    required_minor: int = 11\n",
    "    current_version: tuple[int, int, int] = sys.version_info[:3]\n",
    "\n",
    "    if current_version[0] != required_major or current_version[1] != required_minor:\n",
    "        raise RuntimeError(\n",
    "            f\"Python {required_major}.{required_minor}.xx is required, \"\n",
    "            f\"but you are using {current_version[0]}.{current_version[1]}.{current_version[2]}\"\n",
    "        )\n",
    "\n",
    "\n",
    "check_python_version()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb7c7a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T14:12:31.617631Z",
     "iopub.status.busy": "2023-11-01T14:12:31.617224Z",
     "iopub.status.idle": "2023-11-01T14:12:34.183518Z",
     "shell.execute_reply": "2023-11-01T14:12:34.181750Z"
    },
    "papermill": {
     "duration": 2.584105,
     "end_time": "2023-11-01T14:12:34.186306",
     "exception": false,
     "start_time": "2023-11-01T14:12:31.602201",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import important liberies Important!! usse  python 3.11 or below\n",
    "import pandas as pd\n",
    "import pyreadr\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "from tabulate import tabulate\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313dacea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting .rData into dataframe\n",
    "fault_free_training_dict = pyreadr.read_r(\"data/TEP_FaultFree_Training.RData\")\n",
    "fault_free_testing_dict = pyreadr.read_r(\"data/TEP_FaultFree_Testing.RData\")\n",
    "\n",
    "faulty_training_dict = pyreadr.read_r(\"data/TEP_Faulty_Training.RData\")\n",
    "faulty_testing_dict = pyreadr.read_r(\"data/TEP_Faulty_Testing.RData\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7555ea58",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_FF_TRAINING_RAW = fault_free_training_dict[\"fault_free_training\"]\n",
    "DF_FF_TEST_RAW = fault_free_testing_dict[\"fault_free_testing\"]\n",
    "\n",
    "DF_F_TRAINING_RAW = faulty_training_dict[\"faulty_training\"]\n",
    "DF_F_TEST_RAW = faulty_testing_dict[\"faulty_testing\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e49990",
   "metadata": {},
   "source": [
    "# Global Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5a1d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "VERSION = \"1.00\"\n",
    "OUTPUT_PATH=\"output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89b8c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Names to use in plots and tables\n",
    "\n",
    "XGBOOST = \"XGBoost\"\n",
    "RANDOM_FOREST = \"Random Forest\"\n",
    "NEURAL_NET= \"Neural Net\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503507f5",
   "metadata": {},
   "source": [
    "# Custom functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593a843a",
   "metadata": {},
   "source": [
    "## Compute_first_detection_delay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd6cd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute_first_detection_delay\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Union, List\n",
    "\n",
    "\n",
    "def compute_first_detection_delay(\n",
    "        y_true: Union[List[int], np.ndarray],\n",
    "        y_pred: Union[List[int], np.ndarray]) -> pd.DataFrame:\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    fault_labels = sorted(np.unique(y_true[y_true > 0]))\n",
    "    results = []\n",
    "\n",
    "    for fault in fault_labels:\n",
    "        fault_mask = y_true == fault\n",
    "        fault_indices = np.where(fault_mask)[0]  # Get indices of fault samples\n",
    "        if len(fault_indices) == 0:\n",
    "            delay = None\n",
    "            print(\"No fault samples.\")\n",
    "        else:\n",
    "            start_index: int = fault_indices[\n",
    "                0]  # Get the first index of the fault\n",
    "            detection_indices = np.where(\n",
    "                (y_pred == fault) & (np.arange(len(y_pred)) >= start_index)\n",
    "            )[0]  # Get indices where the prediction matches the fault and is after the start index\n",
    "            delay = (detection_indices[0] -\n",
    "                     start_index if len(detection_indices) > 0 else None)\n",
    "\n",
    "        results.append({\n",
    "            \"Fault\":\n",
    "            fault,\n",
    "            \"First Detection Delay\":\n",
    "            delay if (delay is not None and delay >= 0) else np.nan,\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a64268",
   "metadata": {},
   "source": [
    "## Data export functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7f285d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "VERSION: str = VERSION if \"VERSION\" in globals() and VERSION else \"default\"\n",
    "OUTPUT_PATH: str = OUTPUT_PATH if \"OUTPUT_PATH\" in globals(\n",
    ") and OUTPUT_PATH else \"output\"\n",
    "\n",
    "\n",
    "def save_plot(plot_name: str, suffix: str = \"\", plot_path:str=\"default\") -> None:\n",
    "    \"\"\"\n",
    "    Save current matplotlib figure with a structured filename.\n",
    "\n",
    "    Args:\n",
    "        plot_name (str): Base name for the plot.\n",
    "        suffix (str): Optional suffix, e.g., class ID or 'average'.\n",
    "    \"\"\"\n",
    "    #timestamp: str = datetime.now().strftime(\"%S\")\n",
    "    timestamp: str = \"\"\n",
    "    base_dir: str = os.path.join(OUTPUT_PATH, VERSION, plot_path)\n",
    "    os.makedirs(base_dir, exist_ok=True)\n",
    "\n",
    "    filename: str = f\"{plot_name}_{suffix}_v{VERSION}_{timestamp}.png\" if suffix else f\"{plot_name}_v{VERSION}_{timestamp}.png\"\n",
    "    filepath: str = os.path.join(base_dir, filename)\n",
    "\n",
    "    plt.savefig(filepath, bbox_inches=\"tight\")\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da3be40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataframe(df: pd.DataFrame, name: str, suffix: str = \"\") -> None:\n",
    "    \"\"\"\n",
    "    Save a DataFrame to CSV with versioned and timestamped filename.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame to save.\n",
    "        name (str): Base name for the file.\n",
    "        suffix (str): Optional suffix (e.g. class id, 'summary').\n",
    "    \"\"\"\n",
    "    #timestamp: str = datetime.now().strftime(\"%S\")\n",
    "    timestamp: str = \"\"\n",
    "    base_dir: str = os.path.join(OUTPUT_PATH, VERSION)\n",
    "    os.makedirs(base_dir, exist_ok=True)\n",
    "\n",
    "    filename: str = f\"{name}_{suffix}_v{VERSION}_{timestamp}.csv\" if suffix else f\"{name}_v{VERSION}_{timestamp}.csv\"\n",
    "    filepath: str = os.path.join(base_dir, filename)\n",
    "\n",
    "    df.to_csv(filepath, index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1199022e",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91347a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_VARIABLE_COLUMN_NAME = \"faultNumber\"\n",
    "SIMULATION_RUN_COLUMN_NAME = \"simulationRun\"\n",
    "COLUMNS_TO_REMOVE = [\"simulationRun\", \"sample\"]\n",
    "SKIPED_FAULTS = []\n",
    "FAULTS_TO_BE_MERGED_TOGETHER = [3, 8,9,18, 15]\n",
    "MERGE_FAUTS_TO_NUMBER = 3\n",
    "FAULT_INJECTION_STARTING_POINT = 25\n",
    "\n",
    "DF_F_TRAIN_SKIPPED_FAULTS = DF_F_TRAINING_RAW[~DF_F_TRAINING_RAW[TARGET_VARIABLE_COLUMN_NAME].isin(SKIPED_FAULTS)].reset_index(drop=True)\n",
    "DF_F_TEST_SKIPPED_FAULTS = DF_F_TEST_RAW[~DF_F_TEST_RAW[TARGET_VARIABLE_COLUMN_NAME].isin(SKIPED_FAULTS)].reset_index(drop=True)\n",
    "\n",
    "# **Reduce Training and test data for simplicity during development and testing**\n",
    "# **** THE DATA SHOULD STAY BALANCED !!!! *****\n",
    "\n",
    "# reduce training data\n",
    "DF_FF_TRAINING_REDUCED = DF_FF_TRAINING_RAW[(DF_FF_TRAINING_RAW[SIMULATION_RUN_COLUMN_NAME] > 0) & (DF_FF_TRAINING_RAW[SIMULATION_RUN_COLUMN_NAME] < 2)].drop(columns=COLUMNS_TO_REMOVE, axis=1)\n",
    "DF_F_TRAINING_REDUCED = DF_F_TRAIN_SKIPPED_FAULTS[(DF_F_TRAIN_SKIPPED_FAULTS[SIMULATION_RUN_COLUMN_NAME] > 4 )& (DF_F_TRAIN_SKIPPED_FAULTS[SIMULATION_RUN_COLUMN_NAME] < 6) &(DF_F_TRAIN_SKIPPED_FAULTS[\"sample\"] > FAULT_INJECTION_STARTING_POINT)].drop(columns=COLUMNS_TO_REMOVE, axis=1)\n",
    "\n",
    "# reduce test data\n",
    "DF_FF_TEST_REDUCED = DF_FF_TEST_RAW[(DF_FF_TEST_RAW[SIMULATION_RUN_COLUMN_NAME] > 2) & (DF_FF_TEST_RAW[SIMULATION_RUN_COLUMN_NAME] < 4)].drop(columns=COLUMNS_TO_REMOVE, axis=1)\n",
    "DF_F_TEST_REDUCED = DF_F_TEST_SKIPPED_FAULTS[(DF_F_TEST_SKIPPED_FAULTS[SIMULATION_RUN_COLUMN_NAME] > 5)& (DF_F_TEST_SKIPPED_FAULTS[SIMULATION_RUN_COLUMN_NAME] < 7) &(DF_F_TEST_SKIPPED_FAULTS[\"sample\"] > FAULT_INJECTION_STARTING_POINT)].drop(columns=COLUMNS_TO_REMOVE, axis=1)\n",
    "\n",
    "DF_F_TRAINING_REDUCED.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9848f524",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_balance_difference(df1: pd.DataFrame, df2: pd.DataFrame, threshold: int = 100) -> None:\n",
    "    size_diff: int = abs(df1.shape[0] - df2.shape[0])\n",
    "    print(\"Data differen is: \", size_diff)\n",
    "    if size_diff > threshold:\n",
    "        raise ValueError(f\"Data imbalance too large: difference = {size_diff} rows\")\n",
    "\n",
    "\n",
    "check_balance_difference(DF_FF_TRAINING_REDUCED,DF_F_TRAINING_REDUCED.query(\"faultNumber == 1\"))\n",
    "check_balance_difference(DF_FF_TEST_REDUCED,DF_F_TEST_REDUCED.query(\"faultNumber == 1\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc1e971",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(DF_FF_TRAINING_REDUCED.shape)\n",
    "print(DF_F_TRAINING_REDUCED.query(\"faultNumber == 1\").shape) # the output of this line should be close to df_ff_training_reduced.shape, so the data is balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695bcb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for Supervised training and testing\n",
    "\n",
    "DF_TRAINING_REDUCED_CONCATED = pd.concat([DF_FF_TRAINING_REDUCED, DF_F_TRAINING_REDUCED])\n",
    "DF_TEST_REDUCED_CONCATED = pd.concat([DF_FF_TEST_REDUCED, DF_F_TEST_REDUCED])\n",
    "\n",
    "\n",
    "# Standardize the data: It centers the data around 0 and scales it based on standard deviation.\n",
    "sc = StandardScaler()\n",
    "sc.fit(DF_TRAINING_REDUCED_CONCATED.drop(columns=[TARGET_VARIABLE_COLUMN_NAME],axis=1))\n",
    "X_TRAIN = sc.transform(DF_TRAINING_REDUCED_CONCATED.drop(columns=[TARGET_VARIABLE_COLUMN_NAME],axis=1))\n",
    "Y_TRAIN_DF = DF_TRAINING_REDUCED_CONCATED[TARGET_VARIABLE_COLUMN_NAME]\n",
    "\n",
    "# Encode the target variable: -LabelEncoder() Takes a list/array of categorical labels (e.g., ['shift', 'trend', 'none']), -Assigns a unique integer to each category (e.g., ['none' → 0, 'shift' → 1, 'trend' → 2]), -Returns a NumPy array of integers corresponding to the original labels\n",
    "le = LabelEncoder()\n",
    "Y_TRAIN = le.fit_transform(Y_TRAIN_DF)\n",
    "\n",
    "# Set the features and target variable for testing\n",
    "X_TEST_REDUCED = sc.transform(DF_TEST_REDUCED_CONCATED.drop(columns=[TARGET_VARIABLE_COLUMN_NAME], axis=1))\n",
    "Y_TEST_REDUCED_DF = DF_TEST_REDUCED_CONCATED[TARGET_VARIABLE_COLUMN_NAME]\n",
    "\n",
    "# Encode the target variable for testing\n",
    "Y_TEST_REDUCED = le.fit_transform(Y_TEST_REDUCED_DF)\n",
    "\n",
    "# One-hot encode the target variable\n",
    "encoder_1 = OneHotEncoder(sparse_output=False)\n",
    "Y_reshabed = (DF_TRAINING_REDUCED_CONCATED[TARGET_VARIABLE_COLUMN_NAME].to_numpy().reshape(-1, 1))\n",
    "# .fit_transform() fits the encoder to the data and then transforms it.\n",
    "# Use this on the training data to learn the encoding mapping and apply it.\n",
    "Y_ENC_TRAIN = encoder_1.fit_transform(Y_reshabed)\n",
    "\n",
    "# .transform() only applies the learned encoding to new data.\n",
    "# Use this on test data to encode using the mapping learned from training data.\n",
    "Y_test_reshabed = (DF_TEST_REDUCED_CONCATED[TARGET_VARIABLE_COLUMN_NAME].to_numpy().reshape(-1, 1))\n",
    "Y_ENC_TEST_REDUCED = encoder_1.transform(Y_test_reshabed)\n",
    "\n",
    "print(\n",
    "    \"Training Data with the following Fault Numbers:\",\n",
    "    DF_TEST_REDUCED_CONCATED[TARGET_VARIABLE_COLUMN_NAME].unique(),\n",
    ")\n",
    "DF_TRAINING_REDUCED_CONCATED.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ba1c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of samples in each class\n",
    "class_counts = DF_TEST_REDUCED_CONCATED[\n",
    "    TARGET_VARIABLE_COLUMN_NAME].value_counts()\n",
    "print(\"Class counts in training data:\")\n",
    "print(class_counts)\n",
    "# summe all samples\n",
    "print(\"Total samples in training data:\", class_counts.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c732e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the data Unsupervised learning\n",
    "\n",
    "# TRAIN data\n",
    "\n",
    "# X\n",
    "X_INCONTROL_TRAIN_REDUCED_DF = DF_FF_TRAINING_REDUCED.drop(columns=[TARGET_VARIABLE_COLUMN_NAME], axis=1)\n",
    "sc = StandardScaler()\n",
    "sc.fit(X_INCONTROL_TRAIN_REDUCED_DF)\n",
    "X_INCONTROL_TRAIN_REDUCED = sc.transform(X_INCONTROL_TRAIN_REDUCED_DF)\n",
    "\n",
    "# Y\n",
    "Y_TRAIN_ANOMALY_REDUCED_DF = DF_TRAINING_REDUCED_CONCATED[TARGET_VARIABLE_COLUMN_NAME].apply(lambda x: 0 if x == 0 else 1) ## change target variable to only 2 classes: 0 and 1: 0 is in control , bigger than 0 is faulty\n",
    "encoder_2 = OneHotEncoder(sparse_output=False)\n",
    "Y_reshabed = Y_TRAIN_ANOMALY_REDUCED_DF.to_numpy().reshape(-1, 1)\n",
    "Y_ENC_ANOMALY_TRAIN_REDUCED = encoder_2.fit_transform(Y_reshabed)\n",
    "\n",
    "\n",
    "\n",
    "# Test data\n",
    "\n",
    "# X\n",
    "X_INCONTROL_TEST_REDUCED_DF = DF_FF_TEST_REDUCED.drop( columns=[TARGET_VARIABLE_COLUMN_NAME], axis=1)\n",
    "sc = StandardScaler()\n",
    "sc.fit(X_INCONTROL_TEST_REDUCED_DF)\n",
    "X_INCONTROL_TEST_REDUCED = sc.transform(X_INCONTROL_TEST_REDUCED_DF)\n",
    "\n",
    "X_OUT_OF_CONTROL_TEST_REDUCED_DF = DF_F_TEST_REDUCED.drop(columns=[TARGET_VARIABLE_COLUMN_NAME], axis=1)\n",
    "sc = StandardScaler()\n",
    "sc.fit(X_OUT_OF_CONTROL_TEST_REDUCED_DF)\n",
    "X_OUT_OF_CONTROL_TEST_REDUCED = sc.transform(X_OUT_OF_CONTROL_TEST_REDUCED_DF)\n",
    "\n",
    "# Y\n",
    "Y_TEST_ANOMALY_REDUCED_DF = DF_TEST_REDUCED_CONCATED[TARGET_VARIABLE_COLUMN_NAME].apply(lambda x: 0 if x == 0 else 1) ## change target variable to only 2 classes: 0 and 1: 0 is in control , bigger than 0 is faulty\n",
    "Y_test_reshabed = Y_TEST_ANOMALY_REDUCED_DF.to_numpy().reshape(-1, 1)\n",
    "Y_ENC_ANOMALY_TEST_REDUCED = encoder_2.transform(Y_test_reshabed)\n",
    "\n",
    "y_test_binary = Y_test_reshabed.ravel().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5456193d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge hard faut together to a new faut, like faut 3,5,15 to new faut 3 or some other number like 40\n",
    "\n",
    "# Y Train\n",
    "Y_TRAIN_MERGED_FAULTS_REDUCED_DF = DF_TRAINING_REDUCED_CONCATED[TARGET_VARIABLE_COLUMN_NAME].apply(lambda x: 3 if x in FAULTS_TO_BE_MERGED_TOGETHER else x)\n",
    "encoder_3 = OneHotEncoder(sparse_output=False)\n",
    "Y_reshabed_train = Y_TRAIN_MERGED_FAULTS_REDUCED_DF.to_numpy().reshape(-1, 1)\n",
    "Y_ENC_MERGED_TRAIN_REDUCED = encoder_3.fit_transform(Y_reshabed_train)\n",
    "\n",
    "# Y Test\n",
    "Y_TEST_MERGED_FAULTS_DF = DF_TEST_REDUCED_CONCATED[TARGET_VARIABLE_COLUMN_NAME].apply(lambda x: 3 if x in FAULTS_TO_BE_MERGED_TOGETHER else x)\n",
    "Y_reshabed_test = Y_TEST_MERGED_FAULTS_DF.to_numpy().reshape(-1, 1)\n",
    "Y_ENC_MERGED_TEST_REDUCED = encoder_3.transform(Y_reshabed_test)\n",
    "\n",
    "# The output should not have the merged fault, like 15, it is merged to 3, or what ever it is configured for\n",
    "print(np.unique(Y_reshabed_train))\n",
    "print(np.unique(Y_reshabed_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8950f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_F_TEST_REDUCED.query(\"faultNumber == 2\").shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59d6e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the data for EDA\n",
    "\n",
    "# Train\n",
    "sc = StandardScaler()\n",
    "sc.fit(DF_F_TRAINING_RAW.drop(columns=[TARGET_VARIABLE_COLUMN_NAME], axis=1))\n",
    "X_F_TRAIN = sc.transform(DF_F_TRAINING_RAW.drop(columns=[TARGET_VARIABLE_COLUMN_NAME], axis=1))\n",
    "\n",
    "Y_F_TRAIN = DF_F_TRAINING_RAW[TARGET_VARIABLE_COLUMN_NAME].to_numpy()\n",
    "le = LabelEncoder()\n",
    "Y_F_TRAIN = le.fit_transform(Y_F_TRAIN)\n",
    "\n",
    "# Test\n",
    "sc = StandardScaler()\n",
    "sc.fit(DF_FF_TRAINING_RAW.drop(columns=[TARGET_VARIABLE_COLUMN_NAME], axis=1))\n",
    "X_FF_TRAIN = sc.transform(DF_FF_TRAINING_RAW.drop(columns=[TARGET_VARIABLE_COLUMN_NAME], axis=1))\n",
    "Y_FF_TRAIN = DF_FF_TRAINING_RAW[TARGET_VARIABLE_COLUMN_NAME].to_numpy()\n",
    "le = LabelEncoder()\n",
    "Y_FF_TRAIN = le.fit_transform(Y_FF_TRAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59d6e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_F_TRAINING_RAW.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e288344b",
   "metadata": {
    "papermill": {
     "duration": 0.01139,
     "end_time": "2023-11-01T14:12:34.209726",
     "exception": false,
     "start_time": "2023-11-01T14:12:34.198336",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade7197c",
   "metadata": {},
   "source": [
    "## Metadata overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db7835e",
   "metadata": {},
   "source": [
    "**The TEP variables (columns 4 to 55) were sampled every 3 minutes for a total duration of 25 hours and 48 hours respectively.\n",
    "The faults were introduced 1 hour into the Faulty Training\n",
    "and 8 hours into Faulty Testing datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3d005b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from scipy.stats import skew, kurtosis\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "def calculate_statistics_per_fault_run(\n",
    "        df_faulty: pd.DataFrame, df_normal: pd.DataFrame) -> pd.DataFrame:\n",
    "    base_dir: str = os.path.join(OUTPUT_PATH, VERSION, \"fault_statistics.csv\")\n",
    "    output_path: Path = Path(base_dir)\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if output_path.exists():\n",
    "        return pd.read_csv(output_path)\n",
    "\n",
    "    features: list[str] = [\n",
    "        col for col in df_faulty.columns\n",
    "        if col not in [TARGET_VARIABLE_COLUMN_NAME, SIMULATION_RUN_COLUMN_NAME, \"time\", \"sample\"]\n",
    "    ]\n",
    "\n",
    "    def compute_stats(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        rows: list[dict] = []\n",
    "        grouped = df.groupby([TARGET_VARIABLE_COLUMN_NAME, SIMULATION_RUN_COLUMN_NAME])\n",
    "        for (fault, run), group in grouped:\n",
    "            for feature in features:\n",
    "                values: pd.Series = group[feature].dropna()\n",
    "                if values.empty:\n",
    "                    continue\n",
    "                rows.append({\n",
    "                    TARGET_VARIABLE_COLUMN_NAME:\n",
    "                    fault,\n",
    "                    SIMULATION_RUN_COLUMN_NAME:\n",
    "                    run,\n",
    "                    \"feature\":\n",
    "                    feature,\n",
    "                    \"mean\":\n",
    "                    values.mean(),\n",
    "                    \"std\":\n",
    "                    values.std(),\n",
    "                    \"skewness\":\n",
    "                    skew(values),\n",
    "                    \"kurtosis\":\n",
    "                    kurtosis(values),\n",
    "                    \"autocorr_1\":\n",
    "                    values.autocorr(lag=1) if len(values) > 1 else np.nan,\n",
    "                })\n",
    "        return pd.DataFrame(rows)\n",
    "\n",
    "    df_all: pd.DataFrame = pd.concat([df_normal, df_faulty], ignore_index=True)\n",
    "    result_df: pd.DataFrame = compute_stats(df_all)\n",
    "    result_df.to_csv(output_path, index=False)\n",
    "    save_dataframe(df=result_df, name=\"fault_statistics\")\n",
    "    return result_df\n",
    "\n",
    "\n",
    "# Reduce data for speed\n",
    "statics_f_df = DF_F_TRAINING_RAW[(DF_F_TRAINING_RAW[SIMULATION_RUN_COLUMN_NAME] >= 1)\n",
    "                             & (DF_F_TRAINING_RAW[SIMULATION_RUN_COLUMN_NAME] < 4)]\n",
    "statics_ff_df = DF_FF_TRAINING_RAW[(DF_FF_TRAINING_RAW[SIMULATION_RUN_COLUMN_NAME] >= 1)\n",
    "                               & (DF_FF_TRAINING_RAW[SIMULATION_RUN_COLUMN_NAME] < 4)]\n",
    "\n",
    "stats_df = calculate_statistics_per_fault_run(df_faulty=statics_f_df,\n",
    "                                              df_normal=statics_ff_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f2e964",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df.query(\"faultNumber == 3 and simulationRun == 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df8cc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac2e82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_fault3 = set(\n",
    "    stats_df.query(\"faultNumber == 3 and simulationRun == 1\")[\"feature\"])\n",
    "features_fault0 = set(\n",
    "    stats_df.query(\"faultNumber == 0 and simulationRun == 1\")[\"feature\"])\n",
    "\n",
    "missing_in_0 = features_fault3 - features_fault0\n",
    "missing_in_3 = features_fault0 - features_fault3\n",
    "\n",
    "print(\"Missing in fault 0:\", missing_in_0)\n",
    "print(\"Missing in fault 3:\", missing_in_3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d525cf11",
   "metadata": {},
   "source": [
    "## Fault injection point analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d040ceb",
   "metadata": {},
   "source": [
    "## Boxplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b43e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting boxplots for all features\n",
    "\n",
    "faultNumber = 3\n",
    "simulationRun = 10\n",
    "df_selected_f_data = DF_F_TRAINING_RAW[\n",
    "    (DF_F_TRAINING_RAW[TARGET_VARIABLE_COLUMN_NAME] == faultNumber)\n",
    "    & (DF_F_TRAINING_RAW[SIMULATION_RUN_COLUMN_NAME] == simulationRun)]\n",
    "df_select_ff_data = DF_FF_TRAINING_RAW[\n",
    "    (DF_FF_TRAINING_RAW[TARGET_VARIABLE_COLUMN_NAME] == 0)\n",
    "    & (DF_FF_TRAINING_RAW[SIMULATION_RUN_COLUMN_NAME] == simulationRun)]\n",
    "\n",
    "used_data = df_select_ff_data\n",
    "\n",
    "feature_columns = DF_FF_TRAINING_RAW.columns[3:]\n",
    "num_features = len(feature_columns)\n",
    "\n",
    "# Define the number of rows and columns for the subplots in the grid\n",
    "num_cols = min(4, num_features)  # Maximum of 4 columns\n",
    "num_rows = int(np.ceil(num_features / num_cols))\n",
    "fig, axes = plt.subplots(num_rows,\n",
    "                         num_cols,\n",
    "                         figsize=(4 * num_cols, 3 * num_rows))\n",
    "fig.suptitle(\"Boxplots of All Features\", fontsize=16, y=1.02)\n",
    "\n",
    "for i, col in enumerate(feature_columns):\n",
    "    row_index = i // num_cols\n",
    "    col_index = i % num_cols\n",
    "    ax = axes[row_index, col_index]\n",
    "\n",
    "    ax.boxplot(\n",
    "        used_data[col],\n",
    "        patch_artist=True,\n",
    "        boxprops=dict(facecolor=\"lightblue\", color=\"navy\"),\n",
    "        medianprops=dict(color=\"red\"),\n",
    "        whiskerprops=dict(color=\"gray\"),\n",
    "        capprops=dict(color=\"gray\"),\n",
    "        flierprops=dict(marker=\"o\", color=\"darkorange\", alpha=0.5),\n",
    "    )\n",
    "\n",
    "    ax.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "    ax.set_ylabel(col)\n",
    "    # ax.set_xticklabels([col], rotation=45)\n",
    "    ax.set_title(col.replace(\"_\", \" \"))\n",
    "\n",
    "for i in range(num_features, num_rows * num_cols):\n",
    "    fig.delaxes(axes.flatten()[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08dad2a2",
   "metadata": {},
   "source": [
    "## Distribution Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063351a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting distribution of selected features in f and ff beside each other to compare fault vs normal\n",
    "\n",
    "faultNumber = 3\n",
    "simulationRun = 1\n",
    "df_selected_f_data = DF_F_TRAINING_RAW[\n",
    "    (DF_F_TRAINING_RAW[TARGET_VARIABLE_COLUMN_NAME] == faultNumber)\n",
    "    & (DF_F_TRAINING_RAW[SIMULATION_RUN_COLUMN_NAME] == simulationRun)]\n",
    "df_select_ff_data = DF_FF_TRAINING_RAW[\n",
    "    (DF_FF_TRAINING_RAW[TARGET_VARIABLE_COLUMN_NAME] == 0)\n",
    "    & (DF_FF_TRAINING_RAW[SIMULATION_RUN_COLUMN_NAME] == simulationRun)]\n",
    "\n",
    "# plotting distribution of selected features f and ff beside each other to compare fault vs normal\n",
    "# selected features to plot\n",
    "# selected_features = ['xmeas_1', 'xmeas_2']\n",
    "selected_features = df_selected_f_data.columns[\n",
    "    3:]  # Select features from index 3 to the end\n",
    "# Create a figure with subplots for each selected feature\n",
    "fig, axes = plt.subplots(len(selected_features),\n",
    "                         2,\n",
    "                         figsize=(12, 6 * len(selected_features)))\n",
    "for i, feature in enumerate(selected_features):\n",
    "    # Plotting the faulty data gaussian distribution\n",
    "    sns.histplot(\n",
    "        df_selected_f_data[feature],\n",
    "        kde=True,\n",
    "        ax=axes[i, 0],\n",
    "        color=\"red\",\n",
    "        label=\"Faulty Data\",\n",
    "    )\n",
    "    axes[i, 0].set_title(f\"Faulty Data - {feature}\")\n",
    "    axes[i, 0].set_xlabel(feature)\n",
    "    axes[i, 0].set_ylabel(\"Density\")\n",
    "    axes[i, 0].legend()\n",
    "    # Plotting the fault-free data gaussian distribution\n",
    "    sns.histplot(\n",
    "        df_select_ff_data[feature],\n",
    "        kde=True,\n",
    "        ax=axes[i, 1],\n",
    "        color=\"blue\",\n",
    "        label=\"Fault-Free Data\",\n",
    "    )\n",
    "    axes[i, 1].set_title(f\"Fault-Free Data - {feature}\")\n",
    "    axes[i, 1].set_xlabel(feature)\n",
    "    axes[i, 1].set_ylabel(\"Density\")\n",
    "    axes[i, 1].legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c0c77d",
   "metadata": {},
   "source": [
    "## Visualize class separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0dd3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize class separation and structure in high-dimensional process data using t-SNE embedding to 2D.\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def plot_tsne_visualization(x_f_train: np.ndarray,\n",
    "                            y_f_labeled_train: np.ndarray,\n",
    "                            step: int = 50) -> None:\n",
    "    \"\"\"\n",
    "    Visualize class separation and structure in high-dimensional process data using t-SNE embedding to 2D.\n",
    "\n",
    "    Input:\n",
    "        x_f_train: High-dimensional feature matrix (e.g., 54 dimensions).\n",
    "        y_f_labeled_train: 1D label array corresponding to x_f_train.\n",
    "        step: Downsampling factor to reduce computational load (default=50).\n",
    "\n",
    "    Output:\n",
    "        A 2D scatter plot showing t-SNE projection colored and styled by labels.\n",
    "    \"\"\"\n",
    "\n",
    "    # Downsample the data to reduce computation\n",
    "    x_down = x_f_train[::step, :]\n",
    "    y_label = y_f_labeled_train[::step]\n",
    "\n",
    "    # Apply t-SNE to project high-dimensional data into 2D space\n",
    "    x_embedded = TSNE(n_components=2, learning_rate=\"auto\",\n",
    "                      init=\"random\").fit_transform(x_down)\n",
    "\n",
    "    # Create a scatter plot of the 2D embedded data\n",
    "    f, ax = plt.subplots(figsize=(10, 6))\n",
    "    sns.scatterplot(\n",
    "        x=x_embedded[:, 0],\n",
    "        y=x_embedded[:, 1],\n",
    "        hue=y_label,\n",
    "        style=y_label,\n",
    "        palette=\"bright\",\n",
    "        edgecolor=\"black\",\n",
    "    )\n",
    "    plt.legend(bbox_to_anchor=(1.1, 1.05))\n",
    "    plt.title(\"t-SNE Visualization of Labeled Data\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# plot_tsne_visualization(x_f_train, y_f_labeled_train)  # Uncomment to run t-SNE visualization (heavy- take time)\n",
    "plot_tsne_visualization(\n",
    "    X_TRAIN,\n",
    "    Y_TRAIN_DF)  # Uncomment to run t-SNE visualization (heavy- take time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8cf16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter plot of the first two features of the training data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_TRAIN[:, 0],\n",
    "            X_TRAIN[:, 1],\n",
    "            c=Y_TRAIN_DF,\n",
    "            cmap=\"viridis\",\n",
    "            alpha=0.5)\n",
    "plt.colorbar(label=\"Fault Number\")\n",
    "plt.title(\"Scatter Plot of First Two Features of Training Data\")\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee53e35",
   "metadata": {},
   "source": [
    "## play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8035fb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot distribution gaussian distribution of the first two features of the training data\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(X_TRAIN[:, 0], kde=True, color=\"blue\", label=\"Feature 1\")\n",
    "sns.histplot(X_TRAIN[:, 1], kde=True, color=\"orange\", label=\"Feature 2\")\n",
    "plt.title(\"Gaussian Distribution of First Two Features of Training Data\")\n",
    "plt.xlabel(\"Feature Value\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0775f31",
   "metadata": {},
   "source": [
    "## Fault injection point analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce56dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fault injection point analysis: its started at sample  almost 25\n",
    "feature = \"xmeas_3\"  # Example feature to inspect\n",
    "fault_number = 3  # Example fault number to inspect\n",
    "simulationRun = 6\n",
    "feature_a = DF_F_TRAINING_RAW.query(\n",
    "    \"faultNumber == @fault_number and simulationRun == @simulationRun\"\n",
    ")[feature].reset_index(drop=True)\n",
    "feature_b = DF_FF_TRAINING_RAW.query(\n",
    "    \"simulationRun == @simulationRun\")[feature].reset_index(drop=True)\n",
    "\n",
    "delta = feature_a - feature_b\n",
    "\n",
    "# print(delta)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(delta[:50], label=\"Delta (feature A - feature B)\", color=\"red\")\n",
    "plt.axhline(0, color=\"black\", linestyle=\"--\", linewidth=1)\n",
    "plt.title(\n",
    "    f\"Delta for {feature} between Fault A and Fault B for same simulation run (same seed)\"\n",
    ")\n",
    "plt.xlabel(\"Sample Index\")\n",
    "plt.ylabel(\"Delta Value\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "def compare_faults_stat(\n",
    "    stats_df: pd.DataFrame,\n",
    "    fault_a: int,\n",
    "    fault_b: int,\n",
    "    run_id: int,\n",
    "    stat: str = \"mean\",\n",
    ") -> pd.DataFrame:\n",
    "    df_a = stats_df.query(\n",
    "        \"faultNumber == @fault_a and simulationRun == @run_id\")[[\n",
    "            \"feature\", stat\n",
    "        ]].rename(columns={stat: f\"fault_{fault_a}\"})\n",
    "\n",
    "    df_b = stats_df.query(\n",
    "        \"faultNumber == @fault_b and simulationRun == @run_id\")[[\n",
    "            \"feature\", stat\n",
    "        ]].rename(columns={stat: f\"fault_{fault_b}\"})\n",
    "\n",
    "    merged = pd.merge(df_a, df_b, on=\"feature\", how=\"outer\")\n",
    "    merged[\"delta\"] = merged[f\"fault_{fault_a}\"] - merged[f\"fault_{fault_b}\"]\n",
    "    return merged\n",
    "\n",
    "\n",
    "compare_df = compare_faults_stat(stats_df,\n",
    "                                 fault_a=3,\n",
    "                                 fault_b=0,\n",
    "                                 run_id=1,\n",
    "                                 stat=\"mean\")\n",
    "\n",
    "compare_df.head()\n",
    "\n",
    "stats_df.feature.unique()\n",
    "\n",
    "des = DF_FF_TRAINING_RAW.iloc[:, 3:].describe()\n",
    "cal = des.iloc[1:3, :].T\n",
    "cal[\"variance\"] = cal[\"std\"]**2\n",
    "print(cal.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e37ed82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the fault insection data point\n",
    "\n",
    "# Determine the fault insection data point\n",
    "\n",
    "# This function will plot the feature time series before and after fault injection\n",
    "# It will also mark the fault injection point and show median values before and after the injection.\n",
    "# It is useful for visualizing how features change around the time a fault is injected.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_fault_injection_segment(\n",
    "    df: pd.DataFrame,\n",
    "    fault_injection_index: int,\n",
    "    features: list[str],\n",
    "    fault_number: int,\n",
    "    point_start: int = 0,\n",
    "    point_end: int = 500,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Plot feature time series before and after fault injection.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame containing time series (1 simulation run).\n",
    "    - fault_injection_index: Time step where fault is injected.\n",
    "    - features: List of feature names to plot.\n",
    "    - window: Number of time steps before and after injection to show.\n",
    "    \"\"\"\n",
    "    # Filter only the rows with the selected fault number\n",
    "    df: pd.DataFrame = df[df[TARGET_VARIABLE_COLUMN_NAME] == fault_number].reset_index(\n",
    "        drop=True)\n",
    "\n",
    "    x_range: np.ndarray = np.arange(point_start, point_end)\n",
    "\n",
    "    df_window: pd.DataFrame = df.iloc[point_start:point_end]\n",
    "\n",
    "    n_features: int = len(features)\n",
    "    fig, axes = plt.subplots(n_features,\n",
    "                             1,\n",
    "                             figsize=(30, 3 * n_features),\n",
    "                             sharex=True)\n",
    "\n",
    "    if n_features == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for i, feature in enumerate(features):\n",
    "        ax = axes[i]\n",
    "        y = df_window[feature].values\n",
    "        ax.plot(x_range, y, label=feature, color=\"black\")\n",
    "\n",
    "        # Mark every 5th time index\n",
    "        for x in x_range:\n",
    "            if x % 5 == 0:\n",
    "                ax.axvline(x, color=\"gray\", linestyle=\":\", linewidth=0.5)\n",
    "                ax.text(\n",
    "                    x,\n",
    "                    ax.get_ylim()[1],\n",
    "                    f\"{x}\",\n",
    "                    color=\"gray\",\n",
    "                    fontsize=8,\n",
    "                    ha=\"center\",\n",
    "                    va=\"bottom\",\n",
    "                    rotation=90,\n",
    "                    backgroundcolor=\"white\",\n",
    "                )\n",
    "\n",
    "        # Vertical fault injection marker\n",
    "        ax.axvline(\n",
    "            fault_injection_index,\n",
    "            color=\"red\",\n",
    "            linestyle=\"--\",\n",
    "            linewidth=2,\n",
    "            label=\"Fault Injected\",\n",
    "        )\n",
    "\n",
    "        # Text showing the time index\n",
    "        ax.text(\n",
    "            fault_injection_index,\n",
    "            ax.get_ylim()[1],\n",
    "            f\"t={fault_injection_index}\",\n",
    "            color=\"red\",\n",
    "            fontsize=10,\n",
    "            ha=\"center\",\n",
    "            va=\"bottom\",\n",
    "            rotation=0,\n",
    "            backgroundcolor=\"white\",\n",
    "        )\n",
    "\n",
    "        # Median before/after\n",
    "        before = df.loc[point_start:fault_injection_index - 1, feature]\n",
    "        after = df.loc[fault_injection_index:point_end - 1, feature]\n",
    "        before_med = np.median(before)\n",
    "        after_med = np.median(after)\n",
    "\n",
    "        ax.axhline(before_med,\n",
    "                   color=\"blue\",\n",
    "                   linestyle=\":\",\n",
    "                   label=\"Median Before\")\n",
    "        ax.axhline(after_med,\n",
    "                   color=\"green\",\n",
    "                   linestyle=\":\",\n",
    "                   label=\"Median After\")\n",
    "\n",
    "        ax.set_ylabel(feature)\n",
    "        ax.grid(True)\n",
    "\n",
    "        if i == 0:\n",
    "            ax.legend(loc=\"best\")\n",
    "\n",
    "    plt.xlabel(\"Time Index\")\n",
    "    plt.suptitle(\"Feature Response Around Fault Injection\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "df_fault = DF_F_TRAINING_RAW[(DF_F_TRAINING_RAW[\"simulationRun\"] == 2.0)].reset_index(\n",
    "    drop=True)\n",
    "\n",
    "print(len(df_fault))\n",
    "\n",
    "plot_fault_injection_segment(\n",
    "    df=df_fault,\n",
    "    fault_injection_index=FAULT_INJECTION_STARTING_POINT,\n",
    "    # features=['xmeas_1', 'xmeas_2', 'xmeas_3', 'xmeas_4', 'xmeas_5', 'xmeas_6'],\n",
    "    features=df_fault.columns[3:9],\n",
    "    fault_number=1,\n",
    "    point_start=1,\n",
    "    point_end=500,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd7ce61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare fault vs normal time series for selected features.\n",
    "# This help to caputre the differences between faulty and normal segments of the time series data and determine the start of the fault injection.\n",
    "\n",
    "simulationRun = 1.0\n",
    "fault_number = 3\n",
    "df_fault = DF_F_TRAINING_RAW[(\n",
    "    DF_F_TRAINING_RAW[SIMULATION_RUN_COLUMN_NAME] == simulationRun)].reset_index(drop=True)\n",
    "df_normal = DF_FF_TRAINING_RAW[(\n",
    "    DF_FF_TRAINING_RAW[SIMULATION_RUN_COLUMN_NAME] == simulationRun)].reset_index(drop=True)\n",
    "\n",
    "# Compare fault vs normal time series for selected features.\n",
    "# This help to caputre the differences between faulty and normal segments of the time series data and determine the start of the fault injection.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_fault_vs_normal_segment(\n",
    "    df_fault: pd.DataFrame,\n",
    "    df_normal: pd.DataFrame,\n",
    "    features: list[str],\n",
    "    point_start: int,\n",
    "    point_end: int,\n",
    "    fault_number: int,\n",
    "    fault_injection_index: int = 160,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Compare fault vs normal time series for selected features.\n",
    "\n",
    "    Parameters:\n",
    "    - df_fault: DataFrame with faults (should include faultNumber column).\n",
    "    - df_normal: Normal (non-faulty) baseline DataFrame.\n",
    "    - features: List of feature names to compare.\n",
    "    - point_start: Start index of window.\n",
    "    - point_end: End index of window (exclusive).\n",
    "    - fault_number: Fault number to extract from df_fault.\n",
    "    \"\"\"\n",
    "    df_fault = df_fault[df_fault[TARGET_VARIABLE_COLUMN_NAME] == fault_number].reset_index(\n",
    "        drop=True)\n",
    "    df_fault_window = df_fault.iloc[point_start:point_end]\n",
    "    df_normal_window = df_normal.iloc[point_start:point_end]\n",
    "\n",
    "    x_range = np.arange(point_start, point_end)\n",
    "    n_features = len(features)\n",
    "    fig, axes = plt.subplots(n_features,\n",
    "                             1,\n",
    "                             figsize=(30, 3 * n_features),\n",
    "                             sharex=True)\n",
    "\n",
    "    if n_features == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for i, feature in enumerate(features):\n",
    "        ax = axes[i]\n",
    "\n",
    "        y_fault = df_fault_window[feature].values\n",
    "        y_normal = df_normal_window[feature].values\n",
    "\n",
    "        ax.plot(x_range, y_fault, label=f\"Fault {fault_number}\", color=\"red\")\n",
    "        ax.plot(x_range, y_normal, label=\"Normal\", color=\"black\")\n",
    "\n",
    "        # Median lines\n",
    "        ax.axhline(\n",
    "            np.median(y_normal),\n",
    "            color=\"black\",\n",
    "            linestyle=\"--\",\n",
    "            label=\"Normal Median\",\n",
    "        )\n",
    "        ax.axhline(\n",
    "            np.median(y_fault),\n",
    "            color=\"red\",\n",
    "            linestyle=\"--\",\n",
    "            label=\"Fault Median\",\n",
    "        )\n",
    "        ax.axvline(\n",
    "            fault_injection_index,\n",
    "            color=\"green\",\n",
    "            linestyle=\"--\",\n",
    "            linewidth=2,\n",
    "            label=\"Fault Injected\",\n",
    "        )\n",
    "        # Mark every 5th index\n",
    "        for x in x_range:\n",
    "            if x % 5 == 0:\n",
    "                ax.axvline(x, color=\"gray\", linestyle=\":\", linewidth=0.5)\n",
    "                ax.text(\n",
    "                    x,\n",
    "                    ax.get_ylim()[1],\n",
    "                    f\"{x}\",\n",
    "                    color=\"gray\",\n",
    "                    fontsize=8,\n",
    "                    ha=\"center\",\n",
    "                    va=\"bottom\",\n",
    "                    rotation=90,\n",
    "                    backgroundcolor=\"white\",\n",
    "                )\n",
    "        ax.set_ylabel(feature)\n",
    "        ax.grid(True)\n",
    "        if i == 0:\n",
    "            ax.legend(loc=\"best\")\n",
    "    # Place suptitle before tight_layout to ensure it appears above the plots\n",
    "    # plt.suptitle(f\"Comparison of Fault {fault_number} vs Normal\", fontsize=14)\n",
    "    plt.xlabel(\"Time Index\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_fault_vs_normal_segment(\n",
    "    df_fault=df_fault,\n",
    "    df_normal=df_normal,\n",
    "    features=DF_F_TRAINING_RAW.\n",
    "    columns[3:],  # Select all features, which strats from index 3\n",
    "    point_start=20,\n",
    "    point_end=50,\n",
    "    fault_number=fault_number,\n",
    "    fault_injection_index=FAULT_INJECTION_STARTING_POINT,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da298aa8",
   "metadata": {},
   "source": [
    "## Time Serie Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d228e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Training Data for Simulation Run 1 and Fault Number 0\n",
    "df_data_to_plot = DF_FF_TRAINING_RAW[(DF_FF_TRAINING_RAW[TARGET_VARIABLE_COLUMN_NAME] == 0)\n",
    "                                 & (DF_FF_TRAINING_RAW[SIMULATION_RUN_COLUMN_NAME] == 1)]\n",
    "\n",
    "feature_columns = df_normal.columns[3:]\n",
    "num_features = len(feature_columns)\n",
    "\n",
    "# Define the number of rows and columns for the subplots\n",
    "num_cols = min(4, num_features)  # Maximum of 4 columns\n",
    "num_rows = int(np.ceil(num_features / num_cols))\n",
    "size_multiplier = 2  # Adjust this value to change the size of the subplots\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(\n",
    "    num_rows,\n",
    "    num_cols,\n",
    "    figsize=(4 * num_cols * size_multiplier, 3 * num_rows * size_multiplier),\n",
    ")\n",
    "\n",
    "for i, col in enumerate(feature_columns):\n",
    "    # Determine the row and column index (position) for the subplot in the grid\n",
    "    row_index = i // num_cols\n",
    "    col_index = i % num_cols\n",
    "    ax = axes[row_index, col_index]\n",
    "\n",
    "    ax.plot(df_data_to_plot[\"sample\"], df_data_to_plot[col])\n",
    "    ax.set_xlabel(\"sample no.\")\n",
    "    ax.set_ylabel(col)\n",
    "    ax.set_title(col.replace(\"_\", \" \"))\n",
    "\n",
    "for i in range(num_features, num_rows * num_cols):\n",
    "    fig.delaxes(axes.flatten()[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1af912e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting a selected feature against sample numbers for multiple simulation runs and no faults\n",
    "\n",
    "# selected column to plot # Uncomment the column you want to plot\n",
    "# col = 'A_feed_stream'\n",
    "col = \"xmeas_1\"\n",
    "\n",
    "# Define the simulation runs to plot\n",
    "simRuns = np.arange(1, 500, 50)\n",
    "num_simRuns = len(simRuns)\n",
    "\n",
    "# Define the number of rows and columns for the subplots in the grid\n",
    "num_cols = min(3, num_simRuns)  # Maximum of 2 columns\n",
    "num_rows = int(np.ceil(num_simRuns / num_cols))\n",
    "size_multiplier = 2  # Adjust this value to change the size of the subplots\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(\n",
    "    num_rows,\n",
    "    num_cols,\n",
    "    figsize=(4 * num_cols * size_multiplier, 3 * num_rows * size_multiplier),\n",
    ")\n",
    "\n",
    "for i, simRun in enumerate(simRuns):\n",
    "    pp = DF_FF_TRAINING_RAW[(DF_FF_TRAINING_RAW[TARGET_VARIABLE_COLUMN_NAME] == 0)\n",
    "                        & (DF_FF_TRAINING_RAW[SIMULATION_RUN_COLUMN_NAME] == simRun)]\n",
    "    row_index = i // num_cols\n",
    "    col_index = i % num_cols\n",
    "    ax = axes[row_index, col_index]\n",
    "    ax.plot(pp[\"sample\"], pp[col])\n",
    "    ax.set_xlabel(\"sample no.\")\n",
    "    ax.set_ylabel(col)\n",
    "    ax.set_title(f\"Simulation run - {simRun}\")\n",
    "\n",
    "# Remove any empty subplots\n",
    "for i in range(num_simRuns, num_rows * num_cols):\n",
    "    fig.delaxes(axes.flatten()[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4665e4a5",
   "metadata": {},
   "source": [
    "## Correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a277e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T13:56:08.567462Z",
     "iopub.status.busy": "2023-11-01T13:56:08.567171Z",
     "iopub.status.idle": "2023-11-01T13:56:13.052484Z",
     "shell.execute_reply": "2023-11-01T13:56:13.050728Z",
     "shell.execute_reply.started": "2023-11-01T13:56:08.567437Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plotting the correlation matrix of the features\n",
    "\n",
    "# Load the data into a pandas DataFrame\n",
    "data = DF_FF_TRAINING_RAW[DF_FF_TRAINING_RAW[\"simulationRun\"] == 1].iloc[:, 3:]\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "corr = data.corr()\n",
    "\n",
    "# Create a heatmap with annotations\n",
    "sns.set_theme(style=\"white\")\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "f, ax = plt.subplots(figsize=(50, 50))\n",
    "sns.heatmap(\n",
    "    corr,\n",
    "    mask=mask,\n",
    "    cmap=\"coolwarm\",\n",
    "    annot=True,\n",
    "    fmt=\".2f\",\n",
    "    square=True,\n",
    "    linewidths=0.5,\n",
    "    cbar_kws={\"shrink\": 0.5},\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d11d500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the correlation matrix with a threshold\n",
    "# Set correlation threshold\n",
    "threshold: float = 0.95\n",
    "\n",
    "# Load the data into a pandas DataFrame\n",
    "data: pd.DataFrame = DF_FF_TRAINING_RAW[DF_FF_TRAINING_RAW[SIMULATION_RUN_COLUMN_NAME] ==\n",
    "                                    1].iloc[:, 3:]\n",
    "\n",
    "# Calculate correlation matrix\n",
    "corr: pd.DataFrame = data.corr()\n",
    "\n",
    "# Remove self-correlation (diagonal only)\n",
    "corr_abs: pd.DataFrame = corr.abs().copy()\n",
    "np.fill_diagonal(corr_abs.values, 0.0)\n",
    "\n",
    "# Identify features with any correlation above threshold\n",
    "selected_features: set[str] = set(corr_abs.columns[(corr_abs\n",
    "                                                    > threshold).any()])\n",
    "selected_features_list: list[str] = sorted(selected_features)\n",
    "\n",
    "# Filter correlation matrix to only those features\n",
    "filtered_corr: pd.DataFrame = corr.loc[selected_features_list,\n",
    "                                       selected_features_list]\n",
    "\n",
    "# Plot heatmap\n",
    "sns.set_theme(style=\"white\")\n",
    "mask: np.ndarray = np.triu(np.ones_like(filtered_corr, dtype=bool))\n",
    "\n",
    "f, ax = plt.subplots(figsize=(12, 10))\n",
    "sns.heatmap(\n",
    "    filtered_corr,\n",
    "    mask=mask,\n",
    "    cmap=\"coolwarm\",\n",
    "    annot=True,\n",
    "    fmt=\".2f\",\n",
    "    square=True,\n",
    "    linewidths=0.5,\n",
    "    cbar_kws={\"shrink\": 0.5},\n",
    ")\n",
    "plt.title(f\"Correlation Matrix (|corr| > {threshold})\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39104ec1",
   "metadata": {},
   "source": [
    "## Plotting high-correlation pairs with interactive widgets as Time Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcb3b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting high-correlation pairs with interactive widgets as Time Series\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import Button, HBox, VBox, Output, Dropdown\n",
    "\n",
    "# 1. Compute correlation matrix and extract high-correlation pairs\n",
    "data: pd.DataFrame = DF_FF_TRAINING_RAW[DF_FF_TRAINING_RAW[\"simulationRun\"] ==\n",
    "                                    1].iloc[:, 3:]\n",
    "corr: pd.DataFrame = data.corr()\n",
    "\n",
    "# Zero diagonal\n",
    "corr_abs: pd.DataFrame = corr.abs().copy()\n",
    "np.fill_diagonal(corr_abs.values, 0.0)\n",
    "\n",
    "# Threshold\n",
    "threshold: float = 0.95\n",
    "high_pairs = [\n",
    "    (col1, col2, corr.loc[col1, col2]) for i, col1 in enumerate(corr.columns)\n",
    "    for j, col2 in enumerate(corr.columns)\n",
    "    if j > i and isinstance(corr.loc[col1, col2], (\n",
    "        float, np.floating)) and abs(corr.loc[col1, col2]) > threshold\n",
    "]\n",
    "\n",
    "# 2. Prepare dropdown options\n",
    "pair_labels: list[str] = [\n",
    "    f\"{a} - {b} (corr={c:.2f})\" for a, b, c in high_pairs\n",
    "]\n",
    "pair_dict: dict[str, tuple[str, str]] = {\n",
    "    label: (a, b)\n",
    "    for label, (a, b, _) in zip(pair_labels, high_pairs)\n",
    "}\n",
    "dropdown = Dropdown(options=pair_labels, description=\"Pair:\")\n",
    "\n",
    "# 3. Global state\n",
    "index: int = 0\n",
    "window_size: int = 150\n",
    "out = Output()\n",
    "\n",
    "\n",
    "# 4. Plot logic\n",
    "def plot_selected_pair(start: int, feature1: str, feature2: str) -> None:\n",
    "    global out\n",
    "    series1: pd.Series = data[feature1].reset_index(drop=True)\n",
    "    series2: pd.Series = data[feature2].reset_index(drop=True)\n",
    "    mean1, std1 = series1.mean(), series1.std()\n",
    "    mean2, std2 = series2.mean(), series2.std()\n",
    "\n",
    "    end: int = min(start + window_size, len(series1))\n",
    "    x = np.arange(start, end)\n",
    "    y1 = series1[start:end]\n",
    "    y2 = series2[start:end]\n",
    "\n",
    "    with out:\n",
    "        out.clear_output(wait=True)\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        plt.plot(x, y1, label=feature1, color=\"blue\")\n",
    "        plt.plot(x, y2, label=feature2, color=\"red\")\n",
    "\n",
    "        plt.axhline(mean1, color=\"blue\", linestyle=\"--\")\n",
    "        plt.axhline(mean1 + std1, color=\"blue\", linestyle=\":\")\n",
    "        plt.axhline(mean1 - std1, color=\"blue\", linestyle=\":\")\n",
    "        plt.axhline(mean2, color=\"red\", linestyle=\"--\")\n",
    "        plt.axhline(mean2 + std2, color=\"red\", linestyle=\":\")\n",
    "        plt.axhline(mean2 - std2, color=\"red\", linestyle=\":\")\n",
    "\n",
    "        plt.title(f\"{feature1} vs {feature2} [{start}:{end}]\")\n",
    "        plt.xlabel(\"Time Index\")\n",
    "        plt.ylabel(\"Value\")\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# 5. Button callbacks\n",
    "def on_next_clicked(_):\n",
    "    global index\n",
    "    if index + window_size < len(data):\n",
    "        index += window_size\n",
    "        f1, f2 = pair_dict[dropdown.value]\n",
    "        plot_selected_pair(index, f1, f2)\n",
    "\n",
    "\n",
    "def on_back_clicked(_):\n",
    "    global index\n",
    "    if index - window_size >= 0:\n",
    "        index -= window_size\n",
    "        f1, f2 = pair_dict[dropdown.value]\n",
    "        plot_selected_pair(index, f1, f2)\n",
    "\n",
    "\n",
    "def on_dropdown_change(change):\n",
    "    global index\n",
    "    if change[\"type\"] == \"change\" and change[\"name\"] == \"value\":\n",
    "        index = 0  # reset on pair change\n",
    "        f1, f2 = pair_dict[change[\"new\"]]\n",
    "        plot_selected_pair(index, f1, f2)\n",
    "\n",
    "\n",
    "# Bind events\n",
    "btn_next = Button(description=\"Next →\")\n",
    "btn_back = Button(description=\"← Back\")\n",
    "btn_next.on_click(on_next_clicked)\n",
    "btn_back.on_click(on_back_clicked)\n",
    "dropdown.observe(on_dropdown_change)\n",
    "\n",
    "# Init plot\n",
    "initial_f1, initial_f2 = pair_dict[dropdown.value]\n",
    "plot_selected_pair(index, initial_f1, initial_f2)\n",
    "\n",
    "# Show all widgets\n",
    "VBox([dropdown, HBox([btn_back, btn_next]), out])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafce1fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T13:56:13.054723Z",
     "iopub.status.busy": "2023-11-01T13:56:13.054377Z",
     "iopub.status.idle": "2023-11-01T13:56:13.124616Z",
     "shell.execute_reply": "2023-11-01T13:56:13.123631Z",
     "shell.execute_reply.started": "2023-11-01T13:56:13.054690Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print the correlation where the upper triangle is greater than 0.90\n",
    "# This will help identify highly correlated features\n",
    "\n",
    "corr_matrix = data.corr()\n",
    "\n",
    "upper_tri = corr_matrix.where(\n",
    "    np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "to_drop = [\n",
    "    column for column in upper_tri.columns if any(upper_tri[column] > 1.0)\n",
    "]\n",
    "print(len(to_drop))\n",
    "print(to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd8090b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Fault Classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfedf3e0",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Methode that works with LabelEncoded target variable Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcae33e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T13:56:13.533814Z",
     "iopub.status.busy": "2023-11-01T13:56:13.533356Z",
     "iopub.status.idle": "2023-11-01T14:00:14.282502Z",
     "shell.execute_reply": "2023-11-01T14:00:14.281256Z",
     "shell.execute_reply.started": "2023-11-01T13:56:13.533767Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fit models to the data\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "# Create an instance of each algorithm\n",
    "# logreg = LogisticRegression(max_iter=10000)\n",
    "# knn = KNeighborsClassifier()\n",
    "# dt = DecisionTreeClassifier()\n",
    "# nb = GaussianNB()\n",
    "# svm = SVC()\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "xg = xgb.XGBClassifier()\n",
    "\n",
    "# Train the algorithms on the data\n",
    "# logreg.fit(x_train, Y_TRAIN)\n",
    "# knn.fit(x_train, Y_TRAIN)\n",
    "# dt.fit(x_train, Y_TRAIN)\n",
    "# nb.fit(x_train, Y_TRAIN)\n",
    "# svm.fit(x_train, Y_TRAIN)\n",
    "\n",
    "rf.fit(X_TRAIN, Y_TRAIN_DF)\n",
    "xg.fit(X_TRAIN, Y_TRAIN_DF)\n",
    "\n",
    "# Use the trained models to make predictions on new data\n",
    "# y_pred_logreg =logreg.predict(x_test)\n",
    "# y_pred_dt = dt.predict(x_test)\n",
    "# y_pred_nb = nb.predict(x_test)\n",
    "# y_pred_knn = knn.predict(x_test)\n",
    "# y_pred_svm = svm.predict(x_test)\n",
    "\n",
    "y_pred_rf = rf.predict(X_TEST_REDUCED)\n",
    "y_pred_xg = xg.predict(X_TEST_REDUCED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9037510f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T14:11:06.895448Z",
     "iopub.status.busy": "2023-11-01T14:11:06.894289Z",
     "iopub.status.idle": "2023-11-01T14:11:07.510720Z",
     "shell.execute_reply": "2023-11-01T14:11:07.509722Z",
     "shell.execute_reply.started": "2023-11-01T14:11:06.895402Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import joblib\n",
    "# joblib.dump(logreg, 'exported-models/logistic_regression_model.pkl')\n",
    "# joblib.dump(dt, 'exported-models/decision_tree_model.pkl')\n",
    "# joblib.dump(rf, 'exported-models/random_forest_model.pkl')\n",
    "# joblib.dump(nb, 'exported-models/naive_bayes_model.pkl')\n",
    "# joblib.dump(knn, 'exported-models/knn_model.pkl')\n",
    "# joblib.dump(xg, 'exported-models/xgboost_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3fe7d4",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### From above analysis, XGB and Random forest Performs well.\n",
    "### Metrics:\n",
    "\n",
    "In this series, we will use accuracy as the primary metric for evaluating the performance of the different machine learning algorithms. We will update the table as we evaluate the performance of other algorithms in the subsequently.\n",
    "\n",
    "\n",
    "Average accuracy score obtained for each method, excluding fault No. 9 and 15 (**No feature were Dropped, all 52 sensor measurements were used**)\n",
    "\n",
    "| Method                                    |Accuracy  |\n",
    "|-----------------------------------------  |----------|\n",
    "| XG Boost                                  |  0.887   |\n",
    "| Neural Network                            |  0.943   |\n",
    "| 1D CNN-Timeserise                         |  0.892   |\n",
    "| LSTM-Timeserise                           |  0.924   |\n",
    "| ANN+RandomForest                          |  0.936   |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf94cc52",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a3a11f",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-01T13:51:27.869976Z",
     "iopub.status.idle": "2023-11-01T13:51:27.870265Z",
     "shell.execute_reply": "2023-11-01T13:51:27.870140Z",
     "shell.execute_reply.started": "2023-11-01T13:51:27.870124Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DNN Model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Define input layer\n",
    "inputs = Input(shape=(X_TRAIN.shape[1], ))\n",
    "\n",
    "# Define hidden layer with 16 nodes and ReLU activation function\n",
    "hidden_layer = Dense(100, activation=\"selu\")(inputs)\n",
    "hidden_layer = Dense(100, activation=\"selu\")(hidden_layer)\n",
    "hidden_layer = Dense(100, activation=\"selu\")(hidden_layer)\n",
    "hidden_layer = Dense(100, activation=\"selu\")(hidden_layer)\n",
    "hidden_layer = Dense(100, activation=\"selu\")(hidden_layer)\n",
    "hidden_layer = Dense(100, activation=\"selu\")(hidden_layer)\n",
    "# Define output layer with softmax activation function for multi-class classification\n",
    "outputs = Dense(Y_ENC_TRAIN.shape[1],\n",
    "                activation=\"softmax\")(hidden_layer)\n",
    "\n",
    "# Define the model\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Compile the model with binary cross-entropy loss function and Adam optimizer\n",
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "              optimizer=\"adam\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "# Print the summary of the model\n",
    "model.summary()\n",
    "\n",
    "# Define early stopping callback to monitor validation loss and stop if it doesn't improve for 5 epochs\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", patience=5)\n",
    "\n",
    "# Train the model with 20 epochs and batch size of 32, using the early stopping callback\n",
    "history = model.fit(\n",
    "    X_TRAIN,\n",
    "    Y_ENC_TRAIN,\n",
    "    epochs=200,\n",
    "    batch_size=256,\n",
    "    validation_data=(X_TEST_REDUCED, Y_ENC_TEST_REDUCED),\n",
    "    callbacks=[early_stop],\n",
    ")\n",
    "\n",
    "# Plot the training history for loss and accuracy\n",
    "plt.plot(history.history[\"loss\"], label=\"Training Loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"Validation Loss\")\n",
    "plt.plot(history.history[\"accuracy\"], label=\"Training Accuracy\")\n",
    "plt.plot(history.history[\"val_accuracy\"], label=\"Validation Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a25e45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_results = model.predict(X_TEST_REDUCED, verbose=0)\n",
    "y_pred_nn = encoder_1.inverse_transform(nn_results)\n",
    "y_true_nn = encoder_1.inverse_transform(Y_ENC_TEST_REDUCED)\n",
    "y_score_nn = model.predict(X_TEST_REDUCED, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a40c655",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_results.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312f780a",
   "metadata": {},
   "source": [
    "## Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e476cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate models and compute metrics\n",
    "from typing import Union, List, Dict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "confusion_matrix\n",
    ")\n",
    "\n",
    "models_results_list: list[dict] = []\n",
    "arl_tables_dict: dict[str, pd.DataFrame] = {}\n",
    "fdr_far_dict: dict[str, pd.DataFrame] = {}\n",
    "delay_tables_dict: dict[str, pd.DataFrame] = {}\n",
    "classification_scores_per_fault_tables_dict: dict[str, pd.DataFrame] = {}\n",
    "fpr_per_class_tables_dict: dict[str, pd.DataFrame] = {}\n",
    "positive_alarm_rate_per_class_table_dict: dict[str, pd.DataFrame] = {}\n",
    "\n",
    "\n",
    "def segment_faults(y_true: List[int]) -> List[tuple[int, int, int]]:\n",
    "    segments = []\n",
    "    n = len(y_true)\n",
    "    start = None\n",
    "    current_fault = 0\n",
    "\n",
    "    for i in range(n):\n",
    "        if y_true[i] != 0:\n",
    "            if current_fault != y_true[i]:\n",
    "                if current_fault != 0:\n",
    "                    segments.append((current_fault, start, i - 1))\n",
    "                current_fault = y_true[i]\n",
    "                start = i\n",
    "        else:\n",
    "            if current_fault != 0:\n",
    "                segments.append((current_fault, start, i - 1))\n",
    "                current_fault = 0\n",
    "                start = None\n",
    "    if current_fault != 0:\n",
    "        segments.append((current_fault, start, n - 1))\n",
    "    return segments\n",
    "\n",
    "\n",
    "def compute_arl_df(y_true: List[int], y_pred: List[int]) -> pd.DataFrame:\n",
    "    fault_segments = segment_faults(y_true)\n",
    "    delays_per_fault: dict[int, List[int]] = {}\n",
    "\n",
    "    for fault_label, start, end in fault_segments:\n",
    "        detection_times = [\n",
    "            t for t in range(start, end + 1) if y_pred[t] == fault_label\n",
    "        ]\n",
    "        if detection_times:\n",
    "            delay = detection_times[0] - start\n",
    "        else:\n",
    "            delay = end - start + 1  # penalty for undetected fault\n",
    "        delays_per_fault.setdefault(fault_label, []).append(delay)\n",
    "\n",
    "    # ARL1 per fault (mean delay)\n",
    "    arl1_per_fault = {\n",
    "        fault: np.mean(delays)\n",
    "        for fault, delays in delays_per_fault.items()\n",
    "    }\n",
    "\n",
    "    # ARL1 weighted overall (all delays equally weighted)\n",
    "    all_delays = [\n",
    "        delay for delays in delays_per_fault.values() for delay in delays\n",
    "    ]\n",
    "    weighted_overall_arl1 = float(\n",
    "        np.mean(all_delays)) if all_delays else np.nan\n",
    "\n",
    "    # ARL1 unweighted overall (mean of per-fault averages)\n",
    "    unweighted_overall_arl1 = float(np.mean(list(\n",
    "        arl1_per_fault.values()))) if arl1_per_fault else np.nan\n",
    "\n",
    "    # Compute ARL0 once for all faults (false alarm interval)\n",
    "    normal_indices = [i for i, v in enumerate(y_true) if v == 0]\n",
    "    false_alarm_times = [i for i in normal_indices if y_pred[i] != 0]\n",
    "\n",
    "    if len(false_alarm_times) < 2:\n",
    "        arl0 = float(\n",
    "            'inf') if len(false_alarm_times\n",
    "                          ) == 0 else normal_indices[-1] - false_alarm_times[0]\n",
    "    else:\n",
    "        intervals = [\n",
    "            j - i\n",
    "            for i, j in zip(false_alarm_times[:-1], false_alarm_times[1:])\n",
    "        ]\n",
    "        arl0 = float(np.mean(intervals)) if intervals else float('inf')\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'Fault': list(arl1_per_fault.keys()),\n",
    "        'ARL1': list(arl1_per_fault.values()),\n",
    "        'ARL0': [arl0] * len(arl1_per_fault)\n",
    "    }).set_index('Fault')\n",
    "\n",
    "    # Append average row with both overall ARL1 versions and ARL0\n",
    "    avg_row = pd.DataFrame({\n",
    "        'ARL1': [unweighted_overall_arl1],\n",
    "        'ARL0': [arl0]\n",
    "    },\n",
    "                           index=['Average (unweighted)'])\n",
    "\n",
    "    weighted_row = pd.DataFrame(\n",
    "        {\n",
    "            'ARL1': [weighted_overall_arl1],\n",
    "            'ARL0': [arl0]\n",
    "        },\n",
    "        index=['Average (weighted)'])\n",
    "\n",
    "    df = pd.concat([df, avg_row, weighted_row])\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def compute_fdr_far(\n",
    "    y_true: List[int],\n",
    "    y_pred: List[int]\n",
    ") -> pd.DataFrame:\n",
    "    y_true_arr = np.array(y_true)\n",
    "    y_pred_arr = np.array(y_pred)\n",
    "    classes = sorted(set(y_true_arr) | set(y_pred_arr))\n",
    "    classes = [c for c in classes if c != 0]  # exclude class 0 (normal)\n",
    "\n",
    "    results: Dict[str, List[float]] = {\"Class\": [], \"FDR\": [], \"FAR\": [], \"Support\": []}\n",
    "\n",
    "    for c in classes:\n",
    "        tp = np.sum((y_true_arr == c) & (y_pred_arr == c))\n",
    "        fp = np.sum((y_true_arr != c) & (y_pred_arr == c))\n",
    "        fn = np.sum((y_true_arr == c) & (y_pred_arr != c))\n",
    "\n",
    "        support = tp + fn  # number of true instances of this class\n",
    "\n",
    "        fdr = fp / (tp + fp) if (tp + fp) > 0 else np.nan\n",
    "        far = fp / np.sum(y_pred_arr != 0) if np.sum(y_pred_arr != 0) > 0 else np.nan\n",
    "\n",
    "        results[\"Class\"].append(c)\n",
    "        results[\"FDR\"].append(fdr)\n",
    "        results[\"FAR\"].append(far)\n",
    "        results[\"Support\"].append(support)\n",
    "\n",
    "    df = pd.DataFrame(results).set_index(\"Class\")\n",
    "\n",
    "    # Unweighted mean\n",
    "    avg_row = pd.DataFrame({\n",
    "        \"FDR\": [df[\"FDR\"].mean()],\n",
    "        \"FAR\": [df[\"FAR\"].mean()],\n",
    "        \"Support\": [df[\"Support\"].sum()]\n",
    "    }, index=[\"Average (unweighted)\"])\n",
    "\n",
    "    # Weighted mean\n",
    "    weights = df[\"Support\"] / df[\"Support\"].sum()\n",
    "    weighted_row = pd.DataFrame({\n",
    "        \"FDR\": [(df[\"FDR\"] * weights).sum()],\n",
    "        \"FAR\": [(df[\"FAR\"] * weights).sum()],\n",
    "        \"Support\": [df[\"Support\"].sum()]\n",
    "    }, index=[\"Average (weighted)\"])\n",
    "\n",
    "    df = pd.concat([df, avg_row, weighted_row])\n",
    "    return df\n",
    "\n",
    "\n",
    "def combine_arl_fdr_far(df_arl: pd.DataFrame,\n",
    "                        df_fdr_far: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Convert index to string for safe filtering\n",
    "    df_arl.index = df_arl.index.map(str)\n",
    "    df_fdr_far.index = df_fdr_far.index.map(str)\n",
    "\n",
    "    # Filter out \"Average\" rows from both\n",
    "    df_arl_main = df_arl[~df_arl.index.str.contains(\"Average\")]\n",
    "    df_fdr_far_main = df_fdr_far[~df_fdr_far.index.str.contains(\"Average\")]\n",
    "\n",
    "    # Join on class index\n",
    "    df_combined = df_arl_main.join(df_fdr_far_main, how=\"outer\")\n",
    "\n",
    "    # Add average rows (now strings)\n",
    "    avg_rows = pd.concat([\n",
    "        df_arl[df_arl.index.str.contains(\"Average\")],\n",
    "        df_fdr_far[df_fdr_far.index.str.contains(\"Average\")]\n",
    "    ],\n",
    "                         axis=0)\n",
    "\n",
    "    return pd.concat([df_combined, avg_rows])\n",
    "\n",
    "def compute_classification_scores_per_fault(\n",
    "        y_true: Union[List[int], np.ndarray],\n",
    "        y_pred: Union[List[int], np.ndarray]) -> pd.DataFrame:\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    fault_types = np.unique(y_true[y_true > 0])\n",
    "    scores_per_fault = {}\n",
    "\n",
    "    for fault in fault_types:\n",
    "        y_true_binary = (y_true == fault).astype(int)\n",
    "        y_pred_binary = (y_pred == fault).astype(int)\n",
    "\n",
    "        accuracy = accuracy_score(y_true_binary, y_pred_binary)\n",
    "        precision = precision_score(y_true_binary,\n",
    "                                    y_pred_binary,\n",
    "                                    zero_division=0)\n",
    "        recall = recall_score(y_true_binary, y_pred_binary, zero_division=0)\n",
    "        f1 = f1_score(y_true_binary, y_pred_binary, zero_division=0)\n",
    "\n",
    "        scores_per_fault[fault] = {\n",
    "            \"Accuracy\": accuracy,\n",
    "            \"Precision\": precision,\n",
    "            \"Recall\": recall,\n",
    "            \"F1-score\": f1,\n",
    "        }\n",
    "\n",
    "    return pd.DataFrame(scores_per_fault)\n",
    "\n",
    "def macro_false_alarm_rate(y_true: Union[list[int], np.ndarray],\n",
    "                           y_pred: Union[list[int], np.ndarray]) -> float:\n",
    "    # Compute the confusion matrix for multi-class classification\n",
    "    cm: np.ndarray = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    # Get the number of classes from the confusion matrix shape\n",
    "    n_classes: int = cm.shape[0]\n",
    "\n",
    "    # List to store false positive rate (FPR) for each class\n",
    "    fpr_list: list[float] = []\n",
    "\n",
    "    # Iterate over each class index (one-vs-rest treatment)\n",
    "    for i in range(n_classes):\n",
    "        # False positives for class i: sum of predictions as class i\n",
    "        # excluding the true positives on the diagonal\n",
    "        fp: int = cm[:, i].sum() - cm[i, i]\n",
    "\n",
    "        # True negatives: all samples excluding the positives for class i\n",
    "        # and the false positives for class i\n",
    "        tn: int = cm.sum() - (cm[i, :].sum() + cm[:, i].sum() - cm[i, i])\n",
    "\n",
    "        # Compute FPR: FP / (FP + TN), guard against division by zero\n",
    "        fpr: float = fp / (fp + tn) if (fp + tn) > 0 else 0.0\n",
    "\n",
    "        # Append this class’s FPR to the list\n",
    "        fpr_list.append(fpr)\n",
    "\n",
    "    # Return the macro-average of FPRs across all classes\n",
    "    return float(np.mean(fpr_list).item())\n",
    "\n",
    "def false_alarm_rate_per_class(\n",
    "        y_true: Union[list[int], np.ndarray],\n",
    "        y_pred: Union[list[int], np.ndarray]) -> pd.DataFrame:\n",
    "    # Compute confusion matrix\n",
    "    cm: np.ndarray = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    # Number of classes\n",
    "    n_classes: int = cm.shape[0]\n",
    "\n",
    "    # Dictionary to store FPR per class\n",
    "    fpr_per_class: dict[int, float] = {}\n",
    "\n",
    "    # Compute FPR for each class using one-vs-rest\n",
    "    for i in range(n_classes):\n",
    "        # False positives for class i\n",
    "        fp: int = cm[:, i].sum() - cm[i, i]\n",
    "\n",
    "        # True negatives for class i\n",
    "        tn: int = cm.sum() - (cm[i, :].sum() + cm[:, i].sum() - cm[i, i])\n",
    "\n",
    "        # FPR: FP / (FP + TN), avoid division by zero\n",
    "        fpr: float = fp / (fp + tn) if (fp + tn) > 0 else 0.0\n",
    "\n",
    "        # Store in dictionary\n",
    "        fpr_per_class[i] = fpr\n",
    "\n",
    "    return pd.DataFrame.from_dict(fpr_per_class,\n",
    "                                  orient=\"index\",\n",
    "                                  columns=[\"FPR\"])\n",
    "\n",
    "def positive_alarm_rate_per_class(\n",
    "        y_true: Union[list[int], np.ndarray],\n",
    "        y_pred: Union[list[int], np.ndarray]) -> pd.DataFrame:\n",
    "    cm: np.ndarray = confusion_matrix(y_true, y_pred)\n",
    "    n_classes: int = cm.shape[0]\n",
    "    par_per_class: dict[int, float] = {}\n",
    "\n",
    "    for i in range(n_classes):\n",
    "        tp: int = cm[i, i]\n",
    "        fn: int = cm[i, :].sum() - tp\n",
    "        par: float = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        par_per_class[i] = par\n",
    "\n",
    "    return pd.DataFrame.from_dict(par_per_class,\n",
    "                                  orient=\"index\",\n",
    "                                  columns=[\"PAR\"])\n",
    "\n",
    "def evaluate_model(model_name: str, y_true, y_pred) -> None:\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true,y_pred,average=\"macro\",zero_division=0)\n",
    "    rec = recall_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "    far = macro_false_alarm_rate(y_true, y_pred)\n",
    "    models_results_list.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Accuracy\": acc,\n",
    "        \"Precision\": prec,\n",
    "        \"Recall\": rec,\n",
    "        \"F1-Score\": f1,\n",
    "        \"False Alarm Rate\": far,  # or False Positive Rate (FPR)\n",
    "    })\n",
    "    print(\" \")\n",
    "    print(\"Model name: \", model_name)\n",
    "    print(\"compute arl fdr far \")\n",
    "    arl_tables_dict[model_name] = compute_arl_df(y_true, y_pred)\n",
    "    fdr_far_dict[model_name] = compute_fdr_far(y_true, y_pred)\n",
    "    delay_tables_dict[model_name] = compute_first_detection_delay(y_true, y_pred)\n",
    "    print(\"compute_first_detection_delay \")\n",
    "    classification_scores_per_fault_tables_dict[model_name] = (compute_classification_scores_per_fault(y_true, y_pred))\n",
    "    print(\"classification_scores_per_fault_tables \")\n",
    "    fpr_per_class_tables_dict[model_name] = false_alarm_rate_per_class(y_true, y_pred)\n",
    "    print(\"false_alarm_rate_per_class \")\n",
    "    positive_alarm_rate_per_class_table_dict[model_name] = positive_alarm_rate_per_class( y_true, y_pred)\n",
    "    print(\"positive_alarm_rate_per_class \")\n",
    "\n",
    "\n",
    "# Evaluate\n",
    "evaluate_model(\"Random Forest\", Y_TEST_REDUCED, y_pred_rf)\n",
    "evaluate_model(\"XGBoost\", Y_TEST_REDUCED, y_pred_xg)\n",
    "evaluate_model(\"Neural Net\", Y_TEST_REDUCED, y_pred_nn.ravel())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3306d6d3",
   "metadata": {},
   "source": [
    "### Table average metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76520fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_classification_average=pd.DataFrame(models_results_list)\n",
    "save_dataframe(df=model_classification_average, name=\"Model Results average comparing\")\n",
    "model_classification_average.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a9b8a0",
   "metadata": {},
   "source": [
    "### Plot average metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5facdebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_models_metrics_compareing\n",
    "def plot_models_metrics_compareing(models_results,\n",
    "                                   plot_name: str = \"metric_average\") -> None:\n",
    "    results_df = pd.DataFrame(models_results)\n",
    "    # metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'Detection Rate', 'Avg Delay', 'ARL₁', 'ARL₀', 'False Alarm Rate']\n",
    "    metrics = [\n",
    "        \"Accuracy\",\n",
    "        \"Precision\",\n",
    "        \"Recall\",\n",
    "        \"F1-Score\",\n",
    "        #\"False Alarm Rate\",\n",
    "    ]\n",
    "    melted_df = results_df.melt(\n",
    "        id_vars=\"Model\",\n",
    "        value_vars=metrics,\n",
    "        var_name=\"Metric\",\n",
    "        value_name=\"Score\",\n",
    "    )\n",
    "\n",
    "    plt.figure(figsize=(16, 6))\n",
    "    ax = sns.barplot(data=melted_df, x=\"Metric\", y=\"Score\", hue=\"Model\")\n",
    "    ax.grid(axis=\"y\", linestyle=\"--\", linewidth=0.9, alpha=0.9)\n",
    "    plt.title(\"Model Comparison Across Metrics\")\n",
    "    plt.ylim(0, 1.0)\n",
    "    plt.legend(title=\"Model\", bbox_to_anchor=(1.01, 1), loc=\"upper left\")\n",
    "    plt.tight_layout()\n",
    "    save_plot(plot_name=plot_name, plot_path=\"average\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_models_metrics_compareing(models_results_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bde587",
   "metadata": {},
   "source": [
    "### Table extended metrics average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ae1bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute extended metrics average\n",
    "\n",
    "from os import name\n",
    "from typing import Union\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def compute_average_classification_metrics(\n",
    "        y_true: Union[list[int], np.ndarray],\n",
    "        y_pred: Union[list[int], np.ndarray]) -> pd.DataFrame:\n",
    "    cm: np.ndarray = confusion_matrix(y_true, y_pred)\n",
    "    n_classes: int = cm.shape[0]\n",
    "\n",
    "    # Initialize accumulators\n",
    "    acc_list: list[float] = []\n",
    "    prec_list: list[float] = []\n",
    "    rec_list: list[float] = []\n",
    "    tnr_list: list[float] = []\n",
    "    fpr_list: list[float] = []\n",
    "    fnr_list: list[float] = []\n",
    "    npv_list: list[float] = []\n",
    "    fdr_list: list[float] = []\n",
    "    bal_acc_list: list[float] = []\n",
    "    f1_list: list[float] = []\n",
    "\n",
    "    for i in range(n_classes):\n",
    "        tp: int = cm[i, i]\n",
    "        fn: int = cm[i, :].sum() - tp\n",
    "        fp: int = cm[:, i].sum() - tp\n",
    "        tn: int = cm.sum() - (tp + fp + fn)\n",
    "\n",
    "        # Add safe guards to avoid division by zero\n",
    "        acc: float = ((tp + tn) / (tp + tn + fp + fn) if\n",
    "                      (tp + tn + fp + fn) > 0 else 0.0)\n",
    "        prec: float = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "        rec: float = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        tnr: float = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
    "        fpr: float = fp / (fp + tn) if (fp + tn) > 0 else 0.0\n",
    "        fnr: float = fn / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        npv: float = tn / (tn + fn) if (tn + fn) > 0 else 0.0\n",
    "        fdr: float = fp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "        bal_acc: float = (rec + tnr) / 2\n",
    "        f1: float = (2 * prec * rec) / (prec + rec) if (prec +\n",
    "                                                        rec) > 0 else 0.0\n",
    "\n",
    "        # Accumulate\n",
    "        acc_list.append(acc)\n",
    "        prec_list.append(prec)\n",
    "        rec_list.append(rec)\n",
    "        tnr_list.append(tnr)\n",
    "        fpr_list.append(fpr)\n",
    "        fnr_list.append(fnr)\n",
    "        npv_list.append(npv)\n",
    "        fdr_list.append(fdr)\n",
    "        bal_acc_list.append(bal_acc)\n",
    "        f1_list.append(f1)\n",
    "\n",
    "    # Macro-average\n",
    "    metrics: dict[str, float] = {\n",
    "        \"Macro accuracy\": float(np.mean(acc_list)),  # \"How often am I correct overall, regardless of class?\"\n",
    "        \"Precision\": float(np.mean(prec_list)),  # \"When I flag something, how often is it truly an issue?\"\n",
    "        \"Recall / TPR\": float(np.mean(rec_list)),  # \"Out of all actual issues, how many did I correctly flag?\"\n",
    "        \"F1-Score\": float(np.mean(f1_list)),  # \"What’s the balance between being thorough and being right when flagging?\"\n",
    "        \"FPR\": float(np.mean(fpr_list)),  # \"How often do I wrongly flag a normal case?\"\n",
    "        \"NPV (Negative Predictive Value)\": float(np.mean(npv_list)),  # \"When I say something is normal, how often is that actually true?\"\n",
    "        \"Balanced Accuracy\": float(np.mean(bal_acc_list)),  # \"How well do I perform on both classes, accounting for imbalance?\"\n",
    "        # \"FDR (False Discovery Rate)\": float(np.mean(fdr_list)),  # \"When I raise an alert, how often am I wrong?\" FDR = 1 − Precision\n",
    "        # \"FNR (False Negative Rate)\": float(np.mean(fnr_list)),  # \"How often do I miss a real issue?\" FNR = 1 − Recall\n",
    "        # \"TNR\": float(np.mean(tnr_list)),  # \"Out of all normal cases, how many did I correctly leave alone?\" TNR = 1 − FPR\n",
    "    }\n",
    "\n",
    "\n",
    "    return pd.DataFrame.from_dict(metrics, orient=\"index\", columns=[\"Value\"])\n",
    "\n",
    "\n",
    "# Compute metrics individually\n",
    "metrics_xg: pd.DataFrame = compute_average_classification_metrics(Y_TEST_REDUCED, y_pred_xg)\n",
    "metrics_rf: pd.DataFrame = compute_average_classification_metrics(Y_TEST_REDUCED, y_pred_rf)\n",
    "metrics_nn: pd.DataFrame = compute_average_classification_metrics(Y_TEST_REDUCED, y_pred_nn)\n",
    "\n",
    "# Rename the columns to indicate the model\n",
    "metrics_xg.columns = [XGBOOST]\n",
    "metrics_rf.columns = [RANDOM_FOREST]\n",
    "metrics_nn.columns = [NEURAL_NET]\n",
    "\n",
    "# Concatenate into a single table (column-wise)\n",
    "concated_average_scores_metrics: pd.DataFrame = pd.concat([metrics_xg, metrics_rf, metrics_nn], axis=1)\n",
    "\n",
    "# Print formatted table\n",
    "print(tabulate(concated_average_scores_metrics, headers=\"keys\", tablefmt=\"grid\", showindex=True))\n",
    "save_dataframe(df=concated_average_scores_metrics,name=\"score average\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21591dae",
   "metadata": {},
   "source": [
    "### Plots metrics score average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bc8eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_score_metrics(metrics_df: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Plots a grouped bar chart of classification metrics.\n",
    "\n",
    "    Args:\n",
    "        metrics_df (pd.DataFrame): DataFrame with metrics as rows and model names as columns.\n",
    "    \"\"\"\n",
    "    # Reset index to get metric names as a column\n",
    "    df: pd.DataFrame = metrics_df.reset_index().melt(id_vars=\"index\",\n",
    "                                                     var_name=\"Model\",\n",
    "                                                     value_name=\"Value\")\n",
    "    df.rename(columns={\"index\": \"Metric\"}, inplace=True)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(data=df, x=\"Metric\", y=\"Value\", hue=\"Model\")\n",
    "\n",
    "    plt.title(\"Classification Metrics by Model\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.yticks(np.linspace(0, 1, 21))  # Adds grid lines at intervals of 0.05\n",
    "    plt.yticks(fontsize=8)\n",
    "    plt.grid(axis=\"y\", linestyle=\"--\", linewidth=0.5)\n",
    "    plt.ylim(0, 1.05)\n",
    "    plt.legend(title=\"Model\")\n",
    "    plt.tight_layout()\n",
    "    save_plot(plot_name=\"score average\", plot_path=\"average\")\n",
    "    plt.show()\n",
    "\n",
    "plot_score_metrics(concated_average_scores_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142beefe",
   "metadata": {},
   "source": [
    "### Tables per fault metric comparing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcfcd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute & Print classification scores tables per fault for each model\n",
    "from typing import Union\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "def compute_classification_metrics_per_class(\n",
    "        y_true: Union[list[int], np.ndarray],\n",
    "        y_pred: Union[list[int], np.ndarray]) -> pd.DataFrame:\n",
    "    cm: np.ndarray = confusion_matrix(y_true, y_pred)\n",
    "    n_classes: int = cm.shape[0]\n",
    "    metrics: list[dict] = []\n",
    "\n",
    "    for i in range(n_classes):\n",
    "        tp: int = cm[i, i]\n",
    "        fn: int = cm[i, :].sum() - tp\n",
    "        fp: int = cm[:, i].sum() - tp\n",
    "        tn: int = cm.sum() - (tp + fp + fn)\n",
    "\n",
    "        acc: float = ((tp + tn) / (tp + tn + fp + fn) if\n",
    "                      (tp + tn + fp + fn) > 0 else 0.0)\n",
    "        prec: float = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "        rec: float = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        tnr: float = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
    "        fpr: float = fp / (fp + tn) if (fp + tn) > 0 else 0.0\n",
    "        fnr: float = fn / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        npv: float = tn / (tn + fn) if (tn + fn) > 0 else 0.0\n",
    "        fdr: float = fp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "        bal_acc: float = (rec + tnr) / 2\n",
    "        f1: float = (2 * prec * rec) / (prec + rec) if (prec +\n",
    "                                                        rec) > 0 else 0.0\n",
    "\n",
    "        metrics.append({\n",
    "            \"Class\": i,\n",
    "            \"Accuracy\": acc,  # \"How often am I correct overall, regardless of class?\"\n",
    "            \"Precision\": prec,  # \"When I flag something, how often is it truly an issue?\"\n",
    "            \"Recall / TPR\": rec,  # \"Out of all actual issues, how many did I correctly flag?\"\n",
    "            \"F1-Score\": f1,  # \"What’s the balance between being thorough and being right when flagging?\"\n",
    "            \"FPR\": fpr,  # \"How often do I wrongly flag a normal case?\"\n",
    "            \"NPV (Negative Predictive Value)\": npv,  # \"When I say something is normal, how often is that actually true?\"\n",
    "            \"Balanced Accuracy\": bal_acc,  # \"How well do I perform on both classes, accounting for imbalance?\"\n",
    "            # \"FDR (False Discovery Rate)\": fdr,  # \"When I raise an alert, how often am I wrong?\" FDR = 1 − Precision\n",
    "            # \"FNR (False Negative Rate)\": fnr,  # \"How often do I miss a real issue?\" FNR = 1 − Recall\n",
    "            # \"TNR\": tnr,  # \"Out of all normal cases, how many did I correctly leave alone?\" TNR = 1 − FPR\n",
    "        })\n",
    "\n",
    "\n",
    "    return pd.DataFrame(metrics).set_index(\"Class\")\n",
    "\n",
    "\n",
    "classification_results_per_model: dict[str, pd.DataFrame] = {\n",
    "    \"Random Forest\":\n",
    "    compute_classification_metrics_per_class(Y_TEST_REDUCED, y_pred_rf),\n",
    "    \"XGBoost\":\n",
    "    compute_classification_metrics_per_class(Y_TEST_REDUCED, y_pred_xg),\n",
    "    \"Neural Net\":\n",
    "    compute_classification_metrics_per_class(Y_TEST_REDUCED, y_pred_nn),\n",
    "}\n",
    "\n",
    "# loop through each model and print the classification metrics\n",
    "for model_name, df in classification_results_per_model.items():\n",
    "    # Add an \"Average\" row at the end of the DataFrame\n",
    "    average_row = df.mean(axis=0).to_frame().T\n",
    "    average_row.index = [\"Average\"]\n",
    "    df = pd.concat([df, average_row])\n",
    "    save_dataframe(df=df, name=model_name, suffix=\"metrics\")\n",
    "    print(f\"Classification Metrics for {model_name}:\")\n",
    "    print(tabulate(df, headers=\"keys\", tablefmt=\"grid\"))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29f24a2",
   "metadata": {},
   "source": [
    "### Plots metrics per Fault"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9b2255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Compute per-class metrics for each comparison\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def plot_per_class_metrics_comparison(\n",
    "        model_metrics: dict[str, pd.DataFrame],\n",
    "        plot_name: str = \"metric_fault_number\") -> None:\n",
    "    model_names: list[str] = list(model_metrics.keys())\n",
    "    classes: list[int] = model_metrics[model_names[0]].index.tolist()\n",
    "    metrics: list[str] = model_metrics[model_names[0]].columns.tolist()\n",
    "\n",
    "    for class_id in classes:\n",
    "        # Transpose to shape [metric x model]\n",
    "        plot_df = pd.DataFrame({\n",
    "            model: df.loc[class_id] for model, df in model_metrics.items()\n",
    "        })\n",
    "        plot_df = plot_df.T  # [models x metrics]\n",
    "\n",
    "        # Transpose to group by metric\n",
    "        plot_df = plot_df.T  # [metrics x models]\n",
    "\n",
    "        ax = plot_df.plot(kind=\"bar\", figsize=(20, 8), width=0.75)\n",
    "        ax.set_title(f\"Fault Class {class_id} – Model Comparison per Metric\")\n",
    "        ax.set_xlabel(\"Metric\")\n",
    "        ax.set_ylabel(\"Score\")\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.set_xticks(np.arange(len(metrics)))\n",
    "        ax.set_xticklabels(metrics, rotation=45, ha=\"right\")\n",
    "\n",
    "        # Add fine horizontal grid lines every 0.05\n",
    "        ax.yaxis.set_ticks(np.arange(0, 1.05, 0.05))\n",
    "        ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "        ax.legend(title=\"Model\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "        plt.tight_layout()\n",
    "        save_plot(plot_name=plot_name, suffix=str(class_id), plot_path=\"per_fault\")\n",
    "        plt.show()\n",
    "\n",
    "    # Average per model\n",
    "    avg_df = pd.DataFrame({\n",
    "        model: df.mean(axis=0)\n",
    "        for model, df in model_metrics.items()\n",
    "    })\n",
    "    avg_df = avg_df.T  # [models x metrics]\n",
    "    avg_df = avg_df.T  # [metrics x models]\n",
    "\n",
    "    ax = avg_df.plot(kind=\"bar\", figsize=(20, 8), width=0.75)\n",
    "    ax.set_title(\"Average Metrics Across All Classes – Model Comparison\")\n",
    "    ax.set_xlabel(\"Metric\")\n",
    "    ax.set_ylabel(\"Average Score\")\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_xticks(np.arange(len(metrics)))\n",
    "    ax.set_xticklabels(metrics, rotation=45, ha=\"right\")\n",
    "\n",
    "    ax.yaxis.set_ticks(np.arange(0, 1.05, 0.1))\n",
    "    ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "    ax.legend(title=\"Model\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "    plt.tight_layout()\n",
    "    save_plot(plot_name=plot_name, suffix=\"average\",plot_path=\"average\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_per_class_metrics_comparison(classification_results_per_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da439d70",
   "metadata": {},
   "source": [
    "### Plots per fault per Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e21a200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting classification scores for all models\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def plot_all_fault_scores_per_metric_all_models(\n",
    "        classification_scores_per_fault_tables: dict[str, dict],\n",
    "        plot_name: str = \"Fault_per_Metric\") -> None:\n",
    "    # Flatten input dict into list of records\n",
    "    records = []\n",
    "    for (\n",
    "            model_name,\n",
    "            score_dict,\n",
    "    ) in classification_scores_per_fault_tables.items():\n",
    "        df = score_dict\n",
    "        for metric in df.index:\n",
    "            for fault in df.columns:\n",
    "                records.append({\n",
    "                    \"Model\": model_name,\n",
    "                    \"Fault\": int(fault),  # ensure numerical sorting\n",
    "                    \"Metric\": metric,\n",
    "                    \"Score\": df.loc[metric, fault],\n",
    "                })\n",
    "\n",
    "    combined_df = pd.DataFrame(records)\n",
    "    metrics = combined_df[\"Metric\"].unique()\n",
    "    faults = sorted(combined_df[\"Fault\"].unique())\n",
    "    models = combined_df[\"Model\"].unique()\n",
    "\n",
    "    # Plot one subplot per metric\n",
    "    for metric in metrics:\n",
    "        metric_df = combined_df[combined_df[\"Metric\"] == metric]\n",
    "\n",
    "        # Pivot: rows=faults, columns=models, values=scores\n",
    "        pivot_df = metric_df.pivot(index=\"Fault\",\n",
    "                                   columns=\"Model\",\n",
    "                                   values=\"Score\").loc[faults]\n",
    "\n",
    "        group_spacing: float = 3.0  # control the group spacing\n",
    "        x: np.ndarray = np.arange(0, len(faults) * group_spacing, group_spacing)\n",
    "        bar_width: float = group_spacing * 0.8 / len(models)\n",
    "\n",
    "        plt.figure(figsize=(20, 4))\n",
    "        for i, model in enumerate(models):\n",
    "            plt.bar(x + i * bar_width,\n",
    "                    pivot_df[model],\n",
    "                    width=bar_width,\n",
    "                    label=model)\n",
    "\n",
    "        plt.title(f\"{metric}\")\n",
    "        plt.xlabel(\"Fault\")\n",
    "        plt.ylabel(\"Score\")\n",
    "        plt.xticks(x + bar_width * (len(models) - 1) / 2, faults)\n",
    "        plt.ylim(0, 1)\n",
    "        plt.yticks(np.arange(0, 1.05, 0.05))\n",
    "        plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.5)\n",
    "        plt.legend(title=\"Model\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "        plt.tight_layout()\n",
    "        save_plot(plot_name=plot_name, suffix=str(metric),plot_path=\"per_metric\")\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "plot_all_fault_scores_per_metric_all_models(classification_scores_per_fault_tables_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74de642a",
   "metadata": {},
   "source": [
    "### Plots Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08eb274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Confusion Matrices\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from tabulate import tabulate\n",
    "from typing import Union, List\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(\n",
    "    y_true: Union[List[int], np.ndarray],\n",
    "    y_pred: Union[List[int], np.ndarray],\n",
    "    title: str = \"confusion_matrix\",\n",
    ") -> None:\n",
    "    cm = confusion_matrix(y_true, y_pred, normalize=\"true\")\n",
    "    num_classes: int = cm.shape[0]\n",
    "\n",
    "    width: float = max(6, num_classes * 0.8)\n",
    "    height: float = max(4, num_classes * 0.6)\n",
    "\n",
    "    f, ax = plt.subplots(figsize=(width, height))\n",
    "    sns.heatmap(cm, annot=True, cmap=\"Blues\", ax=ax)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"Predicted\")\n",
    "    ax.set_ylabel(\"Actual\")\n",
    "    ax.set_ylim(num_classes, 0)\n",
    "    plt.tight_layout()\n",
    "    save_plot(plot_name=title,\n",
    "              suffix=\"confusion_matrix\",\n",
    "              plot_path=\"confusion_matrix\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_all_confusion_matrices(model_preds: dict[str, tuple]) -> None:\n",
    "    for name, (y_true, y_pred) in model_preds.items():\n",
    "        plot_confusion_matrix(y_true, y_pred, f\"{name} Confusion Matrix\")\n",
    "\n",
    "\n",
    "plot_all_confusion_matrices({\n",
    "    \"Random Forest\": (Y_TEST_REDUCED, y_pred_rf),\n",
    "    \"XGBoost\": (Y_TEST_REDUCED, y_pred_xg),\n",
    "    \"Neural Net\": (y_true_nn, y_pred_nn),\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3857a103",
   "metadata": {},
   "source": [
    "### Tables for ARL per model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f7625d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "21f24685",
   "metadata": {},
   "source": [
    "### Plots ARL\n",
    "\n",
    "**ARL**\n",
    "- Low ARL₀ is bad for practical use: causes many unnecessary alarms.\n",
    "\n",
    "- Low ARL₁ is good if it's accompanied by high ARL₀ — i.e., a sharp and correct detection only when needed.\n",
    "\n",
    "- Low ARL₀ and ARL₁ mean the feature triggers many false alarms and detects faults too early or too easily. It's likely unstable or noisy, not reliable alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b31fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def plot_arl_tables_dict(arl_tables_dict: dict[str, pd.DataFrame]) -> None:\n",
    "    for model_name, df_arl in arl_tables_dict.items():\n",
    "        df_arl = df_arl.copy()\n",
    "        df_arl.index = df_arl.index.map(str)\n",
    "\n",
    "        # Per-fault (exclude average rows)\n",
    "        df_faults = df_arl[~df_arl.index.str.contains(\"Average\")]\n",
    "        df_faults = df_faults[[\"ARL1\", \"ARL0\"]].dropna(how=\"all\")\n",
    "\n",
    "        x = range(len(df_faults))\n",
    "        width = 0.5\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(25, 6))\n",
    "        ax.bar([i - width / 2 for i in x],\n",
    "               df_faults[\"ARL1\"],\n",
    "               width,\n",
    "               label=\"ARL1\",\n",
    "               color=\"mediumseagreen\")\n",
    "        ax.bar([i + width / 2 for i in x],\n",
    "               df_faults[\"ARL0\"],\n",
    "               width,\n",
    "               label=\"ARL0\",\n",
    "               color=\"coral\")\n",
    "\n",
    "        ax.set_xticks(list(x))\n",
    "        ax.set_xticklabels(df_faults.index, rotation=45, ha=\"right\")\n",
    "        ax.set_ylabel(\"Samples\")\n",
    "        ax.set_title(f\"ARL Metrics per Fault - Model: {model_name}\")\n",
    "        ax.legend()\n",
    "        ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.6)\n",
    "\n",
    "        for i in x:\n",
    "            for j, col in enumerate([\"ARL1\", \"ARL0\"]):\n",
    "                val = df_faults.iloc[i][col]\n",
    "                if pd.notna(val):\n",
    "                    ax.text(i + (j - 0.5) * width,\n",
    "                            val + 0.02 * max(df_faults.max()),\n",
    "                            f\"{val:.2f}\",\n",
    "                            ha=\"center\",\n",
    "                            fontsize=8)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        save_plot(plot_name=\"arl per class\", plot_path=\"arl\")\n",
    "        plt.show()\n",
    "\n",
    "        # Plot averages separately\n",
    "        df_avg = df_arl[df_arl.index.str.contains(\"Average\")]\n",
    "        if not df_avg.empty:\n",
    "            df_avg = df_avg[[\"ARL1\", \"ARL0\"]].dropna(how=\"all\")\n",
    "            df_avg.plot(kind=\"bar\",\n",
    "                        figsize=(5,6),\n",
    "                        color=[\"mediumseagreen\", \"coral\"])\n",
    "            plt.title(f\"ARL Averages - Model: {model_name}\")\n",
    "            plt.ylabel(\"Samples\")\n",
    "            plt.xticks(rotation=0)\n",
    "            plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.6)\n",
    "            ax = plt.gca()\n",
    "            for container in ax.containers:\n",
    "                ax.bar_label(container, fmt=\"%.2f\", padding=3)\n",
    "            plt.tight_layout()\n",
    "            save_plot(plot_name=\"arl average\", plot_path=\"arl\")\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "plot_arl_tables_dict(arl_tables_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9a1610",
   "metadata": {},
   "source": [
    "### Plot FDR FAR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b654b8",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**🔹 WHAT the Metrics Mean**\n",
    "\n",
    "**1. ARL₁ (Average Run Length for Faults)**\n",
    "\n",
    "* Definition: Expected number of time steps **after a fault starts** until it is detected.\n",
    "* Low ARL₁ = **fast detection**\n",
    "* High ARL₁ = **slow detection**\n",
    "\n",
    "**2. FDR (False Detection Rate)**\n",
    "\n",
    "* Definition: Among all times the model predicted a specific fault (e.g. fault 2), how many were **wrong**.\n",
    "* High FDR = **many predictions of fault 2 were incorrect** (i.e., mislabeling)\n",
    "* FDR = FP / (TP + FP) per fault class\n",
    "\n",
    "**3. FAR (False Alarm Rate)**\n",
    "\n",
    "* Definition: How often a fault class is **falsely predicted** during **normal operation** (i.e. when the true class is 0).\n",
    "* Measures how noisy the model is during normal time.\n",
    "* FAR = FP / (# total predicted faults during normal)\n",
    "\n",
    "---\n",
    "\n",
    "**🔹 HOW They Relate**\n",
    "\n",
    "| Metric                                   | Condition                                                            | Interpretation                        |\n",
    "| ---------------------------------------- | -------------------------------------------------------------------- | ------------------------------------- |\n",
    "| **Low ARL₁** + **Low FDR** + **Low FAR** | Ideal                                                                | Fast and accurate detection, no noise |\n",
    "| **Low ARL₁** + **High FDR**              | Fast detection, but many wrong fault predictions                     |                                       |\n",
    "| **High ARL₁** + **Low FDR**              | Late detection, but when it happens, it's correct                    |                                       |\n",
    "| **High FAR**                             | Model predicts faults during normal → false alarms                   |                                       |\n",
    "| **High FDR** + **High FAR**              | Model often predicts faults, and often wrongly — unstable classifier |                                       |\n",
    "\n",
    "---\n",
    "\n",
    "**🔹 HOW to Interpret the Plot**\n",
    "\n",
    "In your **combined plot**:\n",
    "\n",
    "* Each bar group (per fault) shows:\n",
    "\n",
    "  * **ARL₁:** How quickly the fault is detected\n",
    "  * **FDR:** How often the predicted label for this fault was wrong\n",
    "  * **FAR:** How noisy this fault label is during normal time\n",
    "\n",
    "---\n",
    "\n",
    "**🔹 Example Interpretation**\n",
    "\n",
    "Say for **Fault 2**:\n",
    "\n",
    "* ARL₁ = 0.5 → detected fast\n",
    "* FDR = 0.3 → 30% of “Fault 2” predictions were incorrect\n",
    "* FAR = 0.2 → Fault 2 was predicted during normal periods 20% of the time\n",
    "\n",
    "**Interpretation**:\n",
    "\n",
    "* Detection is quick (good),\n",
    "* but 30% of the time the system thinks it's Fault 2, it's actually not (bad),\n",
    "* and it's somewhat noisy during normal operation.\n",
    "\n",
    "You'd ask:\n",
    "\n",
    "* Can we improve the model to reduce false positives for Fault 2?\n",
    "* Is the model too biased toward predicting Fault 2?\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 Summary Table\n",
    "\n",
    "| Metric | Low is Good? | Meaning                   |\n",
    "| ------ | ------------ | ------------------------- |\n",
    "| ARL₁   | ✅            | Faster fault detection    |\n",
    "| FDR    | ✅            | More precise labeling     |\n",
    "| FAR    | ✅            | More stable during normal |\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825c82b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_fdr_far_dict(fdr_far_dict: dict[str, pd.DataFrame]) -> None:\n",
    "    for model_name, df_fdr_far in fdr_far_dict.items():\n",
    "        df = df_fdr_far.copy()\n",
    "        df.index = df.index.map(str)\n",
    "\n",
    "        # Per-fault (exclude averages)\n",
    "        df_faults = df[~df.index.str.contains(\"Average\")]\n",
    "        df_faults = df_faults[[\"FDR\", \"FAR\"]].dropna(how=\"all\")\n",
    "\n",
    "        x = range(len(df_faults))\n",
    "        width = 0.5\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(25, 6))\n",
    "        ax.bar([i - width / 2 for i in x],\n",
    "               df_faults[\"FDR\"],\n",
    "               width,\n",
    "               label=\"FDR\",\n",
    "               color=\"cornflowerblue\")\n",
    "        ax.bar([i + width / 2 for i in x],\n",
    "               df_faults[\"FAR\"],\n",
    "               width,\n",
    "               label=\"FAR\",\n",
    "               color=\"salmon\")\n",
    "\n",
    "        ax.set_xticks(list(x))\n",
    "        ax.set_xticklabels(df_faults.index, rotation=45, ha=\"right\")\n",
    "        ax.set_ylabel(\"Rate\")\n",
    "        ax.set_title(f\"FDR and FAR per Fault - Model: {model_name}\")\n",
    "        ax.legend()\n",
    "        ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.6)\n",
    "\n",
    "        for i in x:\n",
    "            for j, col in enumerate([\"FDR\", \"FAR\"]):\n",
    "                val = df_faults.iloc[i][col]\n",
    "                if pd.notna(val):\n",
    "                    ax.text(i + (j - 0.5) * width,\n",
    "                            val + 0.015,\n",
    "                            f\"{val:.2f}\",\n",
    "                            ha=\"center\",\n",
    "                            fontsize=8)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        save_plot(plot_name=\"fdr_far per class\", plot_path=\"fdr_far\")\n",
    "        plt.show()\n",
    "\n",
    "        # Plot averages separately\n",
    "        df_avg = df[df.index.str.contains(\"Average\")]\n",
    "        if not df_avg.empty:\n",
    "            df_avg = df_avg[[\"FDR\", \"FAR\"]].dropna(how=\"all\")\n",
    "            df_avg = df_avg.rename_axis(\"Average Type\").reset_index()\n",
    "            df_melted = df_avg.melt(id_vars=\"Average Type\",\n",
    "                                    var_name=\"Metric\",\n",
    "                                    value_name=\"Value\")\n",
    "\n",
    "            plt.figure(figsize=(5, 6))\n",
    "            ax = sns.barplot(data=df_melted,\n",
    "                             x=\"Average Type\",\n",
    "                             y=\"Value\",\n",
    "                             hue=\"Metric\",\n",
    "                             palette={\n",
    "                                 \"FDR\": \"cornflowerblue\",\n",
    "                                 \"FAR\": \"salmon\"\n",
    "                             })\n",
    "\n",
    "            ax.set_title(f\"FDR and FAR Averages - Model: {model_name}\")\n",
    "            ax.set_ylabel(\"Rate\")\n",
    "            ax.set_xlabel(\"\")\n",
    "            ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.6)\n",
    "\n",
    "            for container in ax.containers:\n",
    "                ax.bar_label(container, fmt=\"%.2f\", padding=3)\n",
    "\n",
    "            plt.tight_layout()\n",
    "            save_plot(plot_name=\"fdr_far average\", plot_path=\"fdr_far\")\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "plot_fdr_far_dict(fdr_far_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75d57c4",
   "metadata": {},
   "source": [
    "### Plot first correct detection delay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed9dbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot_combined_detection_delay\n",
    "def plot_combined_detection_delay(delay_tables: dict,\n",
    "                                  plot_name: str = \"detection_delay\") -> None:\n",
    "    combined_df = pd.concat(\n",
    "        [df.assign(Model=model) for model, df in delay_tables.items()],\n",
    "        ignore_index=True,\n",
    "    )\n",
    "\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    sns.barplot(data=combined_df,\n",
    "                x=\"Fault\",\n",
    "                y=\"First Detection Delay\",\n",
    "                hue=\"Model\")\n",
    "    plt.title(\"First Correct Detection Delay per Fault for All Models\")\n",
    "    plt.xlabel(\"Fault\")\n",
    "    plt.ylabel(\"First Detection Delay (samples)\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    save_plot(plot_name=plot_name, plot_path=\"detection_delay\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_combined_detection_delay(delay_tables_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8884626f",
   "metadata": {},
   "source": [
    "# Anomaly Detection\n",
    " ** No fault classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385a3f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Variables\n",
    "anomaly_results_per_model: dict[str, pd.DataFrame] = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0669e46d",
   "metadata": {},
   "source": [
    "## MCUSUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242f0d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MCUSUM Code\n",
    "\n",
    "import numpy as np\n",
    "from numpy.typing import NDArray\n",
    "\n",
    "\n",
    "def estimate_incontrol_parameters(\n",
    "    X_incontrol: NDArray[np.float64],\n",
    ") -> tuple[NDArray[np.float64], NDArray[np.float64]]:\n",
    "    mu_0 = np.mean(X_incontrol, axis=0)\n",
    "    sigma = np.cov(X_incontrol, rowvar=False, bias=False)\n",
    "    return mu_0, sigma\n",
    "\n",
    "\n",
    "def compute_mcusum_scores(\n",
    "    X_test: NDArray[np.float64],\n",
    "    mu_0: NDArray[np.float64],\n",
    "    sigma: NDArray[np.float64],\n",
    "    k: float,\n",
    ") -> NDArray[np.float64]:\n",
    "    X_test = np.asarray(X_test)\n",
    "    mu_0 = np.asarray(mu_0)\n",
    "    sigma = np.asarray(sigma)\n",
    "\n",
    "    n_samples, n_features = X_test.shape\n",
    "\n",
    "    eigvals, eigvecs = np.linalg.eigh(sigma)\n",
    "    eigvals_inv_sqrt = np.diag(1.0 / np.sqrt(eigvals))\n",
    "    sigma_inv_sqrt = eigvecs @ eigvals_inv_sqrt @ eigvecs.T\n",
    "\n",
    "    Z = (X_test - mu_0) @ sigma_inv_sqrt.T\n",
    "\n",
    "    S_t = np.zeros(n_features)\n",
    "    T = np.zeros(n_samples)\n",
    "\n",
    "    for t in range(n_samples):\n",
    "        V_t = S_t + Z[t]\n",
    "        norm_V_t = np.linalg.norm(V_t)\n",
    "\n",
    "        if norm_V_t <= k:\n",
    "            S_t = np.zeros(n_features)\n",
    "        else:\n",
    "            shrinkage = 1.0 - k / norm_V_t\n",
    "            S_t = V_t * shrinkage\n",
    "\n",
    "        T[t] = np.linalg.norm(S_t)\n",
    "\n",
    "    return T\n",
    "\n",
    "\n",
    "def compute_reference_value_k(delta: NDArray[np.float64],\n",
    "                              sigma: NDArray[np.float64]) -> float:\n",
    "    \"\"\"\n",
    "    Compute MCUSUM reference value k = 0.5 * ||Σ^{-1/2} δ||\n",
    "\n",
    "    Parameters:\n",
    "    - delta: shift vector (length = n_features)\n",
    "    - sigma: in-control covariance matrix (n_features x n_features)\n",
    "\n",
    "    Returns:\n",
    "    - k: reference value\n",
    "    \"\"\"\n",
    "\n",
    "    # Whitening matrix: Σ^{-1/2} using eigen decomposition\n",
    "    eigvals, eigvecs = np.linalg.eigh(sigma)\n",
    "    eigvals_inv_sqrt = np.diag(1.0 / np.sqrt(eigvals))\n",
    "    sigma_inv_sqrt = eigvecs @ eigvals_inv_sqrt @ eigvecs.T\n",
    "\n",
    "    # Transform delta into whitened space\n",
    "    whitened_delta = sigma_inv_sqrt @ delta\n",
    "\n",
    "    # Compute norm and halve it\n",
    "    k = 0.5 * np.linalg.norm(whitened_delta)\n",
    "\n",
    "    return k\n",
    "\n",
    "\n",
    "def estimate_h(\n",
    "    x_incontrol_np: NDArray[np.float64],\n",
    "    k: float = 0.5,\n",
    "    percential_threshold: int = 98,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Estimate in-control parameters mu_0 and sigma from in-control data.\n",
    "    \"\"\"\n",
    "    mu_0, sigma = estimate_incontrol_parameters(x_incontrol_np)\n",
    "\n",
    "    n_simulations = 500\n",
    "    max_T_values = []\n",
    "\n",
    "    for i in range(n_simulations):\n",
    "        indices = np.random.choice(x_incontrol_np.shape[0],\n",
    "                                   size=300,\n",
    "                                   replace=True)\n",
    "        sample = x_incontrol_np[indices]\n",
    "        T = compute_mcusum_scores(sample, mu_0, sigma, k=k)\n",
    "        max_T_values.append(np.max(T))\n",
    "\n",
    "    # Empirical percential_threshold for false alarm\n",
    "    h = np.percentile(max_T_values, percential_threshold)\n",
    "    print(\"Estimated control limit h:\", h)\n",
    "    return h\n",
    "\n",
    "\n",
    "def get_control_limit(alpha: float, n_features: int) -> float:\n",
    "    return chi2.ppf(1 - alpha, df=n_features)\n",
    "\n",
    "\n",
    "def compute_detection_metrics(\n",
    "        predicted_flags: NDArray[np.bool_],\n",
    "        true_labels: NDArray[np.int64]) -> dict[str, float | str]:\n",
    "    y_pred = predicted_flags.astype(int)\n",
    "    y_true = true_labels.astype(int)\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "\n",
    "    acc: float = (tp + tn) / (tp + tn + fp + fn)\n",
    "    prec: float = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "    rec: float = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    tnr: float = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
    "    fpr: float = fp / (fp + tn) if (fp + tn) > 0 else 0.0\n",
    "    fnr: float = fn / (fn + tp) if (fn + tp) > 0 else 0.0\n",
    "    npv: float = tn / (tn + fn) if (tn + fn) > 0 else 0.0\n",
    "    fdr: float = fp / (fp + tp) if (fp + tp) > 0 else 0.0\n",
    "    bal_acc: float = 0.5 * (rec + tnr)\n",
    "    f1: float = 2 * prec * rec / (prec + rec) if (prec + rec) > 0 else 0.0\n",
    "\n",
    "    return {\n",
    "        \"Accuracy\": acc,  # \"How often am I correct overall, regardless of class?\"\n",
    "        \"Precision\": prec,  # \"When I flag something, how often is it truly an issue?\"\n",
    "        \"Recall / TPR\": rec,  # \"Out of all actual issues, how many did I correctly flag?\"\n",
    "        \"FPR\": fpr,  # \"How often do I wrongly flag a normal case?\"\n",
    "        \"NPV (Negative Predictive Value)\": npv,  # \"When I say something is normal, how often is that actually true?\"\n",
    "        \"Balanced Accuracy\": bal_acc,  # \"How well do I perform on both classes, accounting for imbalance?\"\n",
    "        \"F1-Score\": f1,  # \"What’s the balance between being thorough and being right when flagging?\"\n",
    "        # \"FDR (False Discovery Rate)\": fdr,  # \"When I raise an alert, how often am I wrong?\" FDR = 1 − Precision\n",
    "        # \"FNR (False Negative Rate)\": fnr,  # \"How often do I miss a real issue?\" FNR = 1 − Recall\n",
    "        # \"Specificity / TNR\": tnr,  # \"Out of all normal cases, how many did I correctly leave alone?\" TNR = 1 − FPR\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "def plot_mcusum_static_historgram(mcusum_statistics, threshold=95, h=0):\n",
    "    threshold_value = np.percentile(mcusum_statistics, threshold)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    # Main histogram\n",
    "    sns.histplot(\n",
    "        mcusum_statistics,\n",
    "        bins=50,\n",
    "        kde=True,\n",
    "        stat=\"density\",\n",
    "        color=\"blue\",\n",
    "        edgecolor=\"black\",\n",
    "        alpha=0.6,\n",
    "    )\n",
    "    # Vertical lines\n",
    "    plt.axvline(h, color=\"red\", linestyle=\"--\", label=\"Control Limit (h)\")\n",
    "    plt.axvline(threshold_value,\n",
    "                color=\"orange\",\n",
    "                linestyle=\"--\",\n",
    "                label=\"95th Percentile\")\n",
    "    # Shade area above 95th percentile\n",
    "    density = (sns.kdeplot(mcusum_statistics,\n",
    "                           bw_adjust=1).get_lines()[0].get_data())\n",
    "    plt.fill_between(\n",
    "        density[0],\n",
    "        density[1],\n",
    "        where=density[0] > threshold_value,\n",
    "        color=\"orange\",\n",
    "        alpha=0.3,\n",
    "        label=\"Top 5%\",\n",
    "    )\n",
    "    plt.title(\"MCUSUM Statistics Distribution with Top 5% Highlighted\")\n",
    "    plt.xlabel(\"MCUSUM Value\")\n",
    "    plt.ylabel(\"Density\")\n",
    "    plt.legend()\n",
    "    plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_mcusum_score_as_time_series(mcusum_statistics_in_control,\n",
    "                                     mcusum_statistics_out_of_control=None,\n",
    "                                     h=50):\n",
    "    # Plottting the MCUSUM statiscs as a time series\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    plt.plot(\n",
    "        mcusum_statistics_in_control,\n",
    "        label=\"MCUSUM Statistics (INC)\",\n",
    "        color=\"blue\",\n",
    "    )\n",
    "    if mcusum_statistics_out_of_control is not None:\n",
    "        plt.plot(\n",
    "            mcusum_statistics_out_of_control,\n",
    "            label=\"MCUSUM Statistics (OOC)\",\n",
    "            color=\"green\",\n",
    "            alpha=0.5,\n",
    "        )\n",
    "    plt.axhline(h, color=\"red\", linestyle=\"--\", label=\"Control Limit (h)\")\n",
    "    plt.title(\"MCUSUM Statistics Over Time\")\n",
    "    plt.xlabel(\"Sample Index\")\n",
    "    plt.ylabel(\"MCUSUM Value\")\n",
    "    plt.legend()\n",
    "    plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b34053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MCUSUM Configure and play with hyperparameters\n",
    "\n",
    "# Estimate parameters from in-control data\n",
    "mu_0, sigma = estimate_incontrol_parameters(X_INCONTROL_TRAIN_REDUCED)\n",
    "\n",
    "delta = np.zeros(X_INCONTROL_TRAIN_REDUCED.shape[1])  # Initialize delta vector\n",
    "delta = np.ones(52) * 0.1\n",
    "k = compute_reference_value_k(delta, sigma)\n",
    "h = estimate_h(X_INCONTROL_TRAIN_REDUCED, k, 99)  # control limit\n",
    "\n",
    "mcusum_statistics = compute_mcusum_scores(X_INCONTROL_TEST_REDUCED, mu_0, sigma, k)\n",
    "mcusum_statistics_ofc = compute_mcusum_scores(X_OUT_OF_CONTROL_TEST_REDUCED, mu_0,\n",
    "                                              sigma, k)\n",
    "\n",
    "plot_mcusum_static_historgram(mcusum_statistics=mcusum_statistics,\n",
    "                              threshold=95,\n",
    "                              h=h)\n",
    "plot_mcusum_score_as_time_series(\n",
    "    mcusum_statistics_in_control=mcusum_statistics,\n",
    "    mcusum_statistics_out_of_control=mcusum_statistics_ofc,\n",
    "    h=h,\n",
    ")\n",
    "\n",
    "print(\"Computed k:\", k)\n",
    "print(\"MCUSUM statistics shape:\", mcusum_statistics.shape)\n",
    "print(\"First 10 MCUSUM values:\", mcusum_statistics[:10])\n",
    "print(\"Maximum MCUSUM value:\", np.max(mcusum_statistics))\n",
    "\n",
    "# print 95 percentile of the MCUSUM statistics\n",
    "mcusum_95_percentile = np.percentile(mcusum_statistics, 95)\n",
    "print(f\"95th Percentile of MCUSUM Statistics: {mcusum_95_percentile:.2f}\")\n",
    "\n",
    "# Flagging out-of-control points\n",
    "mcusum_flags = mcusum_statistics > h\n",
    "print(mcusum_flags)\n",
    "# Output summary\n",
    "print(f\"MCUSUM Control Limit: {h:.2f}\")\n",
    "print(\n",
    "    f\"Out-of-Control Points (MCUSUM): {np.sum(mcusum_flags)} / {len(mcusum_flags)}\"\n",
    ")\n",
    "print(\n",
    "    f\"Percentage of Out-of-Control Points: {np.mean(mcusum_flags) * 100:.2f}%\")\n",
    "print(\n",
    "    \"No out-of-control points expected, so the percentage should be close to 0%.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5658d2",
   "metadata": {},
   "source": [
    "**MCUSUM could not seprate normal data from fault type 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4d59d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MCUSUM Run on test data\n",
    "\n",
    "mcusum_statistics = compute_mcusum_scores(X_TEST_REDUCED, mu_0, sigma, k)\n",
    "\n",
    "print(\"MCUSUM statistics shape:\", mcusum_statistics.shape)\n",
    "print(\"First 10 MCUSUM values:\", mcusum_statistics[:10])\n",
    "\n",
    "# Flagging out-of-control points\n",
    "mcusum_flags = mcusum_statistics > h\n",
    "print(mcusum_flags)\n",
    "# Output summary\n",
    "print(f\"MCUSUM Control Limit: {h:.2f}\")\n",
    "print(\n",
    "    f\"Out-of-Control Points (MCUSUM): {np.sum(mcusum_flags)} / {len(mcusum_flags)}\"\n",
    ")\n",
    "print(\n",
    "    tabulate(\n",
    "        [[\n",
    "            np.sum(mcusum_flags),\n",
    "            len(mcusum_flags),\n",
    "            np.mean(mcusum_flags) * 100,\n",
    "        ]],\n",
    "        headers=[\"Out-of-Control Points\", \"Total Points\", \"Percentage (%)\"],\n",
    "        tablefmt=\"grid\",\n",
    "    ))\n",
    "\n",
    "# True labels (0 = normal, 1 = anomaly)\n",
    "y_true = Y_TEST_ANOMALY_REDUCED_DF.astype(int)\n",
    "\n",
    "# Predicted labels from MCUSUM\n",
    "y_pred = mcusum_flags.astype(int)\n",
    "\n",
    "print(\"mcusum_flags.shape:\", mcusum_flags.shape)\n",
    "print(\"y_test_anomaly.shape:\", Y_TEST_ANOMALY_REDUCED_DF.shape)\n",
    "\n",
    "# Compute detection metrics for MCUSUM\n",
    "print(\"mcusum_flags.shape:\", mcusum_flags.shape)\n",
    "print(\"y_test_anomaly.shape:\", Y_TEST_ANOMALY_REDUCED_DF.shape)\n",
    "\n",
    "\n",
    "def compute_detection_metrics(predicted, true_labels: NDArray[np.int64]):\n",
    "    y_pred = predicted.astype(int)\n",
    "    y_true = true_labels.astype(int)\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "\n",
    "    acc: float = (tp + tn) / (tp + tn + fp + fn)\n",
    "    prec: float = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "    rec: float = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    tnr: float = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
    "    fpr: float = fp / (fp + tn) if (fp + tn) > 0 else 0.0\n",
    "    fnr: float = fn / (fn + tp) if (fn + tp) > 0 else 0.0\n",
    "    npv: float = tn / (tn + fn) if (tn + fn) > 0 else 0.0\n",
    "    fdr: float = fp / (fp + tp) if (fp + tp) > 0 else 0.0\n",
    "    bal_acc: float = 0.5 * (rec + tnr)\n",
    "    f1: float = 2 * prec * rec / (prec + rec) if (prec + rec) > 0 else 0.0\n",
    "    return pd.DataFrame([{\n",
    "        \"Accuracy\": acc,  # \"How often am I correct overall, regardless of class?\"\n",
    "        \"Precision\": prec,  # \"When I flag something, how often is it truly an issue?\"\n",
    "        \"Recall / TPR\": rec,  # \"Out of all actual issues, how many did I correctly flag?\"\n",
    "        \"F1-Score\": f1,  # \"What’s the balance between being thorough and being right when flagging?\"\n",
    "        \"FPR\": fpr,  # \"How often do I wrongly flag a normal case?\"\n",
    "        \"NPV (Negative Predictive Value)\": npv,  # \"When I say something is normal, how often is that actually true?\"\n",
    "        \"Balanced Accuracy\": bal_acc,  # \"How well do I perform on both classes, accounting for imbalance?\"\n",
    "        # \"FDR (False Discovery Rate)\": fdr,  # \"When I raise an alert, how often am I wrong?\" FDR = 1 − Precision\n",
    "        # \"FNR (False Negative Rate)\": fnr,  # \"How often do I miss a real issue?\" FNR = 1 − Recall\n",
    "        # \"Specificity / TNR\": tnr,  # \"Out of all normal cases, how many did I correctly leave alone?\" TNR = 1 − FPR\n",
    "    }])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "mcusum_metrics = compute_detection_metrics(mcusum_flags.astype(int),\n",
    "                                           Y_TEST_ANOMALY_REDUCED_DF)\n",
    "\n",
    "# Convert to one-row DataFrame\n",
    "# mcusum_df = pd.DataFrame([mcusum_metrics])\n",
    "\n",
    "# Add to the results dict\n",
    "classification_results_per_model[\"MCUSUM\"] = mcusum_metrics\n",
    "anomaly_results_per_model[\"MCUSUM\"] = mcusum_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f688ce17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Metrics Tables\n",
    "\n",
    "for model_name, df in classification_results_per_model.items():\n",
    "    # Add an \"Average\" row at the end of the DataFrame\n",
    "    average_row = df.mean(axis=0).to_frame().T\n",
    "    average_row.index = [\"Average\"]\n",
    "    df = pd.concat([df, average_row])\n",
    "    save_dataframe(df=df, name=model_name,suffix=\"metrics\")\n",
    "    print(f\"Classification Metrics for {model_name}:\")\n",
    "    print(tabulate(df, headers=\"keys\", tablefmt=\"grid\"))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614886b3",
   "metadata": {},
   "source": [
    "## DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05250956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DNN Model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Define input layer\n",
    "inputs = Input(shape=(X_TRAIN.shape[1], ))\n",
    "\n",
    "# Define hidden layer with 16 nodes and ReLU activation function\n",
    "hidden_layer = Dense(100, activation=\"selu\")(inputs)\n",
    "hidden_layer = Dense(100, activation=\"selu\")(hidden_layer)\n",
    "hidden_layer = Dense(100, activation=\"selu\")(hidden_layer)\n",
    "hidden_layer = Dense(100, activation=\"selu\")(hidden_layer)\n",
    "hidden_layer = Dense(100, activation=\"selu\")(hidden_layer)\n",
    "hidden_layer = Dense(100, activation=\"selu\")(hidden_layer)\n",
    "# Define output layer with softmax activation function for multi-class classification\n",
    "outputs = Dense(Y_ENC_ANOMALY_TEST_REDUCED.shape[1],\n",
    "                activation=\"softmax\")(hidden_layer)\n",
    "\n",
    "# Define the model\n",
    "model_anomaly = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Compile the model with binary cross-entropy loss function and Adam optimizer\n",
    "model_anomaly.compile(loss=\"categorical_crossentropy\",\n",
    "                      optimizer=\"adam\",\n",
    "                      metrics=[\"accuracy\"])\n",
    "\n",
    "# Print the summary of the model\n",
    "model_anomaly.summary()\n",
    "\n",
    "# Define early stopping callback to monitor validation loss and stop if it doesn't improve for 5 epochs\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", patience=5)\n",
    "\n",
    "# Train the model with 20 epochs and batch size of 32, using the early stopping callback\n",
    "history = model_anomaly.fit(\n",
    "    X_TRAIN,\n",
    "    Y_ENC_ANOMALY_TRAIN_REDUCED,\n",
    "    epochs=200,\n",
    "    batch_size=256,\n",
    "    validation_data=(X_TEST_REDUCED, Y_ENC_ANOMALY_TEST_REDUCED),\n",
    "    callbacks=[early_stop],\n",
    ")\n",
    "\n",
    "# Plot the training history for loss and accuracy\n",
    "plt.plot(history.history[\"loss\"], label=\"Training Loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"Validation Loss\")\n",
    "plt.plot(history.history[\"accuracy\"], label=\"Training Accuracy\")\n",
    "plt.plot(history.history[\"val_accuracy\"], label=\"Validation Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc6c597",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_anomaly_nn = encoder_2.inverse_transform(\n",
    "    model_anomaly.predict(X_TEST_REDUCED, verbose=0))\n",
    "y_true_anomaly_nn = encoder_2.inverse_transform(\n",
    "    Y_ENC_ANOMALY_TEST_REDUCED)\n",
    "\n",
    "dnn_anomaly_metrics = compute_detection_metrics(y_pred_anomaly_nn,\n",
    "                                                y_true_anomaly_nn)\n",
    "\n",
    "# Convert to one-row DataFrame\n",
    "# mcusum_df = pd.DataFrame([dnn_anomaly_metrics])\n",
    "\n",
    "# Add to the results dict\n",
    "anomaly_results_per_model[\"DNN\"] = dnn_anomaly_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95e73c4",
   "metadata": {},
   "source": [
    "## ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696203e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit models to the data\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "# Create an instance of each algorithm\n",
    "# logreg = LogisticRegression(max_iter=10000)\n",
    "# knn = KNeighborsClassifier()\n",
    "# dt = DecisionTreeClassifier()\n",
    "# nb = GaussianNB()\n",
    "# svm = SVC()\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "xg = xgb.XGBClassifier()\n",
    "\n",
    "# Train the algorithms on the data\n",
    "# logreg.fit(x_train, Y_TRAIN)\n",
    "# knn.fit(x_train, Y_TRAIN)\n",
    "# dt.fit(x_train, Y_TRAIN)\n",
    "# nb.fit(x_train, Y_TRAIN)\n",
    "# svm.fit(x_train, Y_TRAIN)\n",
    "\n",
    "rf.fit(X_TRAIN, Y_TRAIN_ANOMALY_REDUCED_DF.to_numpy())\n",
    "xg.fit(X_TRAIN, Y_TRAIN_ANOMALY_REDUCED_DF.to_numpy())\n",
    "\n",
    "# Use the trained models to make predictions on new data\n",
    "# y_pred_logreg =logreg.predict(x_test)\n",
    "# y_pred_dt = dt.predict(x_test)\n",
    "# y_pred_nb = nb.predict(x_test)\n",
    "# y_pred_knn = knn.predict(x_test)\n",
    "# y_pred_svm = svm.predict(x_test)\n",
    "\n",
    "y_pred_anomaly_rf = rf.predict(X_TEST_REDUCED)\n",
    "y_pred_anomaly_xg = xg.predict(X_TEST_REDUCED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2a36ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_anomaly_metrics = compute_detection_metrics(y_pred_anomaly_rf,\n",
    "                                               Y_TEST_ANOMALY_REDUCED_DF)\n",
    "xg_anomaly_metrics = compute_detection_metrics(y_pred_anomaly_xg,\n",
    "                                               Y_TEST_ANOMALY_REDUCED_DF)\n",
    "\n",
    "#results_per_model[\"RF_ANOMALY\"] = rf_anomaly_metrics\n",
    "#results_per_model[\"XG_ANOMALY\"] = xg_anomaly_metrics\n",
    "\n",
    "anomaly_results_per_model[\"RF_ANOMALY\"] = rf_anomaly_metrics\n",
    "anomaly_results_per_model[\"XG_ANOMALY\"] = xg_anomaly_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02c4f41",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92f1bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from typing import Tuple\n",
    "\n",
    "\n",
    "def train_pca_model(\n",
    "        X_train: NDArray[np.float64],\n",
    "        n_components=0.9,\n",
    "        contamination: float = 0.05) -> Tuple[StandardScaler, PCA, float]:\n",
    "    \"\"\"\n",
    "    Trains PCA and scaler on normal training data and computes anomaly threshold.\n",
    "\n",
    "    Args:\n",
    "        X_train (NDArray[np.float64]): Training data (normal only or mostly normal).\n",
    "        n_components (int): PCA components.\n",
    "        contamination (float): Assumed anomaly rate to define threshold.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[StandardScaler, PCA, float]: (fitted scaler, fitted PCA model, anomaly threshold)\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "    pca = PCA(n_components=n_components)\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "    X_reconstructed = pca.inverse_transform(X_pca)\n",
    "\n",
    "    errors = np.sum((X_scaled - X_reconstructed)**2, axis=1)\n",
    "    threshold_index = int((1 - contamination) * len(errors))\n",
    "    threshold = np.sort(errors)[threshold_index]\n",
    "\n",
    "    return scaler, pca, threshold\n",
    "\n",
    "\n",
    "def detect_anomalies_pca(\n",
    "    X_test: NDArray[np.float64],\n",
    "    scaler: StandardScaler,\n",
    "    pca: PCA,\n",
    "    threshold: float,\n",
    ") -> Tuple[NDArray[np.float64], NDArray[np.int64]]:\n",
    "    \"\"\"\n",
    "    Applies trained PCA model to test data and flags anomalies.\n",
    "\n",
    "    Args:\n",
    "        X_test (NDArray[np.float64]): Test data.\n",
    "        scaler (StandardScaler): Fitted scaler from training phase.\n",
    "        pca (PCA): Fitted PCA model.\n",
    "        threshold (float): Reconstruction error threshold for anomaly.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[NDArray[np.float64], NDArray[np.int64]]:\n",
    "            - reconstruction_errors\n",
    "            - binary anomaly flags (1 = anomaly)\n",
    "    \"\"\"\n",
    "    X_scaled = scaler.transform(X_test)\n",
    "    X_pca = pca.transform(X_scaled)\n",
    "    X_reconstructed = pca.inverse_transform(X_pca)\n",
    "\n",
    "    reconstruction_errors = np.sum((X_scaled - X_reconstructed)**2, axis=1)\n",
    "    anomaly_flags = (reconstruction_errors > threshold).astype(int)\n",
    "\n",
    "    return reconstruction_errors, anomaly_flags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d250cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 1: Fit\n",
    "scaler, pca_model, threshold = train_pca_model(X_TRAIN,\n",
    "                                               n_components=0.9,\n",
    "                                               contamination=0.05)\n",
    "\n",
    "# Phase 2: Predict\n",
    "errors, y_pred = detect_anomalies_pca(X_TEST_REDUCED, scaler, pca_model, threshold)\n",
    "\n",
    "# Evaluate\n",
    "pca_metrics_df = compute_detection_metrics(y_pred, Y_TEST_ANOMALY_REDUCED_DF)\n",
    "\n",
    "#results_per_model[\"PCA\"] = pca_metrics_df\n",
    "anomaly_results_per_model[\"PCA\"] = pca_metrics_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c09cf2",
   "metadata": {},
   "source": [
    "## LGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4ed54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "\n",
    "def train_lgb_model(X_train: NDArray[np.float64],\n",
    "                    y_train: NDArray[np.int64],\n",
    "                    seed: int = 42) -> lgb.Booster:\n",
    "    \"\"\"\n",
    "    Trains a LightGBM model and returns the trained booster.\n",
    "\n",
    "    Args:\n",
    "        X_train (NDArray[np.float64]): Training features.\n",
    "        y_train (NDArray[np.int64]): Training labels (0 = normal, 1 = anomaly).\n",
    "        seed (int): Random seed.\n",
    "\n",
    "    Returns:\n",
    "        lgb.Booster: Trained LightGBM model.\n",
    "    \"\"\"\n",
    "    train_set = lgb.Dataset(X_train, label=y_train)\n",
    "\n",
    "    params = {\n",
    "        \"objective\": \"binary\",\n",
    "        \"metric\": \"binary_logloss\",\n",
    "        \"verbosity\": -1,\n",
    "        \"boosting_type\": \"gbdt\",\n",
    "        \"learning_rate\": 0.1,\n",
    "        \"num_leaves\": 31,\n",
    "        \"seed\": seed,\n",
    "    }\n",
    "\n",
    "    model: lgb.Booster = lgb.train(params, train_set, num_boost_round=100)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1409d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lgb = train_lgb_model(X_TRAIN, Y_TRAIN_ANOMALY_REDUCED_DF)\n",
    "\n",
    "y_proba: NDArray[np.float64] = model_lgb.predict(X_TEST_REDUCED)\n",
    "y_pred: NDArray[np.int64] = (y_proba >= 0.5).astype(int)\n",
    "\n",
    "lgb_metrics = compute_detection_metrics(y_pred, Y_TEST_ANOMALY_REDUCED_DF)\n",
    "#results_per_model[\"LGB\"] = lgb_metrics\n",
    "anomaly_results_per_model[\"LGB\"] = lgb_metrics\n",
    "print(lgb_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23ac729",
   "metadata": {},
   "source": [
    "## Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228239fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, List\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "def build_autoencoder_dynamic(\n",
    "        input_dim: int,\n",
    "        latent_dim: Optional[int] = None,\n",
    "        hidden_dims: Optional[List[int]] = None) -> keras.Model:\n",
    "    \"\"\"\n",
    "    Builds an autoencoder with configurable number of hidden layers.\n",
    "\n",
    "    Args:\n",
    "        input_dim (int): Number of input features.\n",
    "        latent_dim (Optional[int]): Size of bottleneck. If None, use input_dim // 20, clamped to [4, 64].\n",
    "        hidden_dims (Optional[List[int]]): List of encoder layer sizes before bottleneck.\n",
    "                                           Decoder will mirror them. If None, a default pattern is used.\n",
    "\n",
    "    Returns:\n",
    "        keras.Model: Compiled autoencoder model.\n",
    "    \"\"\"\n",
    "    # Determine latent size if not provided\n",
    "    if latent_dim is None:\n",
    "        latent_dim = max(4, min(64, input_dim // 20))\n",
    "\n",
    "    # Set default encoder sizes if not provided\n",
    "    if hidden_dims is None:\n",
    "        # Example: for input_dim=100, get [64, 32]\n",
    "        hidden_dims = [max(32, input_dim // 2), max(16, input_dim // 4)]\n",
    "\n",
    "    # Define input layer\n",
    "    input_layer = keras.Input(shape=(input_dim, ))\n",
    "    x = input_layer\n",
    "\n",
    "    # Encoder: stack each Dense layer with ReLU\n",
    "    for size in hidden_dims:\n",
    "        x = layers.Dense(size, activation='relu')(x)\n",
    "\n",
    "    # Bottleneck layer\n",
    "    encoded = layers.Dense(latent_dim, activation='relu')(x)\n",
    "\n",
    "    # Decoder: mirror of encoder\n",
    "    for size in reversed(hidden_dims):\n",
    "        encoded = layers.Dense(size, activation='relu')(encoded)\n",
    "\n",
    "    # Final output layer: linear activation to reconstruct inputs\n",
    "    output_layer = layers.Dense(input_dim, activation='linear')(encoded)\n",
    "\n",
    "    # Construct and compile model\n",
    "    model = keras.Model(inputs=input_layer, outputs=output_layer)\n",
    "    model.compile(optimizer=keras.optimizers.Adam(1e-3), loss='mse')\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_autoencoder_tf(X_train: np.ndarray,\n",
    "                         model: keras.Model,\n",
    "                         n_epochs: int = 100,\n",
    "                         batch_size: int = 64,\n",
    "                         patience: int = 10,\n",
    "                         verbose: bool = True) -> keras.Model:\n",
    "    \"\"\"\n",
    "    Trains the Keras autoencoder using early stopping and prints debug output.\n",
    "\n",
    "    Args:\n",
    "        X_train (np.ndarray): Input training data, shape (n_samples, n_features).\n",
    "        model (keras.Model): Keras autoencoder model instance to train.\n",
    "        n_epochs (int): Maximum number of training epochs.\n",
    "        batch_size (int): Number of samples per batch during training.\n",
    "        patience (int): Number of epochs with no improvement before stopping early.\n",
    "        verbose (bool): If True, print debug and training status messages.\n",
    "\n",
    "    Returns:\n",
    "        keras.Model: Trained model with best weights.\n",
    "    \"\"\"\n",
    "    # Ensure training data is in float32 (required by TensorFlow)\n",
    "    if X_train.dtype != np.float32:\n",
    "        X_train = X_train.astype(np.float32)\n",
    "    if verbose:\n",
    "        print(f\"[DEBUG] Input shape: {X_train.shape}, dtype: {X_train.dtype}\")\n",
    "\n",
    "    # Create early stopping callback to halt training when loss plateaus\n",
    "    early_stop = keras.callbacks.EarlyStopping(\n",
    "        monitor=\"loss\",  # monitor training loss\n",
    "        patience=patience,  # stop if no improvement for N epochs\n",
    "        min_delta=1e-6,  # only improvements greater than this count\n",
    "        restore_best_weights=True,  # restore weights from lowest loss\n",
    "        verbose=1 if verbose else 0)\n",
    "\n",
    "    # Fit model on training data using input = output (unsupervised)\n",
    "    model.fit(\n",
    "        X_train,  # input data\n",
    "        X_train,  # target is same as input (autoencoder)\n",
    "        epochs=n_epochs,  # maximum training epochs\n",
    "        batch_size=batch_size,  # batch size\n",
    "        shuffle=True,  # shuffle input data each epoch\n",
    "        callbacks=[early_stop],  # attach early stopping\n",
    "        verbose=1 if verbose else 0  # print training logs if requested\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def compute_reconstruction_error(model: keras.Model,\n",
    "                                 X: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Computes squared reconstruction error for each sample.\n",
    "\n",
    "    Args:\n",
    "        model (keras.Model): Trained autoencoder model.\n",
    "        X (np.ndarray): Input data (e.g., test set), shape (n_samples, n_features).\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Reconstruction error (per sample), shape (n_samples,).\n",
    "    \"\"\"\n",
    "    # Ensure float32 input (required by TensorFlow)\n",
    "    if X.dtype != np.float32:\n",
    "        X = X.astype(np.float32)\n",
    "\n",
    "    # Predict reconstructed input using the trained model\n",
    "    X_reconstructed: np.ndarray = model.predict(X, verbose=0)\n",
    "\n",
    "    # Compute element-wise squared error per sample (L2 norm)\n",
    "    squared_error: np.ndarray = np.sum((X - X_reconstructed)**2, axis=1)\n",
    "\n",
    "    return squared_error\n",
    "\n",
    "\n",
    "def detect_anomalies_from_error(errors: np.ndarray,\n",
    "                                contamination: float = 0.05) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Flags anomalies based on sorted reconstruction error and contamination rate.\n",
    "\n",
    "    Args:\n",
    "        errors (np.ndarray): Reconstruction error per sample.\n",
    "        contamination (float): Expected proportion of anomalies.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Binary array of flags (1 = anomaly, 0 = normal).\n",
    "    \"\"\"\n",
    "    # Compute the threshold (95th percentile if contamination = 0.05)\n",
    "    threshold_idx = int((1 - contamination) * len(errors))\n",
    "    threshold = np.sort(errors)[threshold_idx]\n",
    "\n",
    "    # Flag values above threshold as anomalies\n",
    "    return (errors > threshold).astype(int)\n",
    "\n",
    "\n",
    "def compute_latent_dim(input_dim: int) -> int:\n",
    "    \"\"\"\n",
    "    Computes a reasonable latent dimension based on input complexity.\n",
    "\n",
    "    Args:\n",
    "        input_dim (int): Number of features in the input.\n",
    "\n",
    "    Returns:\n",
    "        int: Latent dimension for autoencoder.\n",
    "    \"\"\"\n",
    "    # Rule: at least 4, at most 64, about 1/20th of input\n",
    "    return max(4, min(64, input_dim // 20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b053fed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim: int = X_TRAIN.shape[1]  # dynamically detect feature count\n",
    "latent_dim = compute_latent_dim(input_dim)\n",
    "autoencoder_model = build_autoencoder_dynamic(input_dim=X_TRAIN.shape[1])\n",
    "autoencoder_model = train_autoencoder_tf(X_TRAIN, autoencoder_model)\n",
    "\n",
    "# After training\n",
    "\n",
    "# 1. Predict reconstruction error on x_test\n",
    "errors_test = compute_reconstruction_error(autoencoder_model, X_TEST_REDUCED)\n",
    "\n",
    "# 2. Convert reconstruction errors to binary anomaly predictions\n",
    "y_pred: np.ndarray = detect_anomalies_from_error(errors_test,\n",
    "                                                 contamination=0.05)\n",
    "\n",
    "# 3. Use your true binary test labels (0 = normal, 1 = anomaly)\n",
    "#     Example: y_test_anomaly = np.array([0, 0, 1, 0, ...])\n",
    "#     It must match x_test.shape[0]\n",
    "\n",
    "# 4. Evaluate metrics using your benchmark function\n",
    "autoencoder_metrics = compute_detection_metrics(y_pred, Y_TEST_ANOMALY_REDUCED_DF)\n",
    "\n",
    "# 5. Optionally print or tabulate results\n",
    "#results_per_model[\"Autoencoder\"] = autoencoder_metrics\n",
    "anomaly_results_per_model[\"Autoencoder\"] = autoencoder_metrics\n",
    "print(autoencoder_metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c71088",
   "metadata": {},
   "source": [
    "## Evaluating\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**Detailed Metric Breakdown**\n",
    "\n",
    "| **Metric**                          | **Formula**                                                                             | **Interpretation in Fault Detection**                                                                                                                                              |\n",
    "| ----------------------------------- | --------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| **Accuracy**                        | $\\frac{TP + TN}{TP + TN + FP + FN}$                                                     | Overall correctness. Can be misleading when faults are rare. High accuracy might simply mean the model always predicts \"normal.\"                                                   |\n",
    "| **Precision**                       | $\\frac{TP}{TP + FP}$                                                                    | Of all alarms raised, how many were actual faults? **Low precision means many false alarms**, which overload operators and reduce trust in the system.                             |\n",
    "| **Recall / Sensitivity / TPR**      | $\\frac{TP}{TP + FN}$                                                                    | Proportion of actual faults correctly detected. **High recall is critical in safety-sensitive environments** — a low recall means faults go undetected, risking damage or hazards. |\n",
    "| **Specificity / TNR**               | $\\frac{TN}{TN + FP}$                                                                    | Proportion of normal conditions correctly identified. High specificity = fewer false alarms, preserving operator trust and avoiding unnecessary shutdowns.                         |\n",
    "| **FPR (False Positive Rate)**       | $\\frac{FP}{FP + TN}$                                                                    | Rate at which the system wrongly flags normal data as faulty. High FPR leads to wasted investigations and “alarm fatigue.”                                                         |\n",
    "| **FNR (False Negative Rate)**       | $\\frac{FN}{FN + TP}$                                                                    | Rate at which faults are missed. **The most dangerous metric** in safety-critical monitoring. A high FNR means faults are silently ignored.                                        |\n",
    "| **NPV (Negative Predictive Value)** | $\\frac{TN}{TN + FN}$                                                                    | When the system says “everything is fine,” how often is it actually correct? Important for trusting normal-state decisions, especially when faults are rare.                       |\n",
    "| **FDR (False Discovery Rate)**      | $\\frac{FP}{FP + TP}$                                                                    | Among all detected anomalies, how many were false? High FDR undermines the system’s credibility. Operators may start ignoring the model’s outputs.                                 |\n",
    "| **Balanced Accuracy**               | $\\frac{1}{2} (\\text{TPR} + \\text{TNR})$                                                 | Averages recall for both classes (normal and fault). Gives fair performance measurement on **imbalanced data** (e.g., few faults).                                                 |\n",
    "| **F1 Score**                        | $\\frac{2 \\cdot \\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}$ | Harmonizes Precision and Recall. Penalizes when either false alarms or missed faults are high. **Best for evaluating fault classifiers where both errors are costly.**             |\n",
    "\n",
    "---\n",
    "\n",
    "🔍 Use-Case Driven Summary\n",
    "\n",
    "| **If you want to...**                          | **Use this metric...**    | **Because...**                                        |\n",
    "| ---------------------------------------------- | ------------------------- | ----------------------------------------------------- |\n",
    "| Detect all faults without missing any          | **Recall (TPR)**          | Missing a fault could cause damage or loss.           |\n",
    "| Avoid false alarms                             | **Precision, FPR, FDR**   | Too many false positives cause alarm fatigue.         |\n",
    "| Know how trustworthy “normal” output is        | **NPV**                   | Operators rely on normal predictions to avoid action. |\n",
    "| Compare performance across imbalanced datasets | **Balanced Accuracy, F1** | Raw accuracy is biased when faults are rare.          |\n",
    "\n",
    "---\n",
    "\n",
    "If both **Recall = 1.0** and **Precision = 1.0**, this means:\n",
    "\n",
    "* **Every actual fault was correctly detected (no false negatives)**\n",
    "* **Every predicted fault was correct (no false positives)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21d0bcc",
   "metadata": {},
   "source": [
    "### Table Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54761198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Metrics Tables\n",
    "\n",
    "\n",
    "def convert_result_dict_to_df(results_dict):\n",
    "    results_df = pd.concat(\n",
    "        [df.assign(Model=model) for model, df in results_dict.items()],\n",
    "        ignore_index=True)\n",
    "    # Move 'Model' column to the first position after index\n",
    "    cols = ['Model'] + [col for col in results_df.columns if col != 'Model']\n",
    "    results_df = results_df[cols]\n",
    "    return results_df\n",
    "\n",
    "    cols = ['Model'] + [col for col in conctated_anomaly_results_df.columns if col != 'Model']\n",
    "conctated_anomaly_results_df = convert_result_dict_to_df(anomaly_results_per_model)\n",
    "save_dataframe(df=conctated_anomaly_results_df,name=\"Anomaly detection metrics\")\n",
    "conctated_anomaly_results_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5921de81",
   "metadata": {},
   "source": [
    "### Plot Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bceeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_model_anomaly_detection_comparison\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from typing import Dict\n",
    "\n",
    "\n",
    "def plot_model_anomaly_detection_comparison(\n",
    "    metrics_dict: Dict[str, pd.DataFrame], ) -> None:\n",
    "    df_combined = pd.concat(\n",
    "        [df.assign(Model=model) for model, df in metrics_dict.items()],\n",
    "        ignore_index=True,\n",
    "    )\n",
    "\n",
    "    df_melted = df_combined.melt(id_vars=\"Model\",\n",
    "                                 var_name=\"Metric\",\n",
    "                                 value_name=\"Value\")\n",
    "\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    ax = plt.subplot()\n",
    "\n",
    "    model_list = df_melted[\"Model\"].unique().tolist()\n",
    "    metric_list = df_melted[\"Metric\"].unique().tolist()\n",
    "    n_models = len(model_list)\n",
    "    bar_width = 0.2\n",
    "    group_spacing = 0.4  # Gap between metric groups\n",
    "\n",
    "    # X-axis base positions for each metric group\n",
    "    base_positions = [\n",
    "        i * (n_models * bar_width + group_spacing)\n",
    "        for i in range(len(metric_list))\n",
    "    ]\n",
    "\n",
    "    # Plot each model\n",
    "    for model_idx, model in enumerate(model_list):\n",
    "        subset = df_melted[df_melted[\"Model\"] == model]\n",
    "\n",
    "        # Compute shifted bar positions for this model within each group\n",
    "        bar_positions = [pos + bar_width * model_idx for pos in base_positions]\n",
    "\n",
    "        ax.bar(bar_positions, subset[\"Value\"], width=bar_width, label=model)\n",
    "\n",
    "    # Set proper x-ticks centered under each metric group\n",
    "    tick_positions = [\n",
    "        pos + (bar_width * n_models / 2) - (bar_width / 2)\n",
    "        for pos in base_positions\n",
    "    ]\n",
    "    ax.set_xticks(tick_positions)\n",
    "    ax.set_xticklabels(metric_list, rotation=45, ha=\"right\")\n",
    "\n",
    "    ax.set_ylabel(\"Score\")\n",
    "    ax.set_title(\"Model Comparison Across Metrics\")\n",
    "    ax.legend()\n",
    "    ax.yaxis.grid(True, linestyle=\"--\", alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    save_plot(plot_name=\"Anomaly detection metrics\", plot_path=\"anomaly\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_model_anomaly_detection_comparison(anomaly_results_per_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83244aee",
   "metadata": {},
   "source": [
    "# Faults merging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f672820",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80e36a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DNN Model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Define input layer\n",
    "inputs = Input(shape=(X_TRAIN.shape[1], ))\n",
    "\n",
    "# Define hidden layer with 16 nodes and ReLU activation function\n",
    "hidden_layer = Dense(100, activation=\"selu\")(inputs)\n",
    "hidden_layer = Dense(100, activation=\"selu\")(hidden_layer)\n",
    "hidden_layer = Dense(100, activation=\"selu\")(hidden_layer)\n",
    "hidden_layer = Dense(100, activation=\"selu\")(hidden_layer)\n",
    "hidden_layer = Dense(100, activation=\"selu\")(hidden_layer)\n",
    "hidden_layer = Dense(100, activation=\"selu\")(hidden_layer)\n",
    "# Define output layer with softmax activation function for multi-class classification\n",
    "outputs = Dense(Y_ENC_MERGED_TRAIN_REDUCED.shape[1],activation=\"softmax\")(hidden_layer)\n",
    "\n",
    "# Define the model\n",
    "model_fm = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Compile the model with binary cross-entropy loss function and Adam optimizer\n",
    "model_fm.compile(loss=\"categorical_crossentropy\",\n",
    "                 optimizer=\"adam\",\n",
    "                 metrics=[\"accuracy\"])\n",
    "\n",
    "# Print the summary of the model\n",
    "model_fm.summary()\n",
    "\n",
    "# Define early stopping callback to monitor validation loss and stop if it doesn't improve for 5 epochs\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", patience=5)\n",
    "\n",
    "# Train the model with 20 epochs and batch size of 32, using the early stopping callback\n",
    "history = model_fm.fit(\n",
    "    X_TRAIN,\n",
    "    Y_ENC_MERGED_TRAIN_REDUCED,\n",
    "    epochs=200,\n",
    "    batch_size=256,\n",
    "    validation_data=(X_TEST_REDUCED, Y_ENC_MERGED_TEST_REDUCED),\n",
    "    callbacks=[early_stop],\n",
    ")\n",
    "\n",
    "# Plot the training history for loss and accuracy\n",
    "plt.plot(history.history[\"loss\"], label=\"Training Loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"Validation Loss\")\n",
    "plt.plot(history.history[\"accuracy\"], label=\"Training Accuracy\")\n",
    "plt.plot(history.history[\"val_accuracy\"], label=\"Validation Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede0dc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_results_fm = model_fm.predict(X_TEST_REDUCED, verbose=0)\n",
    "y_pred_nn_fm = encoder_3.inverse_transform(nn_results_fm)\n",
    "y_true_nn_fm= encoder_3.inverse_transform(Y_ENC_MERGED_TEST_REDUCED)\n",
    "y_score_nn_fm = model_fm.predict(X_TEST_REDUCED, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3fcca7",
   "metadata": {},
   "source": [
    "## Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af296e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(\"Neural Net FM\", y_true_nn_fmy_pred_nn.ravel(), y_pred_nn_fmy_pred_nn.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2861587a",
   "metadata": {},
   "source": [
    "### Plots per fault per Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7656ea4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_fault_scores_per_metric_all_models(classification_scores_per_fault_tables_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae473db7",
   "metadata": {},
   "source": [
    "### Table Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d3456a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#results_per_model[\"Neural Net FM\"] = compute_classification_metrics_per_class(y_true_nn_fm, y_pred_nn_fm)\n",
    "\n",
    "# loop through each model and print the classification metrics\n",
    "for model_name, df in classification_results_per_model.items():\n",
    "    # Add an \"Average\" row at the end of the DataFrame\n",
    "    average_row = df.mean(axis=0).to_frame().T\n",
    "    average_row.index = [\"Average\"]\n",
    "    df = pd.concat([df, average_row])\n",
    "    save_dataframe(df=df, name=model_name,suffix=\"metrics\")\n",
    "    print(f\"Classification Metrics for {model_name}:\")\n",
    "    print(tabulate(df, headers=\"keys\", tablefmt=\"grid\"))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a195d4",
   "metadata": {},
   "source": [
    "### Confusion matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2589e16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_confusion_matrices({\n",
    "    \"Neural Net\": (y_true_nn, y_pred_nn),\n",
    "    \"Neural Net FM\": (y_true_nn_fm, y_pred_nn_fm),\n",
    "})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 139.563836,
   "end_time": "2023-11-01T14:14:28.045526",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-11-01T14:12:08.481690",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
