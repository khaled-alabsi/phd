{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Anomaly Detection Methods - Comparison Template\n",
        "\n",
        "## Purpose\n",
        "This notebook provides a clean template for developing and comparing anomaly detection methods.\n",
        "\n",
        "## Structure\n",
        "1. **Configuration** - Imports and global settings\n",
        "2. **Data Loading** - Load and preprocess datasets\n",
        "3. **Data Preparation** - Create train/test splits\n",
        "4. **Detection Methods** - Implement your methods here\n",
        "5. **Benchmarking** - Compare all methods\n",
        "6. **Results & Visualization** - Analyze and visualize results\n",
        "\n",
        "## How to Use\n",
        "1. Run the **Configuration** and **Data Loading** sections first\n",
        "2. Implement your detection method in the **Detection Methods** section\n",
        "3. Add your method to the `MODELS` dictionary in the **Benchmarking** section\n",
        "4. Run the benchmarking and visualization sections\n",
        "\n",
        "## Notes\n",
        "- Each detection method should implement a `predict(X) -> np.ndarray` function\n",
        "- Predictions should return binary labels: 0 (normal) or 1 (anomaly)\n",
        "- Keep method implementations in separate sections for clarity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Configuration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.1 Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluation and comparison tools\n",
        "from lib.evaluation.ModelComparisonAnalyzer import ModelComparisonAnalyzer\n",
        "from lib.evaluation.model_evaluator import evaluate_models\n",
        "\n",
        "# Data utilities\n",
        "from lib.data.data_generator import generate_synthetic_datasets, load_datasets\n",
        "\n",
        "# Standard libraries\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from IPython.display import display\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from typing import Any, Dict, List, Literal, Optional, Tuple\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "\n",
        "from numpy.typing import NDArray\n",
        "\n",
        "print(\"âœ… All imports successful!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.2 Global Constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Column names\n",
        "TARGET_VARIABLE_COLUMN_NAME = \"faultNumber\"\n",
        "SIMULATION_RUN_COLUMN_NAME = \"simulationRun\"\n",
        "COLUMNS_TO_REMOVE = [TARGET_VARIABLE_COLUMN_NAME, SIMULATION_RUN_COLUMN_NAME, \"sample\"]\n",
        "\n",
        "# Fault injection point (where faults start in the test data)\n",
        "FAULT_INJECTION_POINT = 160\n",
        "\n",
        "# Data source: 'real' (load RData files) or 'synthetic' (lightweight random data)\n",
        "DATA_SOURCE: Literal[\"real\", \"synthetic\"] = \"synthetic\"\n",
        "\n",
        "# Synthetic data configuration (used when DATA_SOURCE = 'synthetic')\n",
        "SYNTHETIC_DATA_CONFIG: Dict[str, Any] = {\n",
        "    \"n_simulation_runs\": 5,\n",
        "    \"n_faults\": 5,\n",
        "    \"n_samples\": 300,\n",
        "    \"n_features\": 52,\n",
        "    \"random_state\": 42,\n",
        "    \"feature_prefix\": \"feature\",\n",
        "}\n",
        "\n",
        "print(\"âœ… Configuration set!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Loading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1 Load Raw Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load datasets\n",
        "datasets = load_datasets(DATA_SOURCE, synthetic_config=SYNTHETIC_DATA_CONFIG)\n",
        "\n",
        "DF_FF_TRAINING_RAW = datasets[\"fault_free_training\"]\n",
        "DF_FF_TEST_RAW = datasets[\"fault_free_testing\"]\n",
        "DF_F_TRAINING_RAW = datasets[\"faulty_training\"]\n",
        "DF_F_TEST_RAW = datasets[\"faulty_testing\"]\n",
        "\n",
        "print(f\"Data source: {DATA_SOURCE}\")\n",
        "print(f\"Fault-free training shape: {DF_FF_TRAINING_RAW.shape}\")\n",
        "print(f\"Fault-free test shape: {DF_FF_TEST_RAW.shape}\")\n",
        "print(f\"Faulty training shape: {DF_F_TRAINING_RAW.shape}\")\n",
        "print(f\"Faulty test shape: {DF_F_TEST_RAW.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 Data Preview"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preview the data structure\n",
        "print(\"Fault-free training data (first 3 rows):\")\n",
        "display(DF_FF_TRAINING_RAW.head(3))\n",
        "\n",
        "print(\"\\nFaulty training data (first 3 rows):\")\n",
        "display(DF_F_TRAINING_RAW.head(3))\n",
        "\n",
        "print(\"\\nUnique faults:\", sorted(DF_F_TRAINING_RAW[TARGET_VARIABLE_COLUMN_NAME].unique()))\n",
        "print(\"Unique simulations:\", sorted(DF_F_TRAINING_RAW[SIMULATION_RUN_COLUMN_NAME].unique()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 Fit Scaler on Full Training Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fit scaler on all fault-free training data (in-control)\n",
        "scaler = StandardScaler()\n",
        "X_INCONTROL_TRAIN_FULL_DF = DF_FF_TRAINING_RAW.drop(columns=COLUMNS_TO_REMOVE, axis=1)\n",
        "\n",
        "scaler.fit(X_INCONTROL_TRAIN_FULL_DF)\n",
        "X_INCONTROL_TRAIN_FULL_SCALED = scaler.transform(X_INCONTROL_TRAIN_FULL_DF)\n",
        "\n",
        "print(\"In-control training data (full) shape:\", X_INCONTROL_TRAIN_FULL_SCALED.shape)\n",
        "print(\"Number of features:\", X_INCONTROL_TRAIN_FULL_SCALED.shape[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 Create Single Simulation Run Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select a specific simulation run and fault for quick testing\n",
        "simulation_run = 1\n",
        "fault_number = 2\n",
        "\n",
        "# Filter data by simulation run\n",
        "DF_FF_TRAINING = DF_FF_TRAINING_RAW.query(\"simulationRun == @simulation_run\")\n",
        "DF_FF_TEST = DF_FF_TEST_RAW.query(\"simulationRun == @simulation_run\")\n",
        "DF_F_TRAINING = DF_F_TRAINING_RAW.query(\"faultNumber == @fault_number and simulationRun == @simulation_run\")\n",
        "DF_F_TEST = DF_F_TEST_RAW.query(\"faultNumber == @fault_number and simulationRun == @simulation_run\")\n",
        "\n",
        "# Prepare feature matrices\n",
        "X_INCONTROL_TRAIN_REDUCED_DF = DF_FF_TRAINING.drop(columns=COLUMNS_TO_REMOVE, axis=1)\n",
        "X_INCONTROL_TEST_REDUCED_DF = DF_FF_TEST.drop(columns=COLUMNS_TO_REMOVE, axis=1)\n",
        "X_OUT_OF_CONTROL_TEST_REDUCED_DF = DF_F_TEST.drop(columns=COLUMNS_TO_REMOVE, axis=1)\n",
        "X_OUT_OF_CONTROL_TRAIN_REDUCED_DF = DF_F_TRAINING.drop(columns=COLUMNS_TO_REMOVE, axis=1)\n",
        "\n",
        "# Scale data\n",
        "X_INCONTROL_TRAIN_REDUCED_SCALED = scaler.transform(X_INCONTROL_TRAIN_REDUCED_DF)\n",
        "X_INCONTROL_TEST_REDUCED_SCALED = scaler.transform(X_INCONTROL_TEST_REDUCED_DF)\n",
        "X_OUT_OF_CONTROL_TEST_REDUCED_SCALED = scaler.transform(X_OUT_OF_CONTROL_TEST_REDUCED_DF)\n",
        "X_OUT_OF_CONTROL_TRAIN_REDUCED_SCALED = scaler.transform(X_OUT_OF_CONTROL_TRAIN_REDUCED_DF)\n",
        "\n",
        "print(f\"Using simulation run: {simulation_run}, fault: {fault_number}\")\n",
        "print(f\"In-control training (reduced): {X_INCONTROL_TRAIN_REDUCED_SCALED.shape}\")\n",
        "print(f\"In-control test (reduced): {X_INCONTROL_TEST_REDUCED_SCALED.shape}\")\n",
        "print(f\"Out-of-control training: {X_OUT_OF_CONTROL_TRAIN_REDUCED_SCALED.shape}\")\n",
        "print(f\"Out-of-control test: {X_OUT_OF_CONTROL_TEST_REDUCED_SCALED.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.3 Cut Data After Fault Injection Point"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Remove samples before fault injection point\n",
        "X_INCONTROL_TEST_REDUCED_SCALED_CUT = X_INCONTROL_TEST_REDUCED_SCALED[FAULT_INJECTION_POINT:]\n",
        "X_OUT_OF_CONTROL_TEST_REDUCED_SCALED_CUT = X_OUT_OF_CONTROL_TEST_REDUCED_SCALED[FAULT_INJECTION_POINT:]\n",
        "\n",
        "print(f\"Fault injection point: {FAULT_INJECTION_POINT}\")\n",
        "print(f\"In-control test (cut): {X_INCONTROL_TEST_REDUCED_SCALED_CUT.shape}\")\n",
        "print(f\"Out-of-control test (cut): {X_OUT_OF_CONTROL_TEST_REDUCED_SCALED_CUT.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.4 Summary of Available Datasets\n",
        "\n",
        "After running the cells above, you have access to:\n",
        "\n",
        "**Full datasets (all simulation runs):**\n",
        "- `X_INCONTROL_TRAIN_FULL_SCALED` - All fault-free training data (scaled)\n",
        "- `DF_FF_TEST_RAW`, `DF_F_TEST_RAW` - Raw test data (for benchmarking)\n",
        "\n",
        "**Single simulation run datasets:**\n",
        "- `X_INCONTROL_TRAIN_REDUCED_SCALED` - In-control training\n",
        "- `X_INCONTROL_TEST_REDUCED_SCALED` - In-control test\n",
        "- `X_OUT_OF_CONTROL_TEST_REDUCED_SCALED` - Out-of-control test\n",
        "\n",
        "**Cut datasets (after fault injection):**\n",
        "- `X_INCONTROL_TEST_REDUCED_SCALED_CUT` - In-control test (cut)\n",
        "- `X_OUT_OF_CONTROL_TEST_REDUCED_SCALED_CUT` - Out-of-control test (cut)\n",
        "\n",
        "**Scaler:**\n",
        "- `scaler` - Fitted StandardScaler for transforming new data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Detection Methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ðŸ“ Instructions for Adding New Methods\n",
        "\n",
        "Each detection method should:\n",
        "1. Be implemented in its own subsection (e.g., `### 4.1 My Method`)\n",
        "2. Train on `X_INCONTROL_TRAIN_FULL_SCALED` (or reduced version)\n",
        "3. Define a prediction function that takes `X: np.ndarray` and returns binary predictions\n",
        "4. Be added to the `MODELS` dictionary in Section 5\n",
        "\n",
        "**Example structure:**\n",
        "```python\n",
        "# Train your method\n",
        "my_detector = MyDetector()\n",
        "my_detector.fit(X_INCONTROL_TRAIN_FULL_SCALED)\n",
        "\n",
        "# Create prediction wrapper\n",
        "def my_detector_predict(X: np.ndarray) -> np.ndarray:\n",
        "    return my_detector.predict(X)  # Returns 0/1 labels\n",
        "\n",
        "# Quick test\n",
        "predictions = my_detector_predict(X_INCONTROL_TEST_REDUCED_SCALED_CUT)\n",
        "arl = np.argmax(predictions == 1) if np.any(predictions == 1) else None\n",
        "print(f\"ARL (first detection): {arl}\")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.1 Method Template (Example)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Replace this with your actual detection method\n",
        "\n",
        "# Example: Simple threshold-based detector\n",
        "class ExampleDetector:\n",
        "    def __init__(self, threshold_percentile=95):\n",
        "        self.threshold_percentile = threshold_percentile\n",
        "        self.threshold = None\n",
        "        self.mean = None\n",
        "    \n",
        "    def fit(self, X):\n",
        "        \"\"\"Fit on in-control training data.\"\"\"\n",
        "        self.mean = np.mean(X, axis=1)\n",
        "        self.threshold = np.percentile(self.mean, self.threshold_percentile)\n",
        "        return self\n",
        "    \n",
        "    def predict(self, X):\n",
        "        \"\"\"Predict anomalies (0 = normal, 1 = anomaly).\"\"\"\n",
        "        scores = np.mean(X, axis=1)\n",
        "        return (scores > self.threshold).astype(int)\n",
        "\n",
        "# Train detector\n",
        "example_detector = ExampleDetector(threshold_percentile=95)\n",
        "example_detector.fit(X_INCONTROL_TRAIN_FULL_SCALED)\n",
        "\n",
        "# Create prediction wrapper\n",
        "def example_detector_predict(X: np.ndarray) -> np.ndarray:\n",
        "    return example_detector.predict(X)\n",
        "\n",
        "# Quick test\n",
        "predictions_normal = example_detector_predict(X_INCONTROL_TEST_REDUCED_SCALED_CUT)\n",
        "predictions_anomaly = example_detector_predict(X_OUT_OF_CONTROL_TEST_REDUCED_SCALED_CUT)\n",
        "\n",
        "arl0 = np.argmax(predictions_normal == 1) if np.any(predictions_normal == 1) else None\n",
        "arl1 = np.argmax(predictions_anomaly == 1) if np.any(predictions_anomaly == 1) else None\n",
        "\n",
        "print(f\"Example Detector - ARL0 (In-control): {arl0}\")\n",
        "print(f\"Example Detector - ARL1 (Out-of-control): {arl1}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2 Your Method 1\n",
        "\n",
        "Add your first detection method here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Implement your first detection method\n",
        "#\n",
        "# # Train your method\n",
        "# method1 = YourMethod1()\n",
        "# method1.fit(X_INCONTROL_TRAIN_FULL_SCALED)\n",
        "#\n",
        "# # Create prediction wrapper\n",
        "# def method1_predict(X: np.ndarray) -> np.ndarray:\n",
        "#     return method1.predict(X)\n",
        "#\n",
        "# # Quick test\n",
        "# predictions = method1_predict(X_INCONTROL_TEST_REDUCED_SCALED_CUT)\n",
        "# arl = np.argmax(predictions == 1) if np.any(predictions == 1) else None\n",
        "# print(f\"Method 1 - ARL: {arl}\")\n",
        "\n",
        "pass  # Remove this when you add your code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.3 Your Method 2\n",
        "\n",
        "Add your second detection method here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Implement your second detection method\n",
        "\n",
        "pass  # Remove this when you add your code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.4 Your Method 3\n",
        "\n",
        "Add additional methods as needed..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Add more methods as needed\n",
        "\n",
        "pass  # Remove this when you add your code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Benchmarking & Comparison"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.1 Define Models Dictionary\n",
        "\n",
        "Add all your detection methods to the `MODELS` dictionary below.\n",
        "Each entry should map a method name to its prediction function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define all models to compare\n",
        "MODELS = {\n",
        "    \"ExampleDetector\": example_detector_predict,\n",
        "    # TODO: Add your methods here\n",
        "    # \"Method1\": method1_predict,\n",
        "    # \"Method2\": method2_predict,\n",
        "    # \"Method3\": method3_predict,\n",
        "}\n",
        "\n",
        "print(f\"Models to compare: {list(MODELS.keys())}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.2 Run Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run evaluation on all simulation runs and faults\n",
        "df_results, summary_df = evaluate_models(\n",
        "    models=MODELS,\n",
        "    df_ff_test_raw=DF_FF_TEST_RAW,\n",
        "    df_f_test_raw=DF_F_TEST_RAW,\n",
        "    scaler=scaler,\n",
        "    fault_injection_point=FAULT_INJECTION_POINT,\n",
        "    simulation_column=SIMULATION_RUN_COLUMN_NAME,\n",
        "    fault_column=TARGET_VARIABLE_COLUMN_NAME,\n",
        "    columns_to_remove=COLUMNS_TO_REMOVE,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"EVALUATION COMPLETE\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Results shape: {df_results.shape}\")\n",
        "print(f\"Summary shape: {summary_df.shape}\")\n",
        "print(f\"Models evaluated: {', '.join(MODELS.keys())}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.3 Initialize Analyzer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the model comparison analyzer\n",
        "analyzer = ModelComparisonAnalyzer(df_results, summary_df)\n",
        "print(\"âœ… Analyzer initialized!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Results & Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.1 ARL Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot ARL (Average Run Length) analysis\n",
        "analyzer.plot_arl_analysis()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.2 Average ARL Performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot average ARL across all faults\n",
        "analyzer.plot_arl_averages()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.3 Overall Classification Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot overall classification metrics (Precision, Recall, F1, etc.)\n",
        "analyzer.plot_overall_metrics()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Metrics explanation:**\n",
        "- **Precision**: High when few false alarms (normal data flagged as anomaly)\n",
        "- **Recall**: High when most anomalies are caught (few missed detections)\n",
        "- **Specificity**: High when normal data is correctly identified\n",
        "- **Accuracy**: High when few mistakes overall\n",
        "- **F1 Score**: Harmonic mean of precision and recall"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.4 Per-Fault Classification Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot metrics for each individual fault\n",
        "analyzer.plot_per_fault_bars()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.5 False Positive and False Negative Rates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot FPR and FNR (lower is better for both)\n",
        "analyzer.plot_false_rates()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.6 Confusion Matrix - Average"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Average confusion matrix across all faults\n",
        "analyzer.plot_confusion_matrix_average()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.7 Confusion Matrix - Per Fault"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Confusion matrix for each fault\n",
        "analyzer.plot_confusion_matrices_per_fault()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.8 Performance Heatmaps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Heatmaps showing performance across faults\n",
        "analyzer.plot_heatmaps()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.9 Statistical Distributions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Distribution plots for various metrics\n",
        "analyzer.plot_distributions()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Summary & Conclusions\n",
        "\n",
        "### Summary Tables\n",
        "\n",
        "Use the cells below to display summary tables and export results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display summary statistics\n",
        "print(\"Summary Statistics:\")\n",
        "display(summary_df.describe())\n",
        "\n",
        "print(\"\\nMean metrics by model:\")\n",
        "display(summary_df.groupby('model').mean())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Export Results\n",
        "\n",
        "Optionally export results to CSV for further analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Uncomment to export results\n",
        "# df_results.to_csv('results/detailed_results.csv', index=False)\n",
        "# summary_df.to_csv('results/summary_results.csv', index=False)\n",
        "# print(\"âœ… Results exported!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Notes\n",
        "\n",
        "Add your conclusions and observations here:\n",
        "\n",
        "- ...\n",
        "- ...\n",
        "- ..."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
